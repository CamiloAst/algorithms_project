@article{ALI2024101172,
title = {Physics-informed neural networks in groundwater flow modeling: Advantages and future directions},
journal = {Groundwater for Sustainable Development},
volume = {25},
pages = {101172},
year = {2024},
issn = {2352-801X},
doi = {https://doi.org/10.1016/j.gsd.2024.101172},
url = {https://www.sciencedirect.com/science/article/pii/S2352801X2400095X},
author = {Ahmed Shakir Ali Ali and Farhad Jazaei and T. Prabhakar Clement and Brian Waldron},
keywords = {Artificial intelligence, Physics-informed neural network, PINN, Groundwater modeling, MODFLOW},
abstract = {In recent years, there has been enormous development in soft computing, especially artificial intelligence (AI), which has developed robust methods for solving complex engineering problems. Researchers in the field of water resources engineering have applied these AI methods to solve a variety of hydrological problems. Despite their widespread use in the surface and atmospheric hydrology fields, groundwater hydrologists have not widely used AI methods in their routine field-scale modeling efforts. This is because AI models have been primarily considered black box models that lack physical meaning. Furthermore, using AI models to generate the space-time distribution of transient groundwater level variations is challenging and requires further flux balance and mass transport analyses. More recently, a new type of physics-informed neural network (PINN) model has been developed to address several limitations by integrating governing physics (groundwater flow equations) into the AI tools. This study presents the systematic advantages of the PINN algorithm for solving groundwater problems using a set of classic test problems. As discussed in detail in the article, these advantages and potentials are associated with the meshless nature of PINN, its continuous time and space dimensions, its independence from time-stepping and incremental marching in space, and its efficiency in running time. However, despite PINN's promising attributes, it is important to acknowledge its nascent stage of development and the inherent limitations of all neural network models, such as training challenges and hyperparameter selection. Thus, collaborative efforts between groundwater modelers and computer scientists are imperative to explore and exploit the full potential of PINN in tackling increasingly complex groundwater problems and nurturing PINN into a dependable modeling tool in industry and academia.}
}
@article{VISWAN2023102808,
title = {Understanding molecular signaling cascades in neural disease using multi-resolution models},
journal = {Current Opinion in Neurobiology},
volume = {83},
pages = {102808},
year = {2023},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2023.102808},
url = {https://www.sciencedirect.com/science/article/pii/S0959438823001332},
author = {Nisha Ann Viswan and Upinder Singh Bhalla},
abstract = {If the genome defines the program for the operations of a cell, signaling networks execute it. These cascades of chemical, cell-biological, structural, and trafficking events span milliseconds (e.g., synaptic release) to potentially a lifetime (e.g., stabilization of dendritic spines). In principle almost every aspect of neuronal function, particularly at the synapse, depends on signaling. Thus dysfunction of these cascades, whether through mutations, local dysregulation, or infection, leads to disease. The sheer complexity of these pathways is matched by the range of diseases and the diversity of their phenotypes. In this review, we discuss how to build computational models, how these models are essential to tackle this complexity, and the benefits of using families of models at different levels of detail to understand signaling in health and disease.}
}
@article{ZHAO2024102465,
title = {GA-GGD: Improving semantic discriminability in graph contrastive learning via Generative Adversarial Network},
journal = {Information Fusion},
volume = {110},
pages = {102465},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102465},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524002434},
author = {Jitao Zhao and Dongxiao He and Meng Ge and Yongqi Huang and Lianze Shan and Yongbin Qin and Zhiyong Feng},
keywords = {Graph representation learning, Generative Adversarial Network, Graph contrastive learning, Adversarial Machine Learning, Semantic discriminability},
abstract = {Graph contrastive learning has garnered considerable research interest due to its ability to effectively embed graph data without manual labels. Among them, methods based on Deep Graph Infomax (DGI) have been widely studied and favored in the industry because of their fast training speed, applicability to large-scale data. DGI-based methods usually obtain a noise graph through node shuffling. The proxy task of these methods encourages the encoder to distinguish whether the nodes come from the original graph or the noise graph, thereby maximizing the mutual information between the node representation and the graph it belongs to, while also maximizing the Jenson–Shannon divergence between the nodes of original graph and noise graph. However, we argue that these approaches only enable the encoder to differentiate between semantically meaningful graphs and noise graphs, but not to effectively identify different semantic graphs. This leads to the inability of the encoder to effectively embed information between different semantics, significantly reducing the robustness and affecting the performance of downstream tasks. In addition, this training mode makes the model more sensitive to attacks. To improve their semantic discriminability, we take advantage of the natural ability of generative adversarial networks to generate semantic data, proposing a method called Generative Adversarial Graph Group Discrimination (GA-GGD). Specifically, it consists of a graph group discriminator and a semantic attack generator. The discriminator aims to encode the graph and identify whether nodes originate from the original graph. The goal of the generator is to use random features and graph structure to find vulnerabilities of discriminator and generate node representations with similar but wrong semantic to confuse the discriminator. GA-GGD can improve the model’s semantic information embedding without significantly increasing computational overhead and memory occupancy. We test the effectiveness of the proposed model on commonly used data sets and large-scale datasets, as well as in various downstream tasks such as classification, clustering, and adversarial attacks defence. A wealth of experimental results confirm the efficacy of the proposed model.}
}
@article{BUHLER1990577,
title = {The COIN model for concurrent computation and its implementation},
journal = {Microprocessing and Microprogramming},
volume = {30},
number = {1},
pages = {577-584},
year = {1990},
note = {Proceedings Euromicro 90: Hardware and Software in System Engineering},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(90)90302-P},
url = {https://www.sciencedirect.com/science/article/pii/016560749090302P},
author = {Peter Buhler},
abstract = {COIN is a model for object-oriented programming with special emphasis on concurrent and distributed systems. It was developed to integrate design, implementation, and visualization of distributed applications. The distinguishing characteristics of COIN are: a) hierarchial object structures; b) multiple explicit object interfaces; c) explicit and dynamic binding of interface operations to operation implementations; d) generation of structures consisting of several objects and their interconnections as an atomic action. The paper gives an overview of the COIN model and its implementation in the COIN/L programming language.}
}
@article{YANG2022101239,
title = {Identifying keyword sleeping beauties: A perspective on the knowledge diffusion process},
journal = {Journal of Informetrics},
volume = {16},
number = {1},
pages = {101239},
year = {2022},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2021.101239},
url = {https://www.sciencedirect.com/science/article/pii/S1751157721001103},
author = {Jinqing Yang and Yi Bu and Wei Lu and Yong Huang and Jiming Hu and Shengzhi Huang and Li Zhang},
keywords = {Sleeping beauty, Delayed recognition, Knowledge diffusion trajectory, Survival analysis},
abstract = {Knowledge diffusion is a significant driving force behind discipline development and technological innovation. Keyword is a unique knowledge diffusion trajectory, in which the sleeping beauty phenomenon sometimes appears. In this paper, we first put forward the concept of Keyword Sleeping Beauties (KSBs) on the basis of the scientific literature phenomenon of sleeping beauties. Then, we construct a parameter-free identification method to distinguish KSBs based on beauty coefficient criteria. Furthermore, we analyze the intrinsic and extrinsic influencing factors to explore the awakening mechanism of KSBs. The experiment results show that sleeping beauty phenomena also exist in the keyword diffusion trajectory and 284 KSBs are identified. The depth of sleep has a positive correlation with awakening intensity, while the length of sleep has a negative correlation with awakening intensity. In the two years of pre-awakening, KSBs tend to appear in the journals with a higher impact factor. In addition, the adoption frequency and the number of KSBs both increase obviously in the one year of pre-awakening. The findings of this paper enrich the patterns of knowledge diffusion and extend academic thinking on the sleeping beauty in science.}
}
@article{XHAXHIU2024270,
title = {Seaweed boards as value-added natural waste product for insulation and building materials},
journal = {Energy Storage and Saving},
volume = {3},
number = {4},
pages = {270-277},
year = {2024},
issn = {2772-6835},
doi = {https://doi.org/10.1016/j.enss.2024.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density of 404.5 g·cm−3 demonstrated a maximum bending resistance of 2.72 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·m−1·K−1, which is comparable to that of polystyrene. Computational analysis of the “seaweed material” model revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young’s and shear moduli, renders it a promising candidate in construction.}
}
@article{THANHEISER2024101176,
title = {Introduction to the virtual special issue: Mathematics that underpins social issues},
journal = {The Journal of Mathematical Behavior},
volume = {75},
pages = {101176},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101176},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000531},
author = {Eva Thanheiser and Ami Mamolo},
keywords = {Mathematics in Society, Educational Research, Social Issues, Mathematical Worldview, Numeracy, Mathematical Literacy},
abstract = {This Virtual Special Issue on Mathematics in Society: Exploring the Mathematics that Underpins Social Issues features 13 articles which expand our understanding of how people build, retain, communicate, apply, and comprehend mathematical ideas as they relate to social and societal issues. The focus is on education research that explores the ways in which mathematics and a mathematical worldview can influence choices, on educational, personal and societal levels. We take a broad view and raise questions about what it means to be mathematical in society, and we consider the multifaceted ways in which abilities to derive and interpret information presented mathematically are also necessary in and for society.}
}
@incollection{RENNE2022147,
title = {Chapter 8 - Measuring and assessing resilience},
editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
booktitle = {Creating Resilient Transportation Systems},
publisher = {Elsevier},
pages = {147-192},
year = {2022},
isbn = {978-0-12-816820-2},
doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.}
}
@incollection{MARR1988534,
title = {A computational theory of human stereo vision††M.I.T. Psychology Department, 79 Amherst Street, Cambridge Ma 02139, U.S.A.},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {534-547},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50046-7},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500467},
author = {D. MARR and T. POGGIO}
}
@article{DING2025121721,
title = {Endogenous dynamics of rumor spreading and debunking considering the influence of attitude: An agent-based modeling approach},
journal = {Information Sciences},
volume = {694},
pages = {121721},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121721},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524016359},
author = {Haixin Ding},
keywords = {Rumor, Rumor debunking, Innovation diffusion, Social Judgement theory, Balance theory, Agent-based Modelling},
abstract = {Rumor and debunking help create one another, but their endogenous dynamics are still worth further exploration. Relatedly, although attitude is important during the communication process, this indispensable element has been underrepresented for rumors as a specific kind of communication. The study first uses the balance logic to reveal the fundamental influence of attitude on rumor-related interactions at the individual level; Based on the innovation diffusion perspective and social judgment theory, the study constructs a conceptual framework and mathematical models of endogenous dynamics of rumor spreading and debunking considering the influence of attitude. Using an agent-based modeling approach, the study conducts systematic computational experiments. The results show that both attitude distribution and attitude interaction structure influence the endogenous dynamics, and the two types of factors interact with each other. The study explains debunking without official rebuttal endogenously, exposes debunking may not result in the intended outcome, and illustrates focusing only on rumor spreaders would always underestimate the impact of rumors systematically.}
}
@article{WU201655,
title = {Vertical position of Chinese power words influences power judgments: Evidence from spatial compatibility task and event-related Potentials},
journal = {International Journal of Psychophysiology},
volume = {102},
pages = {55-61},
year = {2016},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2016.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167876016300253},
author = {Xiangci Wu and Huibin Jia and Enguo Wang and Chenguang Du and Xianghua Wu and Caiping Dang},
keywords = {Conceptual representation, Metaphor, Vertical position, Power},
abstract = {The present study used event-related potentials (ERPs) to explore the influence of vertical position on power judgments. Participants were asked to identify whether a Chinese word represented a powerful or powerless group (e.g., “king” or “servant”), which was presented in the top or bottom of the screen. The behavioral analysis showed that judging the power of powerful words were significantly faster when they were presented at the top position, compared with when they were presented at the bottom position. The ERP analysis showed enhanced N1 amplitude for congruent trials (i.e., the powerful words in the top and the powerless words in the bottom of the screen) and larger P300 and LPC amplitude for incongruent trials (i.e., the powerful words in the bottom and the powerless words in the top of the screen). The present findings provide further electrophysiological evidence that thinking about power can automatically activate the underlying spatial up-down (verticality) image schema and that the influence of vertical position on the power judgments not only occurs at the early perceptual stage of power word processing, but also at the higher cognitive stage (i.e., allocation of attention resources, conflict solving and response selection). This study revealed the neural underpinnings of metaphor congruent effect which have great significance to our understanding of the abstract concept power.}
}
@article{GIANNAKOS201777,
title = {Entertainment, engagement, and education: Foundations and developments in digital and physical spaces to support learning through making},
journal = {Entertainment Computing},
volume = {21},
pages = {77-81},
year = {2017},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1875952117300307},
author = {Michail N. Giannakos and Monica Divitini and Ole Sejer Iversen},
keywords = {Maker movement, Learning technologies, Entertainment technologies, Creativity, Knowledge construction, Technological fluency, Constructionist},
abstract = {Making is a relatively new concept applied to describe the increasing attention paid to constructing activities to enable entertaining, and engaging learning. Making focuses on the process that occurs in digital and/or physical spaces that is not always learning oriented, but enables qualities such as problem solving, design thinking, collaboration, and innovation, to name a few. Contemporary technical and infrastructural developments, such as Hackerspaces, Makerspaces, TechShops, and FabLabs, and the appearance of tools such as wearable computing, robotics, 3D printing, microprocessors, and intuitive programming languages, posit making as a very promising research area to support learning processes, especially towards the acquisition of 21st-century learning competences. Collecting learning evidence via rigorous multidimensional and multidisciplinary case studies will allow us to better understand and improve the value of making and the role of the various digital and physical spaces. Drawing from our experience with a recent workshop that used making as a pathway to foster joyful engagement and creativity in learning (Make2Learn), we present the developments, as well as the four selected contributions of this special issue. The paper further draws attention to the great potential and need for research in the area of making to enable entertaining, and engaging, and learning.}
}
@article{WANG2025111994,
title = {A lightweight progressive joint transfer ensemble network inspired by the Markov process for imbalanced mechanical fault diagnosis},
journal = {Mechanical Systems and Signal Processing},
volume = {224},
pages = {111994},
year = {2025},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2024.111994},
url = {https://www.sciencedirect.com/science/article/pii/S0888327024008926},
author = {Changdong Wang and Jingli Yang and Huamin Jie and Zhen Tao and Zhenyu Zhao},
keywords = {Class imbalance, Ensemble learning, Fault diagnosis, Markov process, Progressive joint-transfer strategy},
abstract = {Owing to safety limitations and data collection costs, scenarios with imbalanced data usually arise, posing a great challenge for precise fault diagnosis. Targeting imbalanced fault diagnosis and the high computational cost of mainstream ensemble learning methods currently used, this article proposes a lightweight and accurate scheme based on a progressive joint-transfer ensemble network (PJTEN) and a Markov-lightweight strategy (MLS). Specifically, a PJTEN is developed, incorporating a multiple excitation-channel attention basic estimator and progressive joint-transfer strategy (PJTS) to maintain diversity of basic estimators better and focus more on key information from minority classes. Besides, the MLS guided by Markov transition probabilities is for the first time constructed for ensemble learning to reduce the network redundancy by alternating optimization. Using a standard dataset and a brand-new dataset of a real ship propulsion system, the proposed method achieves leading results in Accuracy, F1 score and MCC, compared with eight cutting-edge methods, thereby validating its substantial value. In terms of lightweight operation, such as temporal complexity (TC), spatial complexity (SC), and time efficiency, it is also ahead of the latest ensemble-based methods.}
}
@article{EPIOTIS1989213,
title = {Lewis formulae for metallic systems: the Li tetramer paradigm},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {201},
pages = {213-238},
year = {1989},
issn = {0166-1280},
doi = {https://doi.org/10.1016/0166-1280(89)87077-0},
url = {https://www.sciencedirect.com/science/article/pii/0166128089870770},
author = {N.D. Epiotis},
abstract = {In previous works, we argued that metal atoms bind through a mechanism in which overlap is assisted by some other overlap-independent mode like dispersion or induction. The result is the formation of gas pairs (interstitial electron pairs). Unlike overlap, these mechanisms of bonding can be properly reproduced only at the MCSCF level. To draw the chemist away from one-electron thinking, we propose specific Lewis formulae for small metal clusters and we show how these change as we shift from a lower to a higher level of theory so as to project the key point. A qualitative (let alone quantitative) understanding of metallic bonding can only be achieved from examination of properly correlated wavefunctions. From the practical standpoint, we show how usage of these Lewis formulae can inspire analogies for explaining computational and experimental results.}
}
@article{ZHU2023110006,
title = {Deep reinforcement learning-based edge computing offloading algorithm for software-defined IoT},
journal = {Computer Networks},
volume = {235},
pages = {110006},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110006},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004516},
author = {Xiaojuan Zhu and Tianhao Zhang and Jinwei Zhang and Bao Zhao and Shunxiang Zhang and Cai Wu},
keywords = {Edge computing, Computing offloading, Software defined network, Internet of things, Deep reinforcement learning},
abstract = {Edge computing offloading can effectively solve the problem of insufficient computing resources for terminal devices and improve the performance and efficiency of the system. When network states and tasks change rapidly, data-driven intelligent algorithms have difficulty obtaining comprehensive statistics for accurate prediction, resulting in degraded performance of computational offloading and difficulty in adaptive adjustment. It is a current challenge to improve the environment-aware, intelligent optimization so that the computational offloading algorithm can adapt to the dynamic changes in network state and task demands, thus achieving global multi-objective optimization. This paper presents optimized edge computing offloading algorithm for software-defined IoT. First, to provide global state for making decisions, a software defined edge computing (SDEC) architecture is proposed. The edge layer is integrated into the control layer of software-defined IoT, and multiple controllers share the global network state information via east–west message exchange. Moreover, an edge computing offloading algorithm in software-defined IoT (ECO-SDIoT) based on deep reinforcement learning is proposed. It enables the controllers to offload the computing task to the most appropriate edge server according to the global states, task requirements, and reward. Finally, the performance metrics for edge computing offloading were evaluated in terms of unit task processing latency, load balancing of edge servers, task processing energy consumption, and task completion rate, respectively. Simulation results show that ECO-SDIoT can effectively reduce task completion time and energy consumption compared with other strategies.}
}
@article{ZHANG2025115558,
title = {Opportunities of applying Large Language Models in building energy sector},
journal = {Renewable and Sustainable Energy Reviews},
volume = {214},
pages = {115558},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.115558},
url = {https://www.sciencedirect.com/science/article/pii/S136403212500231X},
author = {Liang Zhang and Zhelun Chen},
keywords = {Large language models, Building energy efficiency, Building decarbonization, Knowledge extraction, Intelligent control systems, Data infrastructure, Education and training},
abstract = {In recent years, the rapid advancement and impressive capabilities of Large Language Models have been evident across various engineering domains. This paper explores the application, implications, and potential of Large Language Models in building energy sectors, especially energy efficiency and decarbonization studies, based on an extensive literature review and a survey from building engineers and scientists. The paper explores how LLMs can enhance intelligent control systems, automate code generation for software and modeling tools, optimize data infrastructure, and refine analysis of technical reports and papers. Additionally, the paper discusses the role of LLMs in improving regulatory compliance, supporting building lifecycle management, and revolutionizing education and training practices within the sector. Despite the promising potential of Large Language Models, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned Large Language Models, and self-consistency are discussed. The paper concludes with a call for future research focused on the enhancement of LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research between AI and energy experts.}
}
@article{HASSABIS2007299,
title = {Deconstructing episodic memory with construction},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {7},
pages = {299-306},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307001258},
author = {Demis Hassabis and Eleanor A. Maguire},
abstract = {It has recently been observed that the brain network supporting recall of episodic memories shares much in common with other cognitive functions such as episodic future thinking, navigation and theory of mind. It has been speculated that ‘self-projection’ is the key common process. However, in this Opinion article, we note that other functions (e.g. imagining fictitious experiences) not explicitly connected to either the self or a subjective sense of time, activate a similar brain network. Hence, we argue that the process of ‘scene construction’ is better able to account for the commonalities in the brain areas engaged by an extended range of disparate functions. In light of this, we re-evaluate our understanding of episodic memory, the processes underpinning it and other related cognitive functions.}
}
@article{BHADURI2025100723,
title = {Community partnership design of a maker-related camp for underserved youth: Impacts on youths’ present and future learning trajectories},
journal = {International Journal of Child-Computer Interaction},
volume = {44},
pages = {100723},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2025.100723},
url = {https://www.sciencedirect.com/science/article/pii/S2212868925000030},
author = {Srinjita Bhaduri and Quentin Biddy and Melissa Rummel and Mimi Recker},
keywords = {Collaborative design, Maker technologies, 3D modeling and printing, Community partnership, Informal learning, Middle school youth STEM activities},
abstract = {This paper describes the design of Science, Technology, Engineering, and Mathematics (STEM) maker-related activities offered to middle school youth as part of a free, four-week summer camp. The camp was aimed at academically at-risk youth in a rural, tourism-oriented mountain community with significant income disparities. Guided by an educational model focused on enhancing youths’ present and future interests in and visions of STEM and computing fields, camp activities were collaboratively designed by a community partnership comprised of a local camp provider, the local school district, and researchers. Situating design in a community partnership helped highlight and integrate locally relevant resources, careers, and community opportunities. The paper also reports findings from a study examining how the STEM maker camp activities, which leveraged 3D modeling and printing practices, impacted youths’ perceptions of their disciplinary identity, engagement, and their present and future visions of the relevance of these STEM practices to themselves and their communities. The study also explores design tensions that emerged during the camp design process and identified barriers and opportunities that arose from balancing the needs of each partner, the research team’s focus on youth-centered learning, and the overall program goals.}
}
@article{MAO2025102712,
title = {A survey on pragmatic processing techniques},
journal = {Information Fusion},
volume = {114},
pages = {102712},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102712},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004901},
author = {Rui Mao and Mengshi Ge and Sooji Han and Wei Li and Kai He and Luyao Zhu and Erik Cambria},
keywords = {Pragmatic processing, Metaphor understanding, Sarcasm detection, Personality recognition, Aspect extraction, Sentiment polarity detection},
abstract = {Pragmatics, situated in the domains of linguistics and computational linguistics, explores the influence of context on language interpretation, extending beyond the literal meaning of expressions. It constitutes a fundamental element for natural language understanding in machine intelligence. With the advancement of large language models, the research focus in natural language processing has predominantly shifted toward high-level task processing, inadvertently downplaying the importance of foundational pragmatic processing tasks. Nevertheless, pragmatics serves as a crucial medium for unraveling human language cognition. The exploration of pragmatic processing stands as a pivotal facet in realizing linguistic intelligence. This survey encompasses important pragmatic processing techniques for subjective and emotive tasks, such as personality recognition, sarcasm detection, metaphor understanding, aspect extraction, and sentiment polarity detection. It spans theoretical research, the forefront of pragmatic processing techniques, and downstream applications, aiming to highlight the significance of these low-level tasks in advancing natural language understanding and linguistic intelligence.}
}
@article{DEVGUN2023141,
title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
journal = {Cardiac Electrophysiology Clinics},
volume = {15},
number = {2},
pages = {141-150},
year = {2023},
note = {Left Atrial Appendage Occlusion},
issn = {1877-9182},
doi = {https://doi.org/10.1016/j.ccep.2023.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877918223000205},
author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease}
}
@article{EBBY200573,
title = {The powers and pitfalls of algorithmic knowledge: a case study},
journal = {The Journal of Mathematical Behavior},
volume = {24},
number = {1},
pages = {73-87},
year = {2005},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000768},
author = {Caroline Brayer Ebby},
keywords = {Algorithms, Computation, Learning, Student understanding, Sociocultural perspective, Reform curriculum},
abstract = {This study examines one child's use of computational procedures over a period of 3 years in an urban elementary school where teachers were using a standards-based curriculum. From a sociocultural perspective, the use of standard algorithms to solve mathematical problems is viewed as a cultural tool that both enables and constrains particular practices. As this student appropriated and mastered procedures for addition, subtraction, multiplication and division, she could solve problems that involved fairly straightforward computations or where she could easily model the action to determine an appropriate computation. At the same time, her use of these algorithms, along with other readily available tools, such as her fingers or multiplication tables, constrained her ability to reflect on the tens-structure of the number system, an effect that had serious consequences for her overall mathematical achievement. The results of this study suggest that even when not directly introduced, algorithms have such strong currency that they can mediate more reform-oriented instruction.}
}
@article{GRANJO202021,
title = {Enhancing the autonomy of students in chemical engineering education with LABVIRTUAL platform},
journal = {Education for Chemical Engineers},
volume = {31},
pages = {21-28},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S174977282030018X},
author = {José F.O. Granjo and Maria G. Rasteiro},
keywords = {Web platform, Virtual labs, Chemical processes, Autonomous learning},
abstract = {Engineering educators have been developing different approaches to supplement scientific background and further develop the ability for autonomous and critical thinking in students. In 2009, the University of Coimbra has made available on-line a virtual platform with a wide scope, directed towards Chemical Engineering education. The platform is divided into four different educational topics: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications, and case studies to help understanding chemical/biochemical processes and improve their autonomy. This paper presents an assessment of the use of that platform by two different groups of students in the school years of 2015/2016 and 2018/2019: a group from the 3rd-year of Chemical Engineering, and another one from a Project Design course (2nd cycle, MSc of Chemical Engineering). A case study addressing the synthesis of phthalic anhydride by o-xylene oxidation on a fixed-bed catalytic reactor is also given to show the use of existing simulators in LABVIRTUAL.}
}
@incollection{BILLEN2023385,
title = {Chapter 16 - Lithosphere–Mantle Interactions in Subduction Zones},
editor = {João C. Duarte},
booktitle = {Dynamics of Plate Tectonics and Mantle Convection},
publisher = {Elsevier},
pages = {385-405},
year = {2023},
isbn = {978-0-323-85733-8},
doi = {https://doi.org/10.1016/B978-0-323-85733-8.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857338000147},
author = {Magali I. Billen},
keywords = {Subduction dynamics, Rheology, Phase transitions, Numerical modeling, Mantle mixing},
abstract = {How does the interaction of sinking lithosphere with the mantle contribute to the motion of tectonics plates at the Earth's surface and to long-term mixing in the deep mantle? In the decades immediately following the acceptance of the theory of plate tectonics, these questions were pursued vigorously using analytical, laboratory, and numerical models. In the past two decades, attention has turned to building on this foundational knowledge using numerical simulations to more fully integrate the complexity of Earth materials including the effects of deformation mechanisms, composition, fluids, melting, and phase transitions. This ongoing transition to a more system-centered view of geodynamics and plate tectonics not only presents many challenges (computational, experimental, and theoretical) but also promises to bridge the gaps in our current understanding and address the still enigmatic behavior of sinking lithosphere.}
}
@article{LUPION2025112832,
title = {A holistic approach for resource-constrained neural network architecture search},
journal = {Applied Soft Computing},
volume = {172},
pages = {112832},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112832},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625001437},
author = {M. Lupión and N.C. Cruz and E.M. Ortigosa and P.M. Ortigosa},
keywords = {Artificial neural networks, Neural architecture search, Meta-heuristic, TLBO, Neural network encoding, Performance predictor},
abstract = {The design of Artificial Neural Networks (ANN) is critical for their performance. The research field called Neural Network Search (NAS) investigates automated design strategies. This work proposes a novel NAS stack that stands out in three facets. First, the representation scheme encodes problem-specific ANN as plain vectors of numbers without needing auxiliary conversion models. Second, it is a pioneer in relying on the TLBO meta-heuristic. This optimizer supports large-scale problems and only expects two parameters, contrasting with other meta-heuristics used for NAS. Third, the stack includes a new evaluation predictor that avoids evaluating non-promising architectures. It combines several machine learning methods that train as the optimizer evaluates solutions, which avoids preliminary preparing this component and makes it self-adaptive. The proposal has been tested by using it to build a CIFAR-10 classifier while forcing the architecture to have fewer than 150,000 parameters, assuming that the resulting network must be deployed in a resource-constrained IoT device. The designs found with and without the predictor achieve validation accuracies of 78.68% and 80.65%, respectively. Both outperform a larger model from the recent literature. The predictor slightly constraints the evolution of solutions, but it approximately halves the computational effort. After extending the test to the CIFAR-100 dataset, the proposal achieves a validation accuracy of 65.43% with 478,006 parameters in its fastest configuration, competing with current results in the literature.}
}
@article{SEDJELMACI2019101970,
title = {An efficient cyber defense framework for UAV-Edge computing network},
journal = {Ad Hoc Networks},
volume = {94},
pages = {101970},
year = {2019},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.101970},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519302136},
author = {Hichem Sedjelmaci and Aymen Boudguiga and Inès Ben Jemaa and Sidi Mohammed Senouci},
keywords = {UAV-Edge computing, Detection, Stackelberg game, Energy consumption, Computation overhead},
abstract = {Mobile Edge Computing (MEC) is usually deployed in energy and delay constrained networks, such as internet of things networks and transportation systems to address the issues of energy consumption, computation capacity and network delay. In this work, we focus on a special case, which is Unmanned Aerial Vehicle Edge Computing (UEC) network. Addressing the security issues in UAV-Edge Computing network is mandatory due to the criticality of UEC services, such as network traffic monitoring, or search and rescue operations. However, cyber defense and protection of UEC network have not yet received sufficient research attention. Thereby, we propose and develop a cyber-defense solution based on a non-cooperative game to protect the UEC from network and offloading attacks, while taking into account nodes’ energy constraints and computation overhead. Simulation results show that, the deployment of our cyber defense system in UEC network requires low energy consumption and low computation overhead to obtain a high protection rate.}
}
@article{YONG2023e13529,
title = {Structure bionic topology design method based on biological unit cell},
journal = {Heliyon},
volume = {9},
number = {2},
pages = {e13529},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13529},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023007363},
author = {Yang Yong and Jiang Xue-tao and Zhu Qi-xin and Lu En-hui and Dong Xin-feng and Li Jing-bin},
keywords = {Biological unit cell, Substructure, Matter element, TRIZ},
abstract = {The mechanical structure topology design based on substructure always adopts the traditional substructure design method, which often comes from the experience and is limited by the inherent or stereotyped design thinking. A substructure design method based on biological unit cell (UC) is proposed, which draws inspiration from the biological efficient load-bearing topology structure. Especially, the thought of the formalized problem-solving of extension matter-element is introduced. Through the matter-element definition of UC substructure, the process model for the structure bionic topology design method based on biological UC is formed, which avoids the random or wild mental stimulation of the structure topology design method based on traditional substructure. In particular, in this proposed method, aiming at the problem about how to achieve the integration of high-efficiency load-bearing advantage of different organisms, furthermore, a biological UC hybridization method based on the principle of inventive problem solving theory (TRIZ) is proposed. The typical case is used to illustrate the process of this method in detail. The results from simulations and experiments both show that: the load-bearing capacity of structure design based on biology UC is improved than the initial design; on this basis, the load-bearing capacity of structure design is improved further through UC hybridization. All these show the feasibility and correctness of the proposed method.}
}
@incollection{CALVERT201369,
title = {Chapter 3 - Social Dimensions of Microbial Synthetic Biology},
editor = {Colin Harwood and Anil Wipat},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {40},
pages = {69-86},
year = {2013},
booktitle = {Microbial Synthetic Biology},
issn = {0580-9517},
doi = {https://doi.org/10.1016/B978-0-12-417029-2.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124170292000030},
author = {Jane Calvert and Emma Frow},
keywords = {Anticipation, Governance, Public engagement, Public good, Responsible innovation, Social dimensions, Science and technology studies, Synthetic biology},
abstract = {In this chapter, we outline a number of foundational ideas that underpin our approach to the study of the social, ethical, legal and philosophical dimensions of synthetic biology. We describe these through a series of important shifts that have taken place over the past few decades of social science research. We suggest a move away from discussions centred around ethical ‘implications’, speculations about the future and concerns about risk, regulation and public acceptance, towards a conversation that talks in terms of social ‘dimensions’, anticipating the future, managing uncertainty, tools of governance and research for the public good. We argue that these seemingly subtle changes in vocabulary open up a new and productive space for thinking about the social dimensions of synthetic biology.}
}
@article{KACZYNSKA20214290,
title = {A new multi-criteria model for ranking chess players},
journal = {Procedia Computer Science},
volume = {192},
pages = {4290-4299},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.205},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101944X},
author = {Aleksandra Kaczyńska and Joanna Kołodziejczyk and Wojciech Sałabun},
keywords = {Chess, MCDA, COMET, players evaluation, decision making},
abstract = {Chess is a very demanding sport as it requires advanced planning and strategic thinking skills. The degree of difficulty of the game also depends on the time allotted for a game, which can range from a few minutes to several tens of minutes. For this reason, the games are divided into several categories: standard, blitz, and bullet. However, as many chess players specialize in only some of the categories, it is difficult to determine the best chess player. It is very important to keep a proper ranking of the players. One way to recognize their achievements is the FIDE (Fédération Internationale des Échecs) titles awarded to the best players. However, there is still the problem of how to determine the best among the Grandmasters. There are many very talented players competing in chess. Creating a single ranking for all types of chess, regardless of the time allotted for the game, is a difficult challenge, as many undeniably outstanding chess players do not specialize in all types. Creating a ranking for only one type would not accurately describe the level of players. Therefore, a ranking was created based on all of them using the COMET method, which belongs to the multi-criteria decision-making methods (MCDA). It is based on fuzzy logic and uses characteristic objects for the assessment of alternatives, which guarantees immunity to the paradox of reversal rankings. Expert opinion was used for correct evaluation. This article presents the ranking of chess players regardless of the type of game they specialize in, to prove that it should be possible to identify the single best chess player.}
}
@article{IMM2012130,
title = {Talking mathematically: An analysis of discourse communities},
journal = {The Journal of Mathematical Behavior},
volume = {31},
number = {1},
pages = {130-148},
year = {2012},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2011.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312311000630},
author = {Kara Imm and Despina A. Stylianou},
keywords = {Discourse, Cognitively demand tasks, Local authority},
abstract = {Discourse has always been at the heart of teaching. In more recent years, the mathematics education community has also turned its attention towards understanding the role of discourse in mathematics teaching and learning. Using earlier classifications of discourse, in this paper, we looked at three types of classrooms: classrooms that engage in high discourse, low discourse and a hybrid of the two. We aimed to understand how the elements of each discourse affected classroom learning, relationships between teachers and students, and participatory structures for students. Overall, our findings highlight the important relationship between cognitively demanding tasks and mathematical talk, and the power of discourse as a “thinking device” as opposed to mere conduit of knowledge. Our work also points to the under-theorized nature of hybrid discourse in mathematics classrooms, thereby providing some directions for pedagogy and further research.}
}
@incollection{KIM2009332,
title = {Spatial Data Mining, Geovisualization},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {332-336},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00526-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080449104005265},
author = {C. Kim},
keywords = {Exploratory spatial data analysis, Geovisualization, Knowledge discovery, Spatial autocorrelation, Spatial data mining, Spatial outliers, Spatial uncertainty, Visual data mining},
abstract = {Geovisualization in spatial data mining is one of the main methods that has recently been the subject of knowledge discovery research in geographic information science. Geovisualization is often referred to as knowledge discovery in that it produces previously unseen patterns from a larger set of data. Due to the increase in geospatial data, any techniques that can shift through large sets of data quickly and efficiently are in high demand. Geovisualization uses visual representations to facilitate thinking, understanding, and knowledge construction about human and physic environments, at geographic scales of measurement. It augments human visual ability in perceiving high complex structures, and detecting, exploring, and exploiting significant patterns. It integrates scientific visualization with traditional cartography, and can be utilized at data pre-processing, spatial data mining, and knowledge construction. The main purpose of geovisualization, however, is on insight rather than maps. Research needs in geovisualization are extensive as follows: geovisulation in spatiotemporal databases, the automated discovery of spatial knowledge, geovisualization in remote-sensing data and spatial object-oriented databases, effective geovisualizations of spatial relationships, and efficient geocomputation.}
}
@article{REVACH2021103229,
title = {Expanding the discussion: Revision of the fundamental assumptions framing the study of the neural correlates of consciousness},
journal = {Consciousness and Cognition},
volume = {96},
pages = {103229},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2021.103229},
url = {https://www.sciencedirect.com/science/article/pii/S1053810021001550},
author = {Daniel Revach and Moti Salti},
keywords = {Consciousness, Awareness, Conscious perception, Unconscious perception, Cognition, Neuroscience, Assumptions, Premises, Neurobiological, Mechanism, Phenomenology},
abstract = {The way one asks a question is shaped by a-priori assumptions and constrains the range of possible answers. We identify and test the assumptions underlying contemporary debates, models, and methodology in the study of the neural correlates of consciousness, which was framed by Crick and Koch’s seminal paper (1990). These premises create a sequential and passive conception of conscious perception: it is considered the product of resolved information processing by unconscious mechanisms, produced by a singular event in time and place representing the moment of entry. The conscious percept produced is then automatically retained to be utilized by post-conscious mechanisms. Major debates in the field, such as concern the moment of entry, the all-or-none vs graded nature, and report vs no-report paradigms, are driven by the consensus on these assumptions. We show how removing these assumptions can resolve some of the debates and challenges and prompt additional questions. The potential non-sequential nature of perception suggests new ways of thinking about consciousness as a dynamic and dispersed process, and in turn about the relationship between conscious and unconscious perception. Moreover, it allows us to present a parsimonious account for conscious perception while addressing more aspects of the phenomenon.}
}
@article{JUST20121292,
title = {Autism as a neural systems disorder: A theory of frontal-posterior underconnectivity},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {36},
number = {4},
pages = {1292-1313},
year = {2012},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2012.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0149763412000334},
author = {Marcel Adam Just and Timothy A. Keller and Vicente L. Malave and Rajesh K. Kana and Sashank Varma},
keywords = {Autism, Connectivity, Underconnectivity, 4CAPS, Computational model, fMRI},
abstract = {The underconnectivity theory of autism attributes the disorder to lower anatomical and functional systems connectivity between frontal and more posterior cortical processing. Here we review evidence for the theory and present a computational model of an executive functioning task (Tower of London) implementing the assumptions of underconnectivity. We make two modifications to a previous computational account of performance and brain activity in typical individuals in the Tower of London task (Newman et al., 2003): (1) the communication bandwidth between frontal and parietal areas was decreased and (2) the posterior centers were endowed with more executive capability (i.e., more autonomy, an adaptation is proposed to arise in response to the lowered frontal-posterior bandwidth). The autism model succeeds in matching the lower frontal-posterior functional connectivity (lower synchronization of activation) seen in fMRI data, as well as providing insight into behavioral response time results. The theory provides a unified account of how a neural dysfunction can produce a neural systems disorder and a psychological disorder with the widespread and diverse symptoms of autism.}
}
@article{DULIC201654,
title = {Designing futures: Inquiry in climate change communication},
journal = {Futures},
volume = {81},
pages = {54-67},
year = {2016},
note = {Modelling and Simulation in Futures Studies},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2016.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016328716000057},
author = {Aleksandra Dulic and Jeannette Angel and Stephen Sheppard},
keywords = {Design inquiry, Designing futures, Climate change communication, Interaction design, 3D game simulation, Transdisciplinary research},
abstract = {There are many barriers and challenges associated with climate change communication focused on promoting community-based action for sustainable futures. Of particular interest is the challenge to embed community perspectives in a communication process of climate change solutions. In this paper we argue that 3D interactive simulations using design inquiry as a development process, can be an effective way of communicating climate change solutions and multiple community responses. People are more likely to engage with the challenges associated with complexity of climate change at the local level when their perspectives are integrated into viable and multiple pathways for action. Future scenarios of change processes situated in local experiences in compelling and interactive ways can be disseminated holistically by making links between scientific, social, political, economic and cultural elements. Design inquiry, as a research approach, integrates contextual knowledge into communication processes to aid imagining, re-thinking and reembodying viable pathways that explore the kinds of futures we collectively envision. This paper examines the contributions that design inquiry makes to climate change communication using an interactive simulation environment for designing futures. We discuss these ideas using the example of the Future Delta project, a virtual 3D environment that enables the exploration and simulation of multiple community-based climate change solutions in the Corporation of Delta, British Columbia.}
}
@article{YANG2023125877,
title = {Multi-parameter controlled mechatronics-electro-hydraulic power coupling electric vehicle based on active energy regulation},
journal = {Energy},
volume = {263},
pages = {125877},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125877},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222027633},
author = {Jian Yang and Bo Liu and Tiezhu Zhang and Jichao Hong and Hongxin Zhang},
keywords = {Mechatronics-electro-hydraulic power coupling, Energy efficiency, K-means clustering analysis, Torque characteristic, Fuzzy control},
abstract = {To enhance the hydraulic energy utilization and torque output stability, a novel mechatronics-electro-hydraulic power coupling electric vehicle (MEH-PCEV) is proposed, integrating a hydraulic pump/motor and a motor into a single device for mutual energy conversion. For MEH-PCEVs equipped with multiple energy sources, a cluster analysis method is used to classify the actual road test dataset and provide guiding ideas for designing rule-based energy management strategies (RB-EMS). Simultaneously, for the output torque anomaly phenomenon in RB-EMS, an inverse thinking fuzzy logic optimization energy management strategy (FLO-EMS) conside ring multi-parameter objectives as input is used to adjust the electromagnetic torque in real-time and reasonably allocate the energy flow. The simulation results demonstrate that the electric and total torque output are more stable. The electric peak torque is relieved, with a corresponding increase in the percentage of electrical energy recovery. With the equal power demand, the overall efficiency of the motor working point is substantially improved, and the energy consumption rate is decreased by 24.42%. Under the active regulation of FLO-EMS, hydraulic energy is more reasonably utilized to meet the vehicle demand power while avoiding energy dissipation and waste. Moreover, this work is expected to reference the development and engineering applications of electro-hydraulic coupling systems.}
}
@article{KONDINSKI20241071,
title = {Hacking decarbonization with a community-operated CreatorSpace},
journal = {Chem},
volume = {10},
number = {4},
pages = {1071-1083},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2023.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S2451929423006198},
author = {Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Andrew Breeson and Yong Ren Tan and Simon Rihm and Jiaru Bai and Markus Kraft},
keywords = {decarbonization, chemistry, knowledge graphs, agents, CreatorSpace},
abstract = {Summary
The pressing challenge of decarbonization encompasses a vast combinatorial space of interlinked technologies, thus necessitating an increased reliance on artificial intelligence (AI)-assisted molecular modeling and data analytics. Our backcasting analysis proposes a future rich in efficient decarbonization technologies, such as sustainable fuels for aviation and shipping, as well as carbon capture and utilization. We then retrace the path to this proposed future with the guidance of two constraints: the maximization of scientists’ creative capacities and the evolution of a world-centric AI. Our exploration leads us to the concept of a “CreatorSpace,” a distributed digital system resembling existing hackerspaces and makerspaces known for accelerating the prototyping of new technologies worldwide. The CreatorSpace serves as a virtual, semantic platform where chemists, engineers, and materials scientists can freely collaborate, integrating chemical knowledge with cross-scale, cross-technology tools, and operations. This streamlined molecular-to-process-design pathway facilitates a diverse array of solutions for decarbonization and other sustainability technologies.}
}
@article{BELLANTE2025104341,
title = {Evaluating the potential of quantum machine learning in cybersecurity: A case-study on PCA-based intrusion detection systems},
journal = {Computers & Security},
pages = {104341},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104341},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825000306},
author = {Armando Bellante and Tommaso Fioravanti and Michele Carminati and Stefano Zanero and Alessandro Luongo},
keywords = {Quantum computing, Quantum machine learning, QML, Evaluation, Framework, Impact, PCA, Principal component analysis, Network intrusion detection, Network security},
abstract = {Quantum computing promises to revolutionize our understanding of the limits of computation, and its implications in cryptography have long been evident. Today, cryptographers are actively devising post-quantum solutions to counter the threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists are innovating quantum protocols to empower defenders. However, the broader impact of quantum computing and quantum machine learning (QML) on other cybersecurity domains still needs to be explored. In this work, we investigate the potential impact of QML on cybersecurity applications of traditional ML. First, we explore the potential advantages of quantum computing in machine learning problems specifically related to cybersecurity. Then, we describe a methodology to quantify the future impact of fault-tolerant QML algorithms on real-world problems. As a case study, we apply our approach to standard methods and datasets in network intrusion detection, one of the most studied applications of machine learning in cybersecurity. Our results provide insight into the conditions for obtaining a quantum advantage and the need for future quantum hardware and software advancements.}
}
@article{YU2024107998,
title = {Bridging the gap: Geometry-centric discriminative manifold distribution alignment for enhanced classification in colorectal cancer imaging},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {107998},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.107998},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524000829},
author = {Weiwei Yu and Nuo Xu and Nuanhui Huang and Houliang Chen},
keywords = {Medical image analysis, Colorectal cancer detection, Domain adaptation, Transfer learning, Manifold learning, Computational oncology},
abstract = {The early detection of colorectal cancer (CRC) through medical image analysis is a pivotal concern in healthcare, with the potential to significantly reduce mortality rates. Current Domain Adaptation (DA) methods strive to mitigate the discrepancies between different imaging modalities that are critical in identifying CRC, yet they often fall short in addressing the complexity of cancer's presentation within these images. These conventional techniques typically overlook the intricate geometrical structures and the local variations within the data, leading to suboptimal diagnostic performance. This study introduces an innovative application of the Discriminative Manifold Distribution Alignment (DMDA) method, which is specifically engineered to enhance the medical image diagnosis of colorectal cancer. DMDA transcends traditional DA approaches by focusing on both local and global distribution alignments and by intricately learning the intrinsic geometrical characteristics present in manifold space. This is achieved without depending on the potentially misleading pseudo-labels, a common pitfall in existing methodologies. Our implementation of DMDA on three distinct datasets, involving several unique DA tasks, has consistently demonstrated superior classification accuracy and computational efficiency. The method adeptly captures the complex morphological and textural nuances of CRC lesions, leading to a significant leap in domain adaptation technology. DMDA's ability to reconcile global and local distributional disparities, coupled with its manifold-based geometrical structure learning, signals a paradigm shift in medical imaging analysis. The results obtained are not only promising in terms of advancing domain adaptation theory but also in their practical implications, offering the prospect of substantially improved diagnostic accuracy and faster clinical workflows. This heralds a transformative approach in personalized oncology care, aligning with the pressing need for early and accurate CRC detection.}
}
@article{YU2023114721,
title = {Numerical investigation of splitter blades on the performance of a forward-curved impeller used in a pump as turbine},
journal = {Ocean Engineering},
volume = {281},
pages = {114721},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114721},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823011058},
author = {He Yu and Tao Wang and Yuancheng Dong and Qiuqin Gou and Lei Lei and Yunqi Liu},
keywords = {Special impeller, Pump as turbine, Splitter blade, Entropy generation, Computational fluid dynamics},
abstract = {Abstract
As a type of economical energy recovery device, pump as turbine (PAT) is generally used in micro-hydropower plants and energy recovery. To study the influence of the splitter blade on a special impeller used in PAT, impellers without and with splitter blades are designed in this paper. The influences of splitter blade on the energy loss, external characteristics and internal flow field distribution of a PAT were simulated via a verified computational fluid dynamics (CFD) method. The consequences present that the shaft power, efficiency, and head corresponding to the BEP of the PAT with splitter blades are 16.4%, 1.3%, and 8.8% better than those of the PAT without splitter blades. The total entropy production of the PAT without splitter blade is higher than that of the PAT with splitter blades at the same flow rate. Adding splitter blade increased the number of effective blades, made the fluid flow more evenly along the impeller flow passage, and reduced the flow separation inside the impeller. This paper displays that adding splitter blades not only obviously increases hydraulic performance under large flow conditions but also significantly widens the high-efficiency range of PATs.}
}
@article{CHEN2022101380,
title = {An automated quality evaluation framework of psychotherapy conversations with local quality estimates},
journal = {Computer Speech & Language},
volume = {75},
pages = {101380},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2022.101380},
url = {https://www.sciencedirect.com/science/article/pii/S0885230822000213},
author = {Zhuohao Chen and Nikolaos Flemotomos and Karan Singla and Torrey A. Creed and David C. Atkins and Shrikanth Narayanan},
keywords = {Cognitive behavioral therapy, Computational linguistics, Hierarchical framework, Local quality estimates},
abstract = {Text-based computational approaches for assessing the quality of psychotherapy are being developed to support quality assurance and clinical training. However, due to the long durations of typical conversation based therapy sessions, and due to limited annotated modeling resources, computational methods largely rely on frequency-based lexical features or dialogue acts to assess the overall session level characteristics. In this work, we propose a hierarchical framework to automatically evaluate the quality of transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the richly dynamic nature of the spoken dialog within a talk therapy session, to evaluate the overall session level quality, we propose to consider modeling it as a function of local variations across the interaction. To implement that empirically, we divide each psychotherapy session into conversation segments and initialize the segment-level qualities with the session-level scores. First, we produce segment embeddings by fine-tuning a BERT-based model, and predict segment-level (local) quality scores. These embeddings are used as the lower-level input to a Bidirectional LSTM-based neural network to predict the session-level (global) quality estimates. In particular, we model the global quality as a linear function of the local quality scores, which allows us to update the segment-level quality estimates based on the session-level quality prediction. These newly estimated segment-level scores benefit the BERT fine-tuning process, which in turn results in better segment embeddings. We evaluate the proposed framework on automatically derived transcriptions from real-world CBT clinical recordings to predict session-level behavior codes. The results indicate that our approach leads to improved evaluation accuracy for most codes when used for both regression and classification tasks.}
}
@article{MONNAHAN2024107024,
title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
journal = {Fisheries Research},
volume = {275},
pages = {107024},
year = {2024},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2024.107024},
url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
author = {Cole C. Monnahan},
keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.}
}
@article{HUGHES2021338,
title = {High-content phenotypic and pathway profiling to advance drug discovery in diseases of unmet need},
journal = {Cell Chemical Biology},
volume = {28},
number = {3},
pages = {338-355},
year = {2021},
issn = {2451-9456},
doi = {https://doi.org/10.1016/j.chembiol.2021.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2451945621001008},
author = {Rebecca E. Hughes and Richard J.R. Elliott and John C. Dawson and Neil O. Carragher},
keywords = {high-content imaging, machine learning, structural similarity, network pharmacology, esophageal cancer, glioblastoma},
abstract = {Summary
Conventional thinking in modern drug discovery postulates that the design of highly selective molecules which act on a single disease-associated target will yield safer and more effective drugs. However, high clinical attrition rates and the lack of progress in developing new effective treatments for many important diseases of unmet therapeutic need challenge this hypothesis. This assumption also impinges upon the efficiency of target agnostic phenotypic drug discovery strategies, where early target deconvolution is seen as a critical step to progress phenotypic hits. In this review we provide an overview of how emerging phenotypic and pathway-profiling technologies integrate to deconvolute the mechanism-of-action of phenotypic hits. We propose that such in-depth mechanistic profiling may support more efficient phenotypic drug discovery strategies that are designed to more appropriately address complex heterogeneous diseases of unmet need.}
}
@incollection{KOSSLYN1988615,
title = {Seeing and Imagining in the Cerebral Hemispheres: A Computational Approach},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {615-642},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50052-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500522},
author = {Stephen M. Kosslyn}
}
@incollection{GIGERENZER2015515,
title = {Computers: Impact on the Social Sciences},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {515-518},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.03202-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868032025},
author = {Gerd Gigerenzer},
keywords = {Charles Babbage, Cognitive revolution, Computer simulation, Division of labor, H.A. Simon, Metaphor, Statistics},
abstract = {The social organization of labor in the nineteenth century served as the model for Babbage's first computers. In the second half of the twentieth century, when working computers were finally constructed and invaded the offices of social scientists, they turned into theories of mind. This mutual inspiration first changed the meaning of calculation and then led to a new understanding of thought as computation based on hierarchically organized subroutines. The computer as a research tool has changed the social sciences in a fundamental way, from enabling large-scale simulations of cognitive and social systems to allowing mindless and mechanical use of statistics.}
}
@article{PAIVIO2014141,
title = {Intelligence, dual coding theory, and the brain},
journal = {Intelligence},
volume = {47},
pages = {141-158},
year = {2014},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0160289614001305},
author = {Allan Paivio},
keywords = {IQ theories, IQ tests, Conceptual/empirical flaw, DCT a unified theory, IQ neuroscience},
abstract = {The distinction between verbal and nonverbal cognitive abilities has been a defining feature of psychometric theories of intelligence for the past century. Despite their popularity, however, these theories have not included functional connections between verbal and nonverbal systems that are necessary if they are to explain performance in intellectual tasks involving interactions between language and nonverbal knowledge. This functional gap limits the capacity of psychometric theories to explain and predict fundamental aspects of individual differences in cognitive abilities that have long been studied experimentally. This article summarizes the history, nature, and possible causes of the problem, and then concludes with a neuroscientifically-enhanced, multimodal dual coding approach to intelligence that focuses on the synergistic interactivity of verbal and nonverbal systems.}
}
@article{BASHIRPOURBONAB2023122642,
title = {In complexity we trust: A systematic literature review of urban quantum technologies},
journal = {Technological Forecasting and Social Change},
volume = {194},
pages = {122642},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122642},
url = {https://www.sciencedirect.com/science/article/pii/S004016252300327X},
author = {Aysan {Bashirpour Bonab} and Maria Fedele and Vincenzo Formisano and Ihor Rudko},
keywords = {Quantum City, Uncertainty, Duality, Parallelism, Quantum mechanics, Systematic literature review},
abstract = {Today's cities are facing increasingly complex challenges. The growing uncertainty and complexity—caused by the unremitted differentiation of social, environmental, and technological orders—call for novel ways of conceptualizing urban reality. Although technology-oriented solutions shape the most efficient strategies to manage complexity in contemporary cities, ensuring an effective transition toward a Quantum City paradigm can grant considerable advantages for city administrators and managers facing looming urban challenges. In this article, we introduce the Quantum City metaphor—grounded in fundamental notions of quantum mechanics—as a new conceptual lens for investigating urban complexity. We then build upon the metaphor, theorizing a set of assumptions grounded in three fundamental concepts of quantum theory: relativity, uncertainty, and duality/parallelism. Finally, we propose an empirical conceptualization of Quantum Cities based on the concrete adoption of quantum technologies to deal with urban complexity. This is achieved through a systematic literature review of scholarly records on quantum technologies in the context of social sciences, emphasizing related urban problematics and challenges. Principal component analysis and agglomerative hierarchical clustering reveal two types of quantum technologies most useful for city planners and managers: quantum communication and quantum computing. Accordingly, we perform a qualitative thematic synthesis of related scholarly records, emphasizing the negative and positive aspects of both types of urban quantum technologies.}
}
@article{HUHN201642,
title = {Cognitive framing in action},
journal = {Cognition},
volume = {151},
pages = {42-51},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010027716300439},
author = {John M. Huhn and Cory Adam Potts and David A. Rosenbaum},
keywords = {Action, Cognitive framing, Heuristics, Object manipulation, Motor control, Bimanual actions},
abstract = {Cognitive framing effects have been widely reported in higher-level decision-making and have been ascribed to rules of thumb for quick thinking. No such demonstrations have been reported for physical action, as far as we know, but they would be expected if cognition for physical action is fundamentally similar to cognition for higher-level decision-making. To test for such effects, we asked participants to reach for a horizontally-oriented pipe to move it from one height to another while turning the pipe 180° to bring one end (the “business end”) to a target on the left or right. From a physical perspective, participants could have always rotated the pipe in the same angular direction no matter which end was the business end; a given participant could have always turned the pipe clockwise or counter-clockwise. Instead, our participants turned the business end counter-clockwise for left targets and clockwise for right targets. Thus, the way the identical physical task was framed altered the way it was performed. This finding is consistent with the hypothesis that cognition for physical action is fundamentally similar to cognition for higher-level decision-making. A tantalizing possibility is that higher-level decision heuristics have roots in the control of physical action, a hypothesis that accords with embodied views of cognition.}
}
@article{NAZI2025100124,
title = {Evaluation of open and closed-source LLMs for low-resource language with zero-shot, few-shot, and chain-of-thought prompting},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100124},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000724},
author = {Zabir Al Nazi and Md. Rajib Hossain and Faisal Al Mamun},
keywords = {Large language models, Zero-shot, Few-shot, Chain-of-thought, GPT-4, Llama 3, Ablation studies, Prompting, LLM reasoning, Low-resource, Bangla},
abstract = {As the global deployment of Large Language Models (LLMs) increases, the demand for multilingual capabilities becomes more crucial. While many LLMs excel in real-time applications for high-resource languages, few are tailored specifically for low-resource languages. The limited availability of text corpora for low-resource languages, coupled with their minimal utilization during LLM training, hampers the models’ ability to perform effectively in real-time applications. Additionally, evaluations of LLMs are significantly less extensive for low-resource languages. This study offers a comprehensive evaluation of both open-source and closed-source multilingual LLMs focused on low-resource language like Bengali, a language that remains notably underrepresented in computational linguistics. Despite the limited number of pre-trained models exclusively on Bengali, we assess the performance of six prominent LLMs, i.e., three closed-source (GPT-3.5, GPT-4o, Gemini) and three open-source (Aya 101, BLOOM, LLaMA) across key natural language processing (NLP) tasks, including text classification, sentiment analysis, summarization, and question answering. These tasks were evaluated using three prompting techniques: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT). This study found that the default hyperparameters of these pre-trained models, such as temperature, maximum token limit, and the number of few-shot examples, did not yield optimal outcomes and led to hallucination issues in many instances. To address these challenges, ablation studies were conducted on key hyperparameters, particularly temperature and the number of shots, to optimize Few-Shot learning and enhance model performance. The focus of this research is on understanding how these LLMs adapt to low-resource downstream tasks, emphasizing their linguistic flexibility and contextual understanding. Experimental results demonstrated that the closed-source GPT-4o model, utilizing Few-Shot learning and Chain-of-Thought prompting, achieved the highest performance across multiple tasks: an F1 score of 84.54% for text classification, 99.00% for sentiment analysis, a F1bert score of 72.87% for summarization, and 58.22% for question answering. For transparency and reproducibility, all methodologies and code from this study are available on our GitHub repository: https://github.com/zabir-nabil/bangla-multilingual-llm-eval.}
}
@article{NORGAARD2023105308,
title = {Linked auditory and motor patterns in the improvisation vocabulary of an artist-level jazz pianist},
journal = {Cognition},
volume = {230},
pages = {105308},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105308},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722002967},
author = {Martin Norgaard and Kevin Bales and Niels Chr. Hansen},
keywords = {Improvisation, Jazz, Music, Audiomotor coupling, Expectation, Entropy},
abstract = {Improvising musicians possess a stored library of musical patterns forming the basis for their improvisations. According to a prominent theoretical framework by Pressing (1988), this library includes linked auditory and motor information. Though examples of libraries of melodic patterns have been shown in extant recordings by some improvising musicians, the underlying motor component has not been experimentally investigated nor related to its auditory counterparts. Here we analyzed a large corpus of ∼100,000 notes from improvisations by one artist-level jazz pianist recorded during 11 live performances with audience. We compared the library identified from these recordings to a control corpus consisting of improvisations by 24 different advanced jazz pianists. In addition to pitch, our recordings included accurate micro-timing and key velocity (i.e., force) data. Following a previously validated procedure, this information was used to identify the underlying motor patterns through correlations between relative timing and velocity between notes in different iterations of the same pitch pattern. A computational model was, furthermore, used to estimate the information content and generated entropy exhibited by recurring pitch patterns with high and low timing and velocity correlations as perceived by a stylistically enculturated expert listener. Though both corpora contained a large number of recurring patterns, the single-player corpus showed stronger evidence that pitch patterns were linked to motor programs in that within-pattern timing and velocity correlations were significantly higher compared to the control corpus. Even when controlling for potentially greater baseline levels of motor self-consistency in the single-player corpus, this effect remained significant for velocity correlations. Amongst recurring 5-tone pitch patterns, those exhibiting more consistent motor schema also used less idiomatic pitch transitions that were both more unexpected and generated more uncertain expectations in enculturated experts than less consistently repeated patterns. Interestingly, we only found partial evidence for fixed pattern boundaries as predicted by the Pressing model and therefore suggest an expanded view in which the beginning and ends of idiomatic audio-motor patterns are not always clear-cut. Our results indicate that the library of melodic patterns may be idiosyncratic to the individual improviser and relies both on motor programming and predictive processing to promote stylistic distinctiveness.}
}
@article{ASOULIN201998,
title = {Phrase structure grammars as indicative of uniquely human thoughts},
journal = {Language Sciences},
volume = {74},
pages = {98-109},
year = {2019},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0388000119300117},
author = {Eran Asoulin},
keywords = {Thought, Cognition, Phrase structure grammars, Chomsky hierarchy, Animal cognition},
abstract = {I argue that the ability to compute phrase structure grammars is indicative of a particular kind of thought. This type of thought that is only available to cognitive systems that have access to the computations that allow the generation and interpretation of the structural descriptions of phrase structure grammars. The study of phrase structure grammars, and formal language theory in general, is thus indispensable to studies of human cognition, for it makes explicit both the unique type of human thought and the underlying mechanisms in virtue of which this thought is made possible.}
}
@article{1985174,
title = {Learning to use a word processor: by doing, by thinking, and by knowing: John M. Carroll and Robert L. Mack: Rep. RC 9481, IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, U.S.A., (July 1982)},
journal = {Decision Support Systems},
volume = {1},
number = {2},
pages = {174},
year = {1985},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(85)90071-5},
url = {https://www.sciencedirect.com/science/article/pii/0167923685900715}
}
@article{SOLIMAN2025100131,
title = {A comparative analysis of encoder only and decoder only models for challenging LLM-generated STEM MCQs using a self-evaluation approach},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100131},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100131},
url = {https://www.sciencedirect.com/science/article/pii/S294971912500007X},
author = {Ghada Soliman and Hozaifa Zaki and Mohamed Kilany},
keywords = {NLP, LLM, SLM, Self-evaluation, MCQ},
abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities in various tasks, including Multiple-Choice Question Answering (MCQA) evaluated on benchmark datasets with few-shot prompting. Given the absence of benchmark Science, Technology, Engineering, and Mathematics (STEM) datasets on Multiple-Choice Questions (MCQs) created by LLMs, we employed various LLMs (e.g., Vicuna-13B, Bard, and GPT-3.5) to generate MCQs on STEM topics curated from Wikipedia. We evaluated open-source LLM models such as Llama 2-7B and Mistral-7B Instruct, along with an encoder model such as DeBERTa v3 Large, on inference by adding context in addition to fine-tuning with and without context. The results showed that DeBERTa v3 Large and Mistral-7B Instruct outperform Llama 2-7B, highlighting the potential of LLMs with fewer parameters in answering hard MCQs when given the appropriate context through fine-tuning. We also benchmarked the results of these models against closed-source models such as Gemini and GPT-4 on inference with context, showcasing the potential of narrowing the gap between open-source and closed-source models when context is provided. Our work demonstrates the capabilities of LLMs in creating more challenging tasks that can be used as self-evaluation for other models. It also contributes to understanding LLMs’ capabilities in STEM MCQs tasks and emphasizes the importance of context for LLMs with fewer parameters in enhancing their performance.}
}
@article{LEALVILLASECA2025105833,
title = {Interpreting Deepkriging for spatial interpolation in geostatistics},
journal = {Computers & Geosciences},
volume = {196},
pages = {105833},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105833},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424003169},
author = {Fabian Leal-Villaseca and Edward Cripps and Mark Jessell and Mark Lindsay},
keywords = {Spatial statistics, Deep learning interpretability, Shapley values, Deepkriging, Batched Shapley},
abstract = {In the current era marked by an unprecedented abundance of data, the usage of conventional methods such as kriging persists in some applications of geostatistics, despite their limitations in adequately capturing the intricate relationships found in contemporary, multivariate datasets. Although deep neural networks (DNNs) have demonstrated remarkable efficacy in capturing complex nonlinear feature relationships across various domains, their success in geostatistical applications has been limited. This can be partly attributed to two significant challenges. Firstly, the opaque nature of these black box models raises concerns about the dependability of their outputs for critical decision-making, as the inner workings of the model remain less interpretable. Secondly, DNNs do not explicitly capture spatial dependencies within data. To address these shortcomings, we employ a methodology to interpret the recently proposed spatial DNNs known as Deepkriging, and we apply it to dry bulk rock density estimation, an often-overlooked aspect in mineral resource estimation. Through our adaptation of Shapley values—Batched Shapley—we overcome significant computational challenges to quantify feature importance for Deepkriging. This approach takes into account feature interactions, which is crucial for DNNs, as they rely on high-order interactions, especially in a complex application like mineral resource estimation. Additionally, we demonstrate in the 3D case that Deepkriging outperforms ordinary kriging and regression kriging in terms of mean squared errors, in both the purely spatial case and in the presence of auxiliary variables. Our study produces the first methodology to interpret Deepkriging, which is suitable for any model with a large number of features; it reaffirms the efficacy of Deepkriging through several comparisons in a 3D application, and most importantly; it underscores the adaptability and broader potential of DNNs to cater to various challenges in geostatistics.}
}
@article{SEISING2006237,
title = {From vagueness in medical thought to the foundations of fuzzy reasoning in medical diagnosis},
journal = {Artificial Intelligence in Medicine},
volume = {38},
number = {3},
pages = {237-256},
year = {2006},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2006.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0933365706001072},
author = {Rudolf Seising},
keywords = {History of science and technology, Fuzzy set theory, Vagueness, Medical diagnoses, Computer assistance, Medical philosophy, System theory},
abstract = {Summary
Objective
This article delineates a relatively unknown path in the history of medical philosophy and medical diagnosis. It is concerned with the phenomenon of vagueness in the physician's “style of thinking” and with the use of fuzzy sets, systems, and relations with a view to create a model of such reasoning when physicians make a diagnosis. It represents specific features of medical ways of thinking that were mentioned by the Polish physician and philosopher Ludwik Fleck in 1926. The paper links Lotfi Zadeh's work on system theory before the age of fuzzy sets with system-theory concepts in medical philosophy that were introduced by the philosopher Mario Bunge, and with the fuzzy-theoretical analysis of the notions of health, illness, and disease by the Iranian-German physician and philosopher Kazem Sadegh-Zadeh.
Material
Some proposals to apply fuzzy sets in medicine were based on a suggestion made by Zadeh: symptoms and diseases are fuzzy in nature and fuzzy sets are feasible to represent these entity classes of medical knowledge. Yet other attempts to use fuzzy sets in medicine were self-contained. The use of this approach contributed to medical decision-making and the development of computer-assisted diagnosis in medicine.
Conclusion
With regard to medical philosophy, decision-making, and diagnosis; the framework of fuzzy sets, systems, and relations is very useful to deal with the absence of sharp boundaries of the sets of symptoms, diagnoses, and phenomena of diseases. The foundations of reasoning and computer assistance in medicine were the result of a rapid accumulation of data from medical research. This explosion of knowledge in medicine gave rise to the speculation that computers could be used for the medical diagnosis. Medicine became, to a certain extent, a quantitative science. In the second half of the 20th century medical knowledge started to be stored in computer systems. To assist physicians in medical decision-making and patient care, medical expert systems using the theory of fuzzy sets and relations (such as the Viennese “fuzzy version” of the Computer-Assisted Diagnostic System, Cadiag, which was developed at the end of the 1970s) were constructed. The development of fuzzy relations in medicine and their application in computer-assisted diagnosis show that this fuzzy approach is a framework to deal with the “fuzzy mode of thinking” in medicine.}
}
@article{SIMS1991383,
title = {Computers and experiments in stress analysis: Eds G. M. Carlomagno and C. A. Brebbia Computational Mechanics Publications, Southampton, UK},
journal = {Engineering Structures},
volume = {13},
number = {4},
pages = {383-384},
year = {1991},
issn = {0141-0296},
doi = {https://doi.org/10.1016/0141-0296(91)90027-A},
url = {https://www.sciencedirect.com/science/article/pii/014102969190027A},
author = {P. Sims}
}
@article{MCLEAN20248,
title = {Autoantibodies against acetylcholine receptors are increased in archived serum samples from patients with schizophrenia},
journal = {Schizophrenia Research},
volume = {267},
pages = {8-13},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424001129},
author = {Ryan Thomas McLean and Elizabeth Buist and David {St. Clair} and Jun Wei},
keywords = {Neurotransmitter receptor, CHRM4, GRM3, CHRNA4, CHRNA5 neuroimmunology},
abstract = {Previous studies have demonstrated that the levels of IgG against neurotransmitter receptors are increased in patients with schizophrenia. Genome-wide association (GWA) studies of schizophrenia confirmed that 108 loci harbouring over 300 genes were associated with schizophrenia. Although the functional implications of genetic variants are unclear, theoretical functional alterations of these genes could be replicated by the presence of autoantibodies. This study examined the levels of plasma IgG antibodies against four neurotransmitter receptors, CHRM4, GRM3, CHRNA4 and CHRNA5, using an in-house ELISA in 247 patients with schizophrenia and 344 non-psychiatric controls. Four peptides were designed based on in silico analysis with computational prediction of HLA-DRB1 restricted and B-cell epitopes. The relationship between plasma IgG levels and psychiatric symptoms, as defined by the Operational Criteria Checklist for Psychotic Illness and Affective Illness (OPCRIT), were examined. The results showed that the levels of plasma IgG against peptides derived from CHRM4 and CHRNA4 were significantly increased in patients with schizophrenia compared with control subjects, but there was no significant association of plasma IgG levels with any symptom domain or any specific symptoms. These preliminary results suggest that CHRM4 and CHRNA4 may be novel targets for autoantibody responses in schizophrenia, although the pathogenic relationship between increased serum autoantibody levels and schizophrenia symptoms remains unclear.}
}
@article{RABOY201736,
title = {An introductory microeconomics in-class experiment to reinforce the marginal utility/price maximization rule and the integration of modern theory},
journal = {International Review of Economics Education},
volume = {24},
pages = {36-49},
year = {2017},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1477388016300494},
author = {David G. Raboy},
keywords = {Experimental economics, Modern microeconomics, Principles classes, Alternative pedagogy},
abstract = {This paper presents an in-class experiment used as a teaching tool in an introductory microeconomics class at the undergraduate college level. It is directed at a critical but challenging concept for principles students—constrained utility maximization and a methodology to intuit preferences. The experimental project is nested in the literature pertaining to the current transition in microeconomic theory motivated by contributions from behavioral economics and transactions-cost economics, among other elements; modern pedagogical models; experimental economics; and experiments as in-classroom teaching tools. While not dispositive as to the general efficacy of in-class experiments, the paper provides an example of an alternative instructional approach which is helpful to principles students under strictly defined protocols. The benefits to students include heightened understanding of the core subject topic, greater interest in the subject matter, a closer connection to real-world economics, and enhanced critical thinking capabilities.}
}
@incollection{NICHELLI2016379,
title = {Chapter 23 - Consciousness and Aphasia},
editor = {Steven Laureys and Olivia Gosseries and Giulio Tononi},
booktitle = {The Neurology of Conciousness (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {379-391},
year = {2016},
isbn = {978-0-12-800948-2},
doi = {https://doi.org/10.1016/B978-0-12-800948-2.00023-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128009482000236},
author = {Paolo Nichelli},
keywords = {language impairment, anarthria, dynamic aphasia, fMRI, neurophysiological measures},
abstract = {Different language impairments allow us to investigate how much the use of language can influence the content of conscious awareness and therefore of thinking and reasoning. Pure anarthria (different from mutism) and verbal short-term memory deficits are associated with an impairment of the effect of covert speech on the content of working memory. Dynamic aphasia impairs the processes involved in the transition between thinking and speaking. However, even the most severe agrammatic patients can retain reasoning about others’ beliefs that according to some theories can only take place in explicit sentences of a natural language. Error monitoring is also impaired in many aphasic patients and in some of them is associated with complete lack of error awareness (anosognosia for aphasia). In patients with impaired consciousness whenever language examination is impossible or unreliable, fMRI and neurophysiological measures such as event-related potentials can provide a window for examining residual language capabilities.}
}
@article{FELSCHE2023101530,
title = {Evidence for abstract representations in children but not capuchin monkeys},
journal = {Cognitive Psychology},
volume = {140},
pages = {101530},
year = {2023},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2022.101530},
url = {https://www.sciencedirect.com/science/article/pii/S0010028522000664},
author = {Elisa Felsche and Patience Stevens and Christoph J. Völter and Daphna Buchsbaum and Amanda M. Seed},
keywords = {Overhypotheses, Abstraction, Generalization, Animal cognition, Computational modeling, Cognitive development},
abstract = {The use of abstract higher-level knowledge (also called overhypotheses) allows humans to learn quickly from sparse data and make predictions in new situations. Previous research has suggested that humans may be the only species capable of abstract knowledge formation, but this remains controversial. There is also mixed evidence for when this ability emerges over human development. Kemp et al. (2007) proposed a computational model of how overhypotheses could be learned from sparse examples. We provide the first direct test of this model: an ecologically valid paradigm for testing two species, capuchin monkeys (Sapajus spp.) and 4- to 5-year-old human children. We presented participants with sampled evidence from different containers which suggested that all containers held items of uniform type (type condition) or of uniform size (size condition). Subsequently, we presented two new test containers and an example item from each: a small, high-valued item and a large but low-valued item. Participants could then choose from which test container they would like to receive the next sample – the optimal choice was the container that yielded a large item in the size condition or a high-valued item in the type condition. We compared performance to a priori predictions made by models with and without the capacity to learn overhypotheses. Children's choices were consistent with the model predictions and thus suggest an ability for abstract knowledge formation in the preschool years, whereas monkeys performed at chance level.}
}
@article{HOLYOAK2021118,
title = {Emergence of relational reasoning},
journal = {Current Opinion in Behavioral Sciences},
volume = {37},
pages = {118-124},
year = {2021},
note = {Same-different conceptualization},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620301716},
author = {Keith J Holyoak and Hongjing Lu},
abstract = {We review recent theoretical and empirical work on the emergence of relational reasoning, drawing connections among the fields of comparative psychology, developmental psychology, cognitive neuroscience, cognitive science, and machine learning. Relational learning appears to involve multiple systems: a suite of Early Systems that are available to human infants and are shared to some extent with nonhuman animals; and a Late System that emerges in humans only, at approximately age three years. The Late System supports reasoning with explicit role-governed relations, and is closely tied to the functions of a frontoparietal network in the human brain. Recent work in cognitive science and machine learning suggests that humans (and perhaps machines) may acquire abstract relations from nonrelational inputs by means of processes that enable re-representation.}
}
@article{WHITACRE201747,
title = {Integer comparisons across the grades: Students’ justifications and ways of reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {47-62},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316301742},
author = {Ian Whitacre and Beti Azuz and Lisa L.C. Lamb and Jessica Pierson Bishop and Bonnie P. Schappelle and Randolph A. Philipp},
keywords = {Integers, Negative numbers, Children’s mathematical thinking, Order, Magnitude},
abstract = {This study is an investigation of students’ reasoning about integer comparisons—a topic that is often counterintuitive for students because negative numbers of smaller absolute value are considered greater (e.g., −5>−6). We posed integer-comparison tasks to 40 students each in Grades 2, 4, and 7, as well as to 11th graders on a successful mathematics track. We coded for correctness and for students’ justifications, which we categorized in terms of 3 ways of reasoning: magnitude-based, order-based, and developmental/other. The 7th graders used order-based reasoning more often than did the younger students, and it more often led to correct answers; however, the college-track 11th graders, who responded correctly to almost every problem, used a more balanced distribution of order- and magnitude-based reasoning. We present a framework for students’ ways of reasoning about integer comparisons, report performance trends, rank integer-comparison tasks by relative difficulty, and discuss implications for integer instruction.}
}
@article{KATONA2023115228,
title = {Accuracy of the robust design analysis for the flux barrier modelling of an interior permanent magnet synchronous motor},
journal = {Journal of Computational and Applied Mathematics},
volume = {429},
pages = {115228},
year = {2023},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2023.115228},
url = {https://www.sciencedirect.com/science/article/pii/S0377042723001723},
author = {Mihály Katona and Miklós Kuczmann and Tamás Orosz},
keywords = {Electrical machines, Optimisation, Finite element method, Robust design analysis, Design of Experiment methods},
abstract = {Mass-produced electrical machines are subjected to manufacturing uncertainties in terms of geometry. A robust design is inevitable to ensure the consistent performance of the electric motor. Some parts of the rotor geometry are often simplified, like the flux barrier at the end of the magnets. This paper presents a design optimisation regarding the torque ripple and the average torque. The aim is to assess the effects of the flux barrier on the main properties of a permanent magnet synchronous motor. Also, robust design analysis is presented on the flux barrier. The computational burden of the robust design analysis is immense, even if uniform uncertainties are assumed. In this case, different Design of Experiment (DoE) methods reduce the number of simulations. The efficiency of the DoE methods is compared in terms of simulation number and extreme value approximation. We found that the Central Composite method is the most accurate, while the Plackett–Burman is the most efficient in this particular case.}
}
@incollection{VALLERO20211,
title = {Chapter 1 - Systems science},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {1-24},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000143},
author = {Daniel A. Vallero},
keywords = {Systems science, Scientific method, Data-intensive discovery, Computational methods, Emergence, Fuzziness, Ecotone, Ecocline, Environmental risk, Biosolids},
abstract = {This chapter manifests how some of the connotations of systems apply to environmental science. The discussion begins with the history and evolution of scientific methods and paradigms, especially the agreement on the scientific method, spatial and temporal complexity, and the first principles of thermodynamics and motion. These are compared to modern environmental applications, including computational methods, governance, and emergence. Scientific and technical communication approaches needed for environmental systems science are described.}
}
@article{LESSARD20071754,
title = {Complexity and reflexivity: Two important issues for economic evaluation in health care},
journal = {Social Science & Medicine},
volume = {64},
number = {8},
pages = {1754-1765},
year = {2007},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606006356},
author = {Chantale Lessard},
keywords = {Health economics, Complexity theory, Reflexivity, Economic evaluation in health care},
abstract = {Economic evaluations are analytic techniques to assess the relative costs and consequences of health care programmes and technologies. Their role is to provide rigorous data to inform the health care decision-making process. Economic evaluation may oversimplify complex health care decisions. These analyses often ignore important health consequences, contextual elements, relationships or other relevant modifying factors, which might not be appropriate in a multi-objective, multi-stakeholder issue. One solution would be to develop a new paradigm based on the issues of perspective and context. Complexity theory may provide a useful conceptual framework for economic evaluation in health care. Complexity thinking develops an awareness of issues including uncertainty, contextual issues, multiple perspectives, broader societal involvement, and transdisciplinarity. This points the economic evaluation field towards an accountability and epistemology based on pluralism and uncertainty, requiring new forms of lay-expert engagement and roles of lay knowledge into decision-making processes. This highlights the issue of reflexivity in economic evaluation in health care. A reflexive approach would allow economic evaluators to analyze how objective structures and subjective elements influence their practices. In return, this would point increase the integrity and reliability of economic evaluations. Reflexivity provides opportunities for critically thinking about the organization and activities of the intellectual field, and perhaps the potential of moving in new, creative directions. This paper argues for economic evaluators to have a less positivist attitude towards what is useful knowledge, and to use more imagination about the data and methodologies they use.}
}
@article{GOTLIEB2025104126,
title = {Pathology Education for Undergraduate and Graduate Students: It’s Not just for Clinical Trainees},
journal = {Laboratory Investigation},
pages = {104126},
year = {2025},
issn = {0023-6837},
doi = {https://doi.org/10.1016/j.labinv.2025.104126},
url = {https://www.sciencedirect.com/science/article/pii/S0023683725000364},
author = {Avrum I. Gotlieb and Richard N. Mitchell}
}
@incollection{KUMARI2025219,
title = {Chapter Nine - Harnessing artificial intelligence in identifying and isolation of marine peptides},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {56},
pages = {219-242},
year = {2025},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 2},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000461},
author = {Priyanshi Kumari and Bhavya Gaur and Vaibhav Mishra},
keywords = {Marine peptides, Artificial intelligence, Therapeutic values, Machine learning, Deep learning model, AMP's discovery},
abstract = {Marine ecosystem is a vast and relatively unexplored environment, where innumerable resources reside, including marine microbes, animals, algae, and other organisms' those have potential to produce different bioactive microbial peptides. Moreover, marine peptides are structurally unique and known for their exceptional bioactivity, with minimal to no harmful side effects. These bioactive peptides, isolated from marine sources, exhibit various properties, including antimicrobial, antiviral, anti-obesity, antioxidant, anti-inflammatory, and more hence, are deemed as future drugs. Furthermore, discovery of potential active peptides is exorbitant and laborious with traditional methods. Whereas, advanced computational techniques like Artificial Intelligence (AI) and their prime models make easier in the prediction and detection of important marine peptides. In this chapter we are highlighting modern AI based Machine Learning (ML) and Deep Learning (DL) models including k-Nearest Neighbour (kNN), Random Forest (RF), Artificial Neural Networks (ANNs) as ML, Fuzzy Logic or Adaptive Neuro-Fuzzy Inference System (ANFIS), Support Vector Machine (SVMs), and many more other DL models. Moreover, employing these advanced AI models to ease the isolation and identification of the bioactive microbial peptides from marine environments.}
}
@article{AMALINA2023e19539,
title = {Cognitive and socioeconomic factors that influence the mathematical problem-solving skills of students},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19539},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19539},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023067476},
author = {Ijtihadi Kamilia Amalina and Tibor Vidákovich},
keywords = {External factor, Internal factor, Mathematics thinking skills, Middle-school students, Structural equation modeling},
abstract = {Mathematical problem-solving is necessary to encounter professional, 21st-century, and everyday challenges. The relevant context of mathematical problem-solving is related to science, which is presented using natural language. Mathematical problem-solving requires both mathematical skills and nonmathematical skills, e.g., science knowledge and text comprehension skills. Thus, several internal and external factors affect success in mathematical problem-solving. In this study, we investigated the cognitive (i.e., mathematics domain-specific prior knowledge (DSPK), science background knowledge, and text comprehension skills) and socioeconomic status (SES) (i.e., parents' educational level and family income) factors that affect students' mathematical problem-solving skills. The data considered in this study included tests, documents, and a questionnaire from grade seven to nine students (n = 1067). In addition, a theoretical model was constructed using structural equation modeling. We found that this model was close to satisfying the critical values of fit indices. The model was then modified by deleting the nonsignificant paths, and the modified model exhibited a better fit. We found that most of the exploratory variables directly affected mathematical problem-solving skills, with the exception of the parents' educational levels. The strongest factor was mathematics DSPK. Both the father’s and mother’s educational levels indirectly influenced mathematical problem-solving skills through family income. In addition, text comprehension skills indirectly impacted mathematical problem-solving skills with science background knowledge acting as a mediator.}
}
@article{LOPEZ2023104398,
title = {Facets of social problem-solving as moderators of the real-time relation between social rejection and negative affect in an at-risk sample},
journal = {Behaviour Research and Therapy},
volume = {169},
pages = {104398},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104398},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001468},
author = {Roberto López and Christianne Esposito-Smythers and Annamarie B. Defayette and Katherine M. Harris and Lauren F. Seibel and Emma D. Whitmyre},
keywords = {Social problem-solving, Social rejection, Negative affect},
abstract = {Social rejection predicts negative affect, and theoretical work suggests that problem-solving deficits strengthen this relation in real-time. Nevertheless, few studies have explicitly tested this relation, particularly in samples at risk for suicide. This may be particularly important as social rejection and negative affect are significant predictors of suicide. The aim of the current study was to examine whether cognitive (i.e., perceiving problems as threats) and behavioral (i.e., avoidance) facets of problem-solving deficits moderated the real-time relation between social rejection and negative affect. The sample consisted of 49 young adults with past-month suicidal ideation. Demographic information, social problem-solving deficits, as well as depressive/anxiety symptoms and stress levels were assessed at baseline. Social rejection and negative affect were assessed using ecological momentary assessment over the following 28 days. Dynamic structural equation modeling was used to assess relations among study variables. After accounting for depressive/anxiety symptoms, stress levels, sex, and age, only avoidance of problems bolstered the real-time positive relation between social rejection severity and negative affect (b = 0.04, 95% credibility interval [0.003, 0.072]). Individuals with suicidal ideation who possess an avoidant problem-solving style may be particularly likely to experience heightened negative affect following social rejection and may benefit from instruction in problem-solving skills.}
}
@article{WU2025104172,
title = {TrustCNAV: Certificateless aggregate authentication of civil navigation messages in GNSS},
journal = {Computers & Security},
volume = {148},
pages = {104172},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104172},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004772},
author = {Zhijun Wu and Yun Bai and Yuan Zhang and Liang Liu and Meng Yue},
keywords = {Satellite navigation, Spoofing attacks, Elliptic curve, Aggregate authentication, Authentication protocol},
abstract = {The Global Navigation Satellite System (GNSS) is capable of accurate positioning because it can provide high-precision data. These data are transmitted to the receiver in the form of navigation messages, called civil navigation messages (CNAV). As it is transmitted in an open, transparent environment without data integrity protection mechanisms and secure data transmission measures, the CNAV is suspected to spoofing attacks. In 2023, the OPSGROUP has received approximately 50 reports of GPS spoofing activity. A spoofed plane's navigation system will show it as being in a different place - a security risk if a jet is guided to fly into a hostile country's airspace. To prevent the forging of GNSS positioning data by spoofing attacks targeting CNAV, we propose a certificateless aggregation authentication for CNAV by using the elliptic curve discrete logarithm problem and the combination of the GNAV structural characteristics, called TrustCNAV. Security proof and performance analysis indicate that this authentication scheme can resist spoofing attacks and ensure data security of CNAV, also it avoids pairing operations with high computational complexity, thus meeting security requirements without causing too much time and communication consumption.}
}
@article{BUI1997575,
title = {Computational modelling of thermophysical processes in the light metals industry},
journal = {Revue Générale de Thermique},
volume = {36},
number = {8},
pages = {575-591},
year = {1997},
issn = {0035-3159},
doi = {https://doi.org/10.1016/S0035-3159(97)89985-0},
url = {https://www.sciencedirect.com/science/article/pii/S0035315997899850},
author = {Rung T Bui},
keywords = {computer modelling, light metals, thermophysical processes, electrolysis, casting, furnaces, modèles numériques, métaux légers, procédés thermophysiques, électrolyse, coulée, fours},
abstract = {This survey focuses on the aluminium industry, mostly on process aspects as opposed to metallurgical aspects. It covers recent work on process models involving fluid flow and heat transfer, and extends to all important categories of processes encountered in the primary aluminium industry, from raw materials and reduction to cast shop and recycling. This includes a wide variety of processes from precipitators, calciners, rotary kilns, baking furnaces, reduction cells, casting and mixing furnaces to recycling furnaces and metal filtration. A review is carried out on the modelling work, the applications, and the needs expressed not only in analysis and design but also in process control, optimization and supervision, as well as operator training. A summary is given of the problems perceived, mainly in the field of model parameters and model validation. Indications on future trends are also given. Conclusions are drawn from the survey of this fast-expanding body of knowledge that suggests tough challenges as well as unprecedented opportunities. Suggestions are made as to how some of those challenges could be met.
Résumé
Cette synthèse concerne l'industrie de l'aluminium et s'intéresse surtout aux procédés de fabrication, par opposition aux aspects métallurgiques. Elle couvre les travaux récents sur les modèles de procédés impliquant les écoulements et le transfert de chaleur. Elle inclut toutes les catégories de procédés rencontrées dans l'industrie de l'aluminium de première fusion, allant des matières premières à la réduction, la coulée et le recyclage. On y retrouve les précipitateurs, les calcinateurs, les fours rotatifs, les fours de cuisson, les cuves d'électrolyse, les fours de coulée ou de mélange, les fours de recyclage, ainsi que la filtration du métal. Les travaux de modélisation et leurs applications sont brièvement énumérés, les besoins exprimés, concernant non seulement l'analyse et la conception des procédés, mais aussi le contrôle, sont examinés, ainsi que des aspects touchant à l'optimisation, à la supervision et à la formation du personnel. Un résumé est fait des problèmes, notamment en ce qui a trait à la détermination des paramètres de modélisation et à la validation des modèles. On tente de dégager les tendances d'avenir. Des conclusions sont tirées de cette étude, qui semble mettre en lumière des défis de taille aussi bien que des opportunités sans précédent ; quelques suggestions visant à relever certains de ces défis sont proposées.}
}
@article{CUI2019305,
title = {Developing reflection analytics for health professions education: A multi-dimensional framework to align critical concepts with data features},
journal = {Computers in Human Behavior},
volume = {100},
pages = {305-324},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219300718},
author = {Yi Cui and Alyssa Friend Wise and Kenneth L. Allen},
keywords = {Reflection, Learning analytics, Natural language processing, Professional education, Dental education, Health professions education},
abstract = {Reflection is a key activity in self-regulated learning (SRL) and a critical part of health professions education that supports the development of effective lifelong-learning health professionals. Despite widespread use and plentiful theoretical models, empirical understanding of and support for reflection in health professions education remains limited due to simple manual assessment and rare feedback to students. Recent moves to digital reflection practices offer opportunities to computationally study and support reflection as a part of SRL. The critical task in such an endeavor, and the goal of this paper, is to align high-level reflection qualities that are valued conceptually with low-level features in the data that are possible to extract computationally. This paper approaches this goal by (a) developing a unified framework for conceptualizing reflection analytics in health professions education and (b) empirically examining potential data features through which these elements can be assessed. Synthesizing the prior literature yields a conceptual framework for health professions reflection comprised of six elements: Description, Analysis, Feelings, Perspective, Evaluation, and Outcome. These elements then serve as the conceptual grounding for the computational analysis in which 27 dental students’ reflections (in six reflective statement types) over the course of 4 years were examined using selected LIWC (Linguistic Inquiry and Word Count) indices. Variation in elements of reflection across students, years, and reflection-types supports use of the multi-dimensional analysis framework to (a) increase precision of research claims; (b) evaluate whether reflection activities are engaged in as intended; and (c) diagnose aspects of reflection in which specific students need support. Implications for the development of health professions reflection analytics that can contribute to SRL and promising areas for future research are discussed.}
}
@article{RICHEY2014857,
title = {A Complex Sociotechnical Systems Approach to Provisioning Educational Policies for Future Workforce},
journal = {Procedia Computer Science},
volume = {28},
pages = {857-864},
year = {2014},
note = {2014 Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.03.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914001653},
author = {Michael Richey and Marcus Nance and Leroy Hanneman and William Hubbard and Azad M. Madni and Marc Spraragen},
keywords = {Socio-technical Systems, Systems Engineering Education, Engineering, Worldbuilding and Workforce Development},
abstract = {Reforming the U.S. educational system and workforce is a national challenge. Both industry leaders 30 and academics 2,28 concur that improving the quality, quantity [and alignment] of U.S STEM graduates are national imperatives. Models of the U.S. educational system, using complex sociotechnical systems’ approaches and tools that instill systems thinking, offer a holistic perspective to the educational and workforce challenges we face as a nation and allow us to identify and understand challenges associated with workforce preparedness, and increasing the number and technical excellence of STEM graduates 9,13,29. These models represent a sociotechnical system of systems with various sub-systems, each one representing an inherently complex and interdisciplinary problem of maintaining bi-directional, non-linear feedback relationships between one another. Each system involves multiple disparate stakeholders that need to collaborate within a time-and resource-intensive process while embedded in a larger sociotechnical system, aligned with the people, ideas, and support required to support desired global outcomes, of the system of systems, society and industry in particular11.}
}
@article{BRODO202025,
title = {A Constraint-based Language for Multiparty Interactions},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {351},
pages = {25-50},
year = {2020},
note = {Proceedings of LSFA 2020, the 15th International Workshop on Logical and Semantic Frameworks, with Applications (LSFA 2020)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571066120300396},
author = {Linda Brodo and Carlos Olarte},
keywords = {Concurrency theory, constraints, multiparty interactions},
abstract = {Multiparty interactions are common place in today's distributed systems. An agent usually communicates, in a single session, with other agents to accomplish a given task. Take for instance an online transaction including the vendor, the client, the credit card system and the bank. When specifying this kind of system, we probably observe a single transaction including several (binary) communications leading to changes in the state of all the involved agents. Multiway synchronization process calculi, that move from a binary to a multiparty synchronization discipline, have been proposed to formally study the behavior of those systems. However, adopting models such as Bodei, Brodo, and Bruni's Core Network Algebra (CNA), where the number of participants in an interaction is not fixed a priori, leads to an exponential blow-up in the number of states/behaviors that can be observed from the system. In this paper we explore mechanisms to tackle this problem. We extend CNA with constraints that declaratively allow the modeler to restrict the interaction that should actually happen. Our extended process algebra, called CCNA, finds application in balancing the interactions in a concurrent system, leading to a simple, deadlock-free and fair solution for the Dinning Philosopher problem. Our definition of constraints is general enough and it offers the possibility of accumulating costs in a multiparty negotiation. Hence, only computations respecting the thresholds imposed by the modeler are observed. We use this machinery to neatly model a Service Level Agreement protocol. We develop the theory of CCNA including its operational semantics and a behavioral equivalence that we prove to be a congruence. We also propose a prototypical implementation that allows us to verify, automatically, some of the systems explored in the paper.}
}
@article{GOLDMAN2025104323,
title = {The Value of Real-time Automated Explanations in Stochastic Planning},
journal = {Artificial Intelligence},
pages = {104323},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104323},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000426},
author = {Claudia V. Goldman and Ronit Bustin and Wenyuan Qi and Zhengyu Xing and Rachel McPhearson-White and Sally Rogers},
keywords = {Explainable AI, Decision-Making, Human-Computer Interaction},
abstract = {Recently, we are witnessing an increase in computation power and memory, leading to strong AI algorithms becoming applicable in areas affecting our daily lives. We focus on AI planning solutions for complex, real-life decision-making problems under uncertainty, such as autonomous driving. Human trust in such AI-based systems is essential for their acceptance and market penetration. Moreover, users need to establish appropriate levels of trust to benefit the most from these systems. Previous studies have motivated this work, showing that users can benefit from receiving (handcrafted) information about the reasoning of a stochastic AI planner, for example, controlling automated driving maneuvers. Our solution to automating these hand-crafted notifications with explainable AI algorithms, XAI, includes studying: (1) what explanations can be generated from an AI planning system, applied to a real-world problem, in real-time? What is that content that can be processed from a planner's reasoning that can help users understand and trust the system controlling a behavior they are experiencing? (2) when can this information be displayed? and (3) how shall we display this information to an end user? The value of these computed XAI notifications has been assessed through an online user study with 800 participants, experiencing simulated automated driving scenarios. Our results show that real time XAI notifications decrease significantly subjective misunderstanding of participants compared to those that received only a dynamic HMI display. Also, our XAI solution significantly increases the level of understanding of participants with prior ADAS experience and of participants that lack such experience but have non-negative prior trust to ADAS features. The level of trust significantly increases when XAI was provided to a more restricted set of the participants, including those over 60 years old, with prior ADAS experience and non-negative prior trust attitude to automated features.}
}
@article{RASS202385,
title = {Adaptive dynamical systems modelling of transformational organizational change with focus on organizational culture and organizational learning},
journal = {Cognitive Systems Research},
volume = {79},
pages = {85-108},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000049},
author = {Lars Rass and Jan Treur and Wioleta Kucharska and Anna Wiewiora},
keywords = {Transformational Change, Organizational Culture, Organizational Learning, Safety Culture},
abstract = {Transformative Organizational Change becomes more and more significant both practically and academically, especially in the context of organizational culture and learning. However computational modeling and a formalization of organizational change and learning processes are still largely unexplored. This paper aims to provide an adaptive network model of transformative organizational change and translate a selection of organizational learning and change processes into computationally modelled processes. Additionally, it sets out to connect the dynamic systems view of organizations to self-modelling network models. The creation of the model and the implemented mechanisms of organizational processes are based on extrapolations of an extensive literature study and grounded in related work in this field, and then applied to a specified hospital-related case scenario in the context of safety culture. The model was evaluated by running several simulations and variations thereof. The results of these were investigated by qualitative analysis and comparison to expected emergent behaviour based on related available academic literature. The simulations performed confirmed the occurrence of an organizational transformational change towards a constant learning culture by offering repeated and effective learning and changes to organizational processes. Observations about various interplays and effects of the mechanism have been made, and they exposed that acceptance of mistakes as a part of learning culture facilitates transformational change and may foster sustainable change in the long run. Further, the model confirmed that the self-modelling network model approach applies to a dynamic systems view of organizations and a systems perspective of organizational change. The created model offers the basis for the further creation of self-modelling network models within the field of transformative organizational change and the translated mechanisms of this model can further be extracted and reused in a forthcoming academic exploration of this field.}
}
@article{WU2025111756,
title = {A rapid indoor 3D wind field prediction model based on conditional generative adversarial networks},
journal = {Journal of Building Engineering},
volume = {100},
pages = {111756},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.111756},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224033242},
author = {Yaqi Wu and Xiaoqian Li and Xing Zheng and Chenxi Lei and Ye Yuan and Zhen Han and Gang Liu},
keywords = {3D wind field, Fast prediction, Pix2pix, Image encoding},
abstract = {The prediction of building performance during the early design phase is essential for architects and engineers. Given the complex nature of parameter inputs and the need for time efficiency, surrogate models have become a preferred method for predicting building performance. However, most surrogate models for indoor airflow could not predict the wind flow field information in three-dimensional (3D) space (named 3D wind field). The few advanced 3D data prediction models are often computationally expensive. This paper proposes a surrogate model based on Conditional Generative Adversarial Networks for the prediction of indoor 3D wind fields under natural ventilation. The core innovation lies in compressing indoor 3D wind field information into 2D planes via image encoding and subsequently obtaining wind field maps of arbitrary planes through data post-processing. By taking prefabricated houses as the case study, a database is constructed and the model is trained to predict the wind field at any cross-section within the space. The resulting surrogate model can generate predictions within a 3–5 s timeframe. To evaluate the accuracy of the model prediction, 21 testing planes were selected. Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) were used to assess the numerical accuracy, and Structure Similarity Index Measure (SSIM) was used to comprehensively evaluate the visualization results of the wind field images. The results indicate that the model exhibits outstanding prediction performance for the planes, with an MAE of 0.1233, a MAPE of 12.20 %, and an SSIM of 0.9492 on the test set. Compared to simulation methods, this approach can improve prediction speed by 350 times-450 times, significantly enhancing the efficiency of obtaining 3D wind fields during the early design stages.}
}
@article{SHER1992505,
title = {A computational normative theory of scientific evidence},
journal = {International Journal of Approximate Reasoning},
volume = {6},
number = {4},
pages = {505-524},
year = {1992},
issn = {0888-613X},
doi = {https://doi.org/10.1016/0888-613X(92)90002-H},
url = {https://www.sciencedirect.com/science/article/pii/0888613X9290002H},
author = {David B. Sher},
keywords = {interval probability, evidence combination, experimental evidence, probabilistic logic, statistical inference},
abstract = {A scientific reasoning system makes decisions using objective evidence in the form of independent experimental trials, propositional axioms, and constraints on the probabilities of events. I propose a collection of algorithms that derive probability intervals and estimate conditional probabilities from objective evidence in those forms. This reasoning system can manage uncertainty about data and rules in a rule-based expert system. I expect that the system will be particularly applicable to diagnosis and analysis in domains with a wealth of experimental evidence such as medicine. The algorithms currently apply to systems with arbitrary amounts of experimental evidence but with less than 20 variables. I discuss limitations of this solution and propose future directions for this research. This work can be considered a generalization of Nilsson's “probabilistic logic” to intervals and experimental observations.}
}
@article{ROSSITER20237555,
title = {A suite of MATLAB livescript files to support learning of elementary control and feedback concepts},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {7555-7560},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.657},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323010315},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, independent learning, visualisation, livescripts},
abstract = {This paper builds on a body of work in the community which is focussed on sharing learning and teaching resources, especially those which might support a first course in control. Here attention is given to some of the mathematical, analytical and numerical computations which are required to support simple system and feedback analysis and design. The aim is to provide resources which allow students to focus on core concepts and understanding so that the numerical computations are not an obstacle to their investigations. More specifically, this paper focuses on a number of MATLAB livescript files which have been produced to help students visualise the impact of parameter and design choices on system behaviour, while simultaneously empowering them to understand the source code and thus upskill them for the future. The paper gives an overview of the livescripts available so users can decide whether these could be useful in their own context; all are freely available on the author's website (Rossiter, 2021).}
}
@article{HAPPE2025112240,
title = {Authentic interdisciplinary online courses for alternative pathways into computer science},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112240},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112240},
url = {https://www.sciencedirect.com/science/article/pii/S016412122400284X},
author = {Lucia Happe and Kai Marquardt},
keywords = {Interdisciplinary teaching, e-learning, Interest, Engagement, Diversity, Gender, Computer science education},
abstract = {The field of computer science (CS) is facing a crucial challenge in broadening participation and embracing diversity, especially among underrepresented gender groups. The presented interdisciplinary educational program is an efficient response to this challenge, designed to catalyze diversity in CS through engagement with complex, interest-driven problems. This paper outlines the program’s structure, elucidates the pedagogical underpinnings, and reflects on the emergent challenges and opportunities. We delve into how the fusion of CS with other academic disciplines can allure a more varied demographic, emphasizing the engagement of female high school students—a demographic pivotally positioned yet significantly untapped in CS. Through a systematic survey analysis, we measure the program’s efficacy in increasing interest in CS and in cultivating an appreciation for its application in addressing real-world, cross-disciplinary challenges. Our findings affirm the program’s success in bridging the engagement gap by leveraging students’ intrinsic interests, thus charting alternative pathways into the CS field. These insights underscore the critical role of interdisciplinary approaches, establishing a new standard for transformative CS educational methods.}
}
@article{DOSHI2020103202,
title = {Recursively modeling other agents for decision making: A research perspective},
journal = {Artificial Intelligence},
volume = {279},
pages = {103202},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.103202},
url = {https://www.sciencedirect.com/science/article/pii/S000437021930027X},
author = {Prashant Doshi and Piotr Gmytrasiewicz and Edmund Durfee},
keywords = {Decision theory, Game theory, Hierarchical beliefs, Multiagent systems, Recursive modeling, Theory of mind},
abstract = {Individuals exhibit theory of mind, attributing beliefs, intent, and mental states to others as explanations of observed actions. Dennett's intentional stance offers an analogous abstraction for computational agents seeking to understand, explain, or predict others' behaviors. These recognized theories provide a formal basis to ongoing investigations of recursive modeling. We review and situate various frameworks for recursive modeling that have been studied in game- and decision- theories, and have yielded methods useful to AI researchers. Sustained attention given to these frameworks has produced new analyses and methods with an aim toward making recursive modeling practicable. Indeed, we also review some emerging uses and the insights these yielded, which are indicative of pragmatic progress in this area. The significance of these frameworks is that higher-order reasoning is critical to correctly recognizing others' intent or outthinking opponents. Such reasoning has been utilized in academic, business, military, security, and other contexts both to train and inform decision-making agents in organizational and strategic contexts, and also to more realistically predict and best respond to other agents' intent.}
}
@article{MILANEZ200793,
title = {A new method for real time computation of power quality indices based on instantaneous space phasors},
journal = {Electric Power Systems Research},
volume = {77},
number = {1},
pages = {93-98},
year = {2007},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2006.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378779606000265},
author = {Dalgerti L. Milanez and Rade M. Ciric},
keywords = {Instantaneous space phasors, Instantaneous complex power, Buchholz–Goodhue effective apparent power, Power definitions, Unbalance, Dispersed generators},
abstract = {One of the important issues about using renewable energy is the integration of dispersed generation in the distribution networks. Previous experience has shown that the integration of dispersed generation can improve voltage profile in the network, decrease loss, etc. but can create safety and technical problems as well. This work report the application of the instantaneous space phasors and the instantaneous complex power in observing performances of the distribution networks with dispersed generators in steady state. New IEEE apparent power definition, the so-called Buchholz–Goodhue effective apparent power, as well as new proposed power quality (oscillation) index in the three-phase distribution systems with unbalanced loads and dispersed generators, are applied. Results obtained from several case studies using IEEE 34 nodes test network are presented and discussed.}
}
@article{YUKSEL2025100890,
title = {Transformation of labor: Educational robotics coding in elementary schools for 21st century skills},
journal = {Entertainment Computing},
volume = {52},
pages = {100890},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100890},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002581},
author = {Akça Okan Yüksel and Bilal Atasoy and Selçuk Özdemir},
keywords = {21st century skills, Educational robotics coding, Elementary school students, Sociocultural, Cognitive, Affective},
abstract = {This study aims to examine the effects of educational robotics activities on students’ 21st century skills. In the study, explanatory mixed method design was used. As a quantitative data collection tool, the 21st century skills scale was utilized [47]. In addition to quantitative data, qualitative data were collected from the teachers and students through semi-structured interviews. Within the scope of the research, students participated in robotic design courses with Arduino over the learning management system for ten weeks and participated in a competition with their products at the end of the activity. The activity was held with the participation of 62 students and 10 teachers. The findings of the study showed that educational robotic activities caused a significant increase in the affective domain of the students. While it did not cause any significant increase in the cognitive and sociocultural domains, the average scores of students increased on post-tests compared to the pretests for these two domains. In addition, students’ 21st century skills did not differ according to gender. Although there is no statistically significant difference in pretest–posttest results due to grade level, an increase was observed in posttest averages at each grade level. The observed increase in the post-tests, although not statistical, reveals the positive effect of educational robotic coding in terms of students’ 21st century skills. To support the results of the quantitative data, the analysis of the qualitative data revealed a consensus of both teachers and students on the contribution of such applications to the advancement of contemporary skills. In conclusion, the results of this study show that educational robotic coding can be used to develop 21st century skills of elementary school students.}
}
@article{MORETTI1980145,
title = {Computational aerodynamics using mini computers},
journal = {Computers & Fluids},
volume = {8},
number = {1},
pages = {145-153},
year = {1980},
note = {Special Issue: Computers in Aerodynamics},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(80)90037-7},
url = {https://www.sciencedirect.com/science/article/pii/0045793080900377},
author = {Gino Moretti},
abstract = {The importance of minicomputers as a research tool in gasdynamics is explained, and a few examples are given to show their efficiency.}
}
@article{SMIRNOV20142507,
title = {Domain Ontologies Integration for Virtual Modelling and Simulation Environments},
journal = {Procedia Computer Science},
volume = {29},
pages = {2507-2514},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.234},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004116},
author = {Pavel A. Smirnov and Sergey V. Kovalchuk and Alexey V. Dukhanov},
keywords = {Virtual simulation objects, semantic technologies, computational experiment, knowledge base},
abstract = {This paper presents a model of semantic ontologies integration into workflow co mposition design process via Virtual Simu lation Objects (VSO) concept and technology. Doma in knowledge distributed over open linked data sources may be usefully applied for new VSO-images design and used for organization co mputational-intensive simulation e xpe riments. In this paper we describe the VSO- architecture e xtended with novel functionality regarding integration with lin ked open data sources. We also provide a computational-scientific e xa mp le of do ma in-specific use-case offering a solution for some public-transportation domain problem.}
}
@article{CARLSON201888,
title = {Ghosts in machine learning for cognitive neuroscience: Moving from data to theory},
journal = {NeuroImage},
volume = {180},
pages = {88-100},
year = {2018},
note = {New advances in encoding and decoding of brain signals},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917306663},
author = {Thomas Carlson and Erin Goddard and David M. Kaplan and Colin Klein and J. Brendan Ritchie},
keywords = {Multivariate pattern analysis, Brain decoding, Exploratory methods, fMRI, Magnetoencephalography},
abstract = {The application of machine learning methods to neuroimaging data has fundamentally altered the field of cognitive neuroscience. Future progress in understanding brain function using these methods will require addressing a number of key methodological and interpretive challenges. Because these challenges often remain unseen and metaphorically “haunt” our efforts to use these methods to understand the brain, we refer to them as “ghosts”. In this paper, we describe three such ghosts, situate them within a more general framework from philosophy of science, and then describe steps to address them. The first ghost arises from difficulties in determining what information machine learning classifiers use for decoding. The second ghost arises from the interplay of experimental design and the structure of information in the brain – that is, our methods embody implicit assumptions about information processing in the brain, and it is often difficult to determine if those assumptions are satisfied. The third ghost emerges from our limited ability to distinguish information that is merely decodable from the brain from information that is represented and used by the brain. Each of the three ghosts place limits on the interpretability of decoding research in cognitive neuroscience. There are no easy solutions, but facing these issues squarely will provide a clearer path to understanding the nature of representation and computation in the human brain.}
}
@article{KARVONEN2023101166,
title = {Fundamental concepts of cognitive mimetics},
journal = {Cognitive Systems Research},
volume = {82},
pages = {101166},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101166},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001006},
author = {Antero Karvonen and Tuomo Kujala and Tommi Kärkkäinen and Pertti Saariluoma},
keywords = {AI design, Cognitive mimetics, Design, Artificial intelligence},
abstract = {The rapid development and widespread adoption of Artificial Intelligence (AI) technologies have made the development of AI-specific design methods an important topic to advance. In recent decades, the centre of gravity in AI has shifted away from cognitive science and related fields like psychology. However, there is a clear need and potential for added value in returning to stronger interaction. One potential challenge for this interaction may be the lack of common conceptual grounds and design languages. In this article, we aim to contribute to the development of conceptual interfaces for human-based AI-specific design methods through the idea of cognitive mimetics. We begin by introducing basic concepts from mimetic design and interpret them in the context of this thematic area. These provide some of the basic building blocks for a design language and bring to the surface key questions. These in turn provide a ground for explicating cognitive mimetics. In the second part of this paper, we focus on specifying a key aspect in cognitive mimetics: the contents of information processes. Others engaged in this field can derive value from using or developing the basic conceptual machinery to specify their own approaches in this interdisciplinary field that is still shaping itself. Furthermore, those who resonate with the idea of cognitive mimetics, as specified here, can join in taking this particular approach further.}
}
@article{TERAN2017384,
title = {Dynamic Profiles Using Sentiment Analysis for VAA's Recommendation Design},
journal = {Procedia Computer Science},
volume = {108},
pages = {384-393},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.265},
url = {https://www.sciencedirect.com/science/article/pii/S187705091730902X},
author = {Luis Terán and Jose Mancera},
keywords = {Voting Advice Applications, Dynamic Profiles, Recommender Systems, Decision-Making, Elections},
abstract = {In the context of elections, the Internet opens new and promising possibilities for parties and candidates looking for a better political strategy and visibility. In this way they can also organize their election campaign to gather funds, to mobilize support, and to enter into a direct dialogue with the electorate. This paper presents an ongoing research of recommender systems applied on e-government, particularly it is an extension of so-called voting advice applications (VAA’s). VAA’s are Web applications that support voters, providing relevant information on candidates and political parties by comparing their political interests with parties or candidates on different political issues. Traditional VAA’s provide recommendations of political parties and candidates focusing on static profiles of users. The goal of this work is to develop a candidate profile based on different parameters, such as the perspective of voters, social network activities, and expert opinions, to construct a more accurate dynamic profile of candidates. Understanding the elements that compose a candidate profile will help citizens in the decision-making process when facing a lack of information related to the behavior and thinking of future public authorities. At the end of this work, a fuzzy-based visualization approach for a VAA design is given using as a case study the National Elections of Ecuador in 2013.}
}
@article{GERSHMAN2023104825,
title = {The molecular memory code and synaptic plasticity: A synthesis},
journal = {Biosystems},
volume = {224},
pages = {104825},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104825},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722002064},
author = {Samuel J. Gershman},
keywords = {Memory, Free energy, Synaptic plasticity, Learning, Inference},
abstract = {The most widely accepted view of memory in the brain holds that synapses are the storage sites of memory, and that memories are formed through associative modification of synapses. This view has been challenged on conceptual and empirical grounds. As an alternative, it has been proposed that molecules within the cell body are the storage sites of memory, and that memories are formed through biochemical operations on these molecules. This paper proposes a synthesis of these two views, grounded in a computational model of memory. Synapses are conceived as storage sites for the parameters of an approximate posterior probability distribution over latent causes. Intracellular molecules are conceived as storage sites for the parameters of a generative model. The model stipulates how these two components work together as part of an integrated algorithm for learning and inference.}
}
@article{KELLYPITOU201723,
title = {Microgrids and resilience: Using a systems approach to achieve climate adaptation and mitigation goals},
journal = {The Electricity Journal},
volume = {30},
number = {10},
pages = {23-31},
year = {2017},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1040619017303007},
author = {Katrina M. Kelly-Pitou and Anais Ostroski and Brandon Contino and Brandon Grainger and Alexis Kwasinski and Gregory Reed},
keywords = {Sustainable energy, Microgrid, Sustainable development, Energy policy, Climate change policy, Climate change adaptation, Adaptive capacity, Ecological modernization},
abstract = {Although energy resource sustainability has been researched extensively, the understanding of how we use and interact with electricity sustainably is less understood. New electrical designs, like microgrids, provide opportunities to better address the immediate needs of electrical sustainability and urban development. This paper analyzes the role of microgrids in urban development and examines how greater systemic thinking between infrastructure planning and energy policymaking can increase a city’s resilience.}
}
@article{CARUSO1996135,
title = {Reported earliest memory age: Relationships with personality and coping variables},
journal = {Personality and Individual Differences},
volume = {21},
number = {1},
pages = {135-142},
year = {1996},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(96)00021-9},
url = {https://www.sciencedirect.com/science/article/pii/0191886996000219},
author = {John C. Caruso and Charles L. Spirrison},
abstract = {The present study examined the viability of motivated retrieval failure as a mechanism for childhood amnesia. A total of 115 undergraduates completed the Constructive Thinking Inventory, the NEO Personality Inventory, and answered questions regarding their earliest memory in order to assess the relationships between personality and coping variables and the subject's reported age of earliest memory. The personality and coping inventories were divided into four groups defined by the stated age of each subject's earliest memory. Analyses indicated that scores on two Constructive Thinking Inventory scales (Emotional Coping and Categorical Thinking) and two NEO Personality Inventory scales (Neuroticism and Openness to Experience) were significantly different between groups. These differences were consistent with the motivated retrieval failure hypothesis. Subscales of these four scales were then analyzed individually to further examine the differences between groups. Results are discussed in the context of personality assessment and the repression of early memories.}
}
@article{MA2024100647,
title = {Design of online teaching interaction mode for vocational education based on gamified-learning},
journal = {Entertainment Computing},
volume = {50},
pages = {100647},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100647},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000156},
author = {Zhongbao Ma and Wei Li},
keywords = {Gamified-learning, Traveler-type problems, Genetic-based algorithms, Game design},
abstract = {Along with the process of building China's modern vocational education system, China's higher vocational education has made great progress. With the development of computer and Internet technology, gamified learning, as a new way of learning, combines the advantages of computer games and online learning, which not only meets the needs of people to learn anytime and anywhere, but also increases the fun of learning activities. In this paper, we developed a gamified learning software with traveler-type problems as the research content, through the interaction with the game, so that students can think in the game and learn knowledge through the game. Through the questionnaire for research and analysis, this game is good game fun and can stimulate learning interest well. In addition, this paper carries out an in-depth study of the game's help system, optimizes the algorithm for the help system, and proposes an improved genetic algorithm. The reverse learning method is adopted to improve the accuracy and convergence speed of the optimal solution; then the Metropolis criterion is used to improve the crossover and mutation operators to enhance the local search ability of the algorithm; finally, the concept of realistic elite learning is introduced to further enhance the local search ability of the algorithm. The simulation results show that the algorithm is effectively improved in convergence performance and solution accuracy, which can significantly improve the response speed of the help system, effectively improve the game's fun, and improve the game's playability.}
}
@article{RUDD2025108517,
title = {Fixational eye movements and edge integration in lightness perception},
journal = {Vision Research},
volume = {227},
pages = {108517},
year = {2025},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2024.108517},
url = {https://www.sciencedirect.com/science/article/pii/S0042698924001615},
author = {Michael E. Rudd and Idris Shareef},
keywords = {Lightness perception, ON and OFF cells, Fixational eye movements, Staircase Gelb illusion, Chevreul’s illusion, Fading of stabilized images},
abstract = {A neural theory of human lightness computation is described and computer-simulated. The theory proposes that lightness is derived from transient ON and OFF cell responses in the early visual pathways that have different characteristic neural gains and that are generated by fixational eye movements (FEMs) as the eyes transit luminance edges in the image. The ON and OFF responses are combined with corollary discharge signals that encode the eye movement direction to create directionally selective ON and OFF responses. Cortical neurons with large-scale receptive fields independently integrate the outputs of all of the directional ON or OFF responses whose associated eye movement directions point towards their receptive field centers, with a spatial weighting determined by the receptive field profile. Lightness is computed by subtracting the spatially integrated OFF activity from spatially integrated ON activity and normalizing the difference signal so that the maximum response in the spatial lightness map at any given time equals a fixed activation level corresponding to the percept of white. Two different mechanisms for ON and OFF cells responses are considered and simulated, and both are shown to produce an overall lightness model that explains a host of quantitative and qualitative lightness phenomena, including the Staircase Gelb and related illusions, failures of lightness constancy in the simultaneous contrast illusion, Chevreul’s illusion, lightness filling-in, and perceptual fading of stabilized images. The neural plausibility of the two variants of the theory, as well as its implication for lightness constancy and failures of lightness constancy are discussed.}
}
@article{KANWISHER2025102969,
title = {Animal models of the human brain: Successes, limitations, and alternatives},
journal = {Current Opinion in Neurobiology},
volume = {90},
pages = {102969},
year = {2025},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2024.102969},
url = {https://www.sciencedirect.com/science/article/pii/S0959438824001314},
author = {Nancy Kanwisher},
abstract = {The last three decades of research in human cognitive neuroscience have given us an initial “parts list” for the human mind in the form of a set of cortical regions with distinct and often very specific functions. But current neuroscientific methods in humans have limited ability to reveal exactly what these regions represent and compute, the causal role of each in behavior, and the interactions among regions that produce real-world cognition. Animal models can help to answer these questions when homologues exist in other species, like the face system in macaques. When homologues do not exist in animals, for example for speech and music perception, and understanding of language or other people's thoughts, intracranial recordings in humans play a central role, along with a new alternative to animal models: artificial neural networks.}
}
@article{MARTIN1993141,
title = {Neural connections, mental computation: Lynn Nadel, Lynn A. Cooper, Peter Culicover and R. Michael Harnish, eds.},
journal = {Artificial Intelligence},
volume = {62},
number = {1},
pages = {141-151},
year = {1993},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(93)90052-D},
url = {https://www.sciencedirect.com/science/article/pii/000437029390052D},
author = {Benjamin Martin}
}
@incollection{MONTEIRO202253,
title = {4 - An artificial intelligent cognitive approach for classification and recognition of white blood cells employing deep learning for medical applications},
editor = {Deepak Gupta and Utku Kose and Ashish Khanna and Valentina Emilia Balas},
booktitle = {Deep Learning for Medical Applications with Unique Data},
publisher = {Academic Press},
pages = {53-69},
year = {2022},
isbn = {978-0-12-824145-5},
doi = {https://doi.org/10.1016/B978-0-12-824145-5.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128241455000125},
author = {Ana Carolina Borges Monteiro and Reinaldo Padilha França and Rangel Arthur and Yuzo Iano},
keywords = {Artificial intelligence, Biomedical signals, CNN, Cognitive computing, Cognitive health care, Cognitive models, Deep learning, Digital image, Erythrocytes, Health care, Health care data, Health care informatics, Image processing, Leukocytes, Python},
abstract = {Cognitive computing aims to implement a unified computational theory similar to human thought, consisting of systems whose objective is to mimic human mental tasks based on the concepts of artificial intelligence and machine learning generating and understanding knowledge. Deep learning is an abstraction of the biological neural network, and can understood as a complex structure interconnected by simple processing elements (neurons), can perform operations as calculations in parallel, for data processing and representation of knowledge. Convolutional neural networks for image recognition through deep learning has been modeled to determine good performance with respect to digital image recognition, especially in medical areas, which is a classic problem of computational classification. In this context, an artificial intelligent cognitive approach was developed achieving an accuracy of 84.19%, which used Python employing Jupyter Notebook, with a dataset of 12,500 medical digital images of human blood smear fields of nonpathologic leukocytes.}
}
@article{GRUJIC20243381,
title = {Neurobehavioral meaning of pupil size},
journal = {Neuron},
volume = {112},
number = {20},
pages = {3381-3395},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324004069},
author = {Nikola Grujic and Rafael Polania and Denis Burdakov},
keywords = {pupil, arousal, cognition, noradrenaline, orexin, hypocretin},
abstract = {Summary
Pupil size is a widely used metric of brain state. It is one of the few signals originating from the brain that can be readily monitored with low-cost devices in basic science, clinical, and home settings. It is, therefore, important to investigate and generate well-defined theories related to specific interpretations of this metric. What exactly does it tell us about the brain? Pupils constrict in response to light and dilate during darkness, but the brain also controls pupil size irrespective of luminosity. Pupil size fluctuations resulting from ongoing “brain states” are used as a metric of arousal, but what is pupil-linked arousal and how should it be interpreted in neural, cognitive, and computational terms? Here, we discuss some recent findings related to these issues. We identify open questions and propose how to answer them through a combination of well-defined tasks, neurocomputational models, and neurophysiological probing of the interconnected loops of causes and consequences of pupil size.}
}
@article{STRYCKER2020e04358,
title = {K-12 art teacher technology use and preparation},
journal = {Heliyon},
volume = {6},
number = {7},
pages = {e04358},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2020.e04358},
url = {https://www.sciencedirect.com/science/article/pii/S2405844020312020},
author = {Jesse Strycker},
keywords = {Applications in the subject area, Art education, Educational technology, Elementary education, Instructional technology, Post-secondary education, Secondary education, Teaching/learning strategies, Educational development, Evaluation in education, Media education, Pedagogy, Teaching research, Education},
abstract = {Largely absent from educational/instructional technology journals, this study focused on how K-12 art teachers in a southern state used technology to support teaching and learning, uses they found to be the best, and what kinds of technology training they received as part of their initial teacher preparation. Findings indicated that presentation and resource access technologies had transformed the way art teachers in the study work with students and materials. They also had little use of technology to support students with special needs and had limited technology experiences in their own training. Elementary art teachers were found to have more examples of student higher-order thinking skills promoting technology use, while secondary art teachers had more student media creation and a desire to implement digital portfolios. Additional findings and interpretations are offered.}
}
@article{MAYER2024115725,
title = {Site heterogeneity and broad surface-binding isotherms in modern catalysis: Building intuition beyond the Sabatier principle},
journal = {Journal of Catalysis},
volume = {439},
pages = {115725},
year = {2024},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2024.115725},
url = {https://www.sciencedirect.com/science/article/pii/S002195172400438X},
author = {James M. Mayer},
abstract = {Learning the science of heterogeneous catalysis and electrocatalysis always starts with the simple case of a flat, uniform surface with an ideal adsorbate. It has of course been recognized for a century that real catalysts are more complicated. For the increasingly complex catalysts of the 21st century, this Perspective argues that surface heterogeneity and non-ideal binding isotherms are central features, and their implications need to be incorporated in current thinking. A variety of systems are described herein where catalyst complexity leads to broad, non-Langmuirian surface isotherms for the binding of hydrogen atoms – and this occurs even for ideal, flat Pt(111) surfaces. Modern catalysis employs nanoscale materials whose surfaces have substantial step, edge, corner, impurity, and other defect sites, and they increasingly have both metallic and non-metallic elements MnXm, including metal oxides, chalcogenides, pnictides, carbides, doped carbons, etc. The surfaces of such catalysts are often not crystal facets of the bulk phase underneath, and they typically have a variety of potential active sites. Catalytic surfaces in operando are often non-stoichiometric, amorphous, dynamic, and impure, and often vary from one part of the surface to another. Understanding of the issues that arise at such nanoscale, multi-element catalysts is just beginning to emerge. Yet these catalysts are widely discussed using Brønsted/Bell-Evans-Polanyi (BEP) relations, volcano plots, Tafel slopes, the Butler-Volmer equation, and other linear free energy relations (LFERs), which all depend on the implicit assumption that the active sites are “similar” and that surface adsorption is close to ideal. These assumptions underly the ubiquitous intuition based on the Sabatier Principle, that the fastest catalysis will occur when key intermediates have free energies of adsorption that are not too strong nor too weak. Current catalysis research often aims to minimize the complexity of non-ideal isotherms through experimental and computational design (e.g., the use of single crystal surfaces), and these studies are the foundation of the field. In contrast, this Perspective argues that the heterogeneity of binding sites and binding energies is an inherent strength of these catalysts. This diversity makes many nanoscale catalysts inherently a high-throughput screen wrapped in a tiny package. Only by making the heterogeneity part of the foundation of catalysis models, sorting the types of active sites and dissecting non-ideal binding isotherms, will modern catalysis learn to harness the inherent diversity of real catalysts. Controlling and exploiting diversity rather than avoiding it will help to optimize complex modern catalysts and catalytic conditions.}
}
@article{GROSS2019116125,
title = {Is perception the missing link between creativity, curiosity and schizotypy? Evidence from spontaneous eye-movements and responses to auditory oddball stimuli},
journal = {NeuroImage},
volume = {202},
pages = {116125},
year = {2019},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116125},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919307165},
author = {Madeleine E. Gross and Draulio B. Araujo and Claire M. Zedelius and Jonathan W. Schooler},
keywords = {Creativity, Curiosity, Schizotypy, Eye-tracking, Eye-gaze, Salience, Perception},
abstract = {What is the relationship between creativity, curiosity, and schizotypy? Schizophrenia-spectrum conditions and creativity have been linked to deficits in filtering sensory information, and curiosity is associated with information-seeking. This raises the possibility of a perception-based link between all three concepts. Here, we investigated whether the same individual differences in perceptual encoding explain variance in creativity, curiosity, and schizotypy. We administered an active auditory oddball task and a free viewing eye-tracking paradigm (N = 88). Creativity was measured with the figural portion of the Torrance Tests of Creative Thinking (TTCT) and two self-report scales. Schizotypy and curiosity were measured with self-reports. We found that creativity was associated with increased reaction time to the rare tone in the oddball task and was positively associated with the number and duration of fixations in the free viewing task. Schizotypy, on the other hand, showed a negative trend with the number and duration of fixations. Both creativity and curiosity were positively associated with explorative eye movements (unique number of regions visited) and Shannon entropy, while schizotypy was negatively associated with entropy. We further compared saliency maps finding that individuals high versus low in creativity and curiosity, respectively, exhibit differences in where they look. These findings may suggest a perception-based link between creativity and curiosity, but not schizotypy. Implications and limitations of these findings are discussed.}
}
@article{CLANCY2008248,
title = {Applications of complex systems theory in nursing education, research, and practice},
journal = {Nursing Outlook},
volume = {56},
number = {5},
pages = {248-256.e3},
year = {2008},
note = {Special Issue Informatics: Science and Practice},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2008.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0029655408001619},
author = {Thomas R. Clancy and Judith A. Effken and Daniel Pesut},
abstract = {The clinical and administrative processes in today's healthcare environment are becoming increasingly complex. Multiple providers, new technology, competition, and the growing ubiquity of information all contribute to the notion of health care as a complex system. A complex system (CS) is characterized by a highly connected network of entities (e.g., physical objects, people or groups of people) from which higher order behavior emerges. Research in the transdisciplinary field of CS has focused on the use of computational modeling and simulation as a methodology for analyzing CS behavior. The creation of virtual worlds through computer simulation allows researchers to analyze multiple variables simultaneously and begin to understand behaviors that are common regardless of the discipline. The application of CS principles, mediated through computer simulation, informs nursing practice of the benefits and drawbacks of new procedures, protocols and practices before having to actually implement them. The inclusion of new computational tools and their applications in nursing education is also gaining attention. For example, education in CSs and applied computational applications has been endorsed by The Institute of Medicine, the American Organization of Nurse Executives and the American Association of Colleges of Nursing as essential training of nurse leaders. The purpose of this article is to review current research literature regarding CS science within the context of expert practice and implications for the education of nurse leadership roles. The article focuses on 3 broad areas: CS defined, literature review and exemplars from CS research and applications of CS theory in nursing leadership education. The article also highlights the key role nursing informaticists play in integrating emerging computational tools in the analysis of complex nursing systems.}
}
@article{HEINZE2021R1381,
title = {Fly navigation: Yet another ring},
journal = {Current Biology},
volume = {31},
number = {20},
pages = {R1381-R1383},
year = {2021},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2021.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0960982221012495},
author = {Stanley Heinze},
abstract = {Summary
Flies keep track of a food site by path integration. A novel behavioral paradigm has been combined with computational models to show that Drosophila can track at least three food patches simultaneously by using the center of gravity of all food sites as the reference point for their path integrator.}
}