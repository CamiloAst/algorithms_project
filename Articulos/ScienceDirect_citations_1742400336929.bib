@article{ALI2024101172,
title = {Physics-informed neural networks in groundwater flow modeling: Advantages and future directions},
journal = {Groundwater for Sustainable Development},
volume = {25},
pages = {101172},
year = {2024},
issn = {2352-801X},
doi = {https://doi.org/10.1016/j.gsd.2024.101172},
url = {https://www.sciencedirect.com/science/article/pii/S2352801X2400095X},
author = {Ahmed Shakir Ali Ali and Farhad Jazaei and T. Prabhakar Clement and Brian Waldron},
keywords = {Artificial intelligence, Physics-informed neural network, PINN, Groundwater modeling, MODFLOW},
abstract = {In recent years, there has been enormous development in soft computing, especially artificial intelligence (AI), which has developed robust methods for solving complex engineering problems. Researchers in the field of water resources engineering have applied these AI methods to solve a variety of hydrological problems. Despite their widespread use in the surface and atmospheric hydrology fields, groundwater hydrologists have not widely used AI methods in their routine field-scale modeling efforts. This is because AI models have been primarily considered black box models that lack physical meaning. Furthermore, using AI models to generate the space-time distribution of transient groundwater level variations is challenging and requires further flux balance and mass transport analyses. More recently, a new type of physics-informed neural network (PINN) model has been developed to address several limitations by integrating governing physics (groundwater flow equations) into the AI tools. This study presents the systematic advantages of the PINN algorithm for solving groundwater problems using a set of classic test problems. As discussed in detail in the article, these advantages and potentials are associated with the meshless nature of PINN, its continuous time and space dimensions, its independence from time-stepping and incremental marching in space, and its efficiency in running time. However, despite PINN's promising attributes, it is important to acknowledge its nascent stage of development and the inherent limitations of all neural network models, such as training challenges and hyperparameter selection. Thus, collaborative efforts between groundwater modelers and computer scientists are imperative to explore and exploit the full potential of PINN in tackling increasingly complex groundwater problems and nurturing PINN into a dependable modeling tool in industry and academia.}
}
@article{YIN2024133163,
title = {Mobileception-ResNet for transient stability prediction of novel power systems},
journal = {Energy},
volume = {309},
pages = {133163},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.133163},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224029384},
author = {Linfei Yin and Wei Ge},
keywords = {Transient stability, Deep learning, MobileNet-v2, Convolutional neural network, Inception-ResNet-v2},
abstract = {Power system transient stability prediction (TSP) is particularly important as power systems change and evolve, including the rapid growth of renewable energy, the proliferation of electric vehicles, and the construction of smart grids. Traditional time-domain simulation methods are time-consuming and cannot achieve online prediction. Direct methods are poorly adapted and cannot be applied to complex power systems. Existing machine learning algorithms only classify the transient stability without providing the degree of transient stability of the system. Therefore, a fast and accurate power system TSP method is needed to assist operators in implementing timely measures to improve the stability of the power system running. This study proposes a Mobileception-ResNet network, Mobileception-ResNet is formed by Inception-ResNet-v2, MobileNet-v2, and a fully connected layer. In this study, Mobileception-ResNet and nine comparison models are experimented on two node systems, i.e., the IEEE 10–39 and 69–300 systems. In the IEEE 10–39 system, the root mean square error, mean absolute error, and mean absolute percentage error of Mobileception-ResNet are 44.13 %, 36.74 %, and 39.96 % lower, and the coefficient of determination is 0.04 % higher, respectively, when compared to the comparative model with the best evaluation indicator; in the IEEE 69–300 system, the corresponding values are 2.6 %, 12.83 %, 12.55 %, and 0.01 %, respectively.}
}
@incollection{WANG2022238,
title = {1.10 - CyberGIS and Geospatial Data Science for Advancing Geomorphology},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {238-259},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00122-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818234500122X},
author = {Shaowen Wang and Michael P. Bishop and Zhe Zhang and Brennan W. Young and Zewei Xu},
keywords = {Artificial intelligence, CyberGIS, Deep learning, Geomorphology, Geospatial data science, Land cover science, LiDAR, Uncertainty},
abstract = {Theoretical and practical issues in geomorphology have not been adequately addressed due to a lack of formalization and digital representation of spatial and temporal concepts, given the limitations associated with modern-day geographic information systems (GIS). Rapid advancements in geospatial technologies have resulted in new sensors and large volumes of geospatial data that have yet to be fully exploited given a variety of computational issues. Computational limitations involving storage, preprocessing, analysis, and modeling pose significant problems for Earth scientists. Consequently, advanced cyberinfrastructure is required to address geospatial data-science issues involving communication, representation, computation, information production, decision-making, and geovisualization. We identify and discuss important aspects of exploiting advances in cyberinfrastructure that involve computational scalability, artificial intelligence, and uncertainty characterization and analysis for addressing issues in the Earth sciences. Such developments can be termed cyber geographic information science and systems (cyberGIS). We discuss this important topic by addressing the significant overlap of concepts in GIS and geomorphology that can be formalized, digitally represented, implemented, and evaluated with cyberGIS. We then introduce the fundamentals of cyberinfrastructure and cyberGIS, including a discussion of the utilization of artificial intelligence and deep learning. We finally provide one case study demonstrating operational cyberGIS capabilities.}
}
@article{BISWAS2008127,
title = {Towards an agent-oriented approach to conceptualization},
journal = {Applied Soft Computing},
volume = {8},
number = {1},
pages = {127-139},
year = {2008},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2006.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1568494606001062},
author = {Pratik K. Biswas},
keywords = {Intelligent agents, Multi-agent systems, Agent-oriented software engineering, Agent-oriented thinking, Agent-oriented modeling, Extended agent model, Agent-oriented analysis, Agent-oriented design},
abstract = {Agent-oriented modeling provides a new technique for the conceptualization of agent-based systems. This paper extends and formalizes this agent-oriented modeling approach to the conceptualization process. It defines agent models and proposes a high-level methodology for agent-oriented analysis and design. It also includes analogies with the object-oriented and other existing agent-oriented methodologies, wherever applicable. The paper is concluded with a case study and an insight to future challenges.}
}
@article{RUAN2023100872,
title = {Public perception of electric vehicles on Reddit and Twitter: A cross-platform analysis},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {21},
pages = {100872},
year = {2023},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2023.100872},
url = {https://www.sciencedirect.com/science/article/pii/S2590198223001197},
author = {Tao Ruan and Qin Lv},
keywords = {Cross-platform, Twitter, Reddit, Electric vehicles, Public perception, Topic modeling, Computational social science},
abstract = {Electrified mobility such as electric vehicles (EVs) is a promising solution to reduce carbon emissions in transportation and mitigate global warming. Understanding public perception of EVs can help better support their adoption. A previous study shows that online social networks (OSNs) such as Reddit can be a valuable source for studying public perceptions of EVs and provide different perspectives from traditional methods that leverage surveys, questionnaires, or interviews (Ruan and Lv, 2022). Our work aims to investigate this direction further through the following research question: Given the distinct mechanisms of various OSNs, can we obtain a more comprehensive picture of public perception of EVs by integrating the analysis from different platforms? Specifically, our study is based on EV-related discussions on two popular OSN platforms: Twitter and Reddit. We have collected 3,437,917 Reddit posts (including 274,979 submissions and 3,162,938 comments) and 7,383,327 Tweets between January 2011 and December 2020 and analyzed them from several perspectives. Our analysis shows that users have had different topic and sentiment patterns in EV-related discussions on the two platforms over the past decade. We also leverage the verified account information on Twitter to reveal that the most influential users are politicians and news media; however, the general public has very different conversation patterns with the two types of accounts — politicians seem to be increasingly (over) optimistic about EVs while the public may think differently.}
}
@article{DAS2022104116,
title = {Role of non-motorized transportation and buses in meeting climate targets of urban regions},
journal = {Sustainable Cities and Society},
volume = {86},
pages = {104116},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104116},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722004292},
author = {Deepjyoti Das and Pradip P. Kalbar and Nagendra R. Velaga},
keywords = {Decarbonization, Life cycle thinking, Avoided trip and material, Carbon budget, Climate change, Sustainable transportation},
abstract = {Studies examining the potential of low-carbon modes of passenger transportation for achieving climate goals are limited. The study is one of the first to assess the potential of non-motorized transportation (NMT) and buses to meet regional climate targets representing 2 °C, 1.5 °C, and Intended Nationally Determined Contributions from 2018 to 2050. Also, the approach towards quantifying contribution from avoided trips and materials in holistically understanding the potential of NMT and buses is novel. Data from the transportation model of Mumbai Metropolitan Region's Comprehensive Mobility Plan is used to assess multiple scenarios of upgrading NMT and bus infrastructure to reduce cumulative carbon dioxide emissions (CCE) from passenger transportation. The assessment is based on three push levels, i.e., conservative, moderate, and aggressive. Results show that upgrading bus infrastructure contributes higher to reducing CCE than NMT. As NMT also contributes significantly to decreasing CCE, it is recommended that bus and NMT development should be integrated. However, their combined contribution will not meet the climate targets. Since avoided materials contribute considerably more than avoided trips, high emission materials such as aluminum used in light-weighting should be questioned. The results provide policy guidance to authorities in prioritizing buses and NMT infrastructure development during city planning.}
}
@article{FOO201410,
title = {Evolution of acquired resistance to anti-cancer therapy},
journal = {Journal of Theoretical Biology},
volume = {355},
pages = {10-20},
year = {2014},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2014.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S0022519314001003},
author = {Jasmine Foo and Franziska Michor},
keywords = {Drug resistance, Cancer, Evolution, Mathematical modeling, Optimal dosing strategies},
abstract = {Acquired drug resistance is a major limitation for the successful treatment of cancer. Resistance can emerge due to a variety of reasons including host environmental factors as well as genetic or epigenetic alterations in the cancer cells. Evolutionary theory has contributed to the understanding of the dynamics of resistance mutations in a cancer cell population, the risk of resistance pre-existing before the initiation of therapy, the composition of drug cocktails necessary to prevent the emergence of resistance, and optimum drug administration schedules for patient populations at risk of evolving acquired resistance. Here we review recent advances towards elucidating the evolutionary dynamics of acquired drug resistance and outline how evolutionary thinking can contribute to outstanding questions in the field.}
}
@incollection{HUNG2017227,
title = {Chapter 12 - Rationality and Escherichia Coli},
editor = {T.-W. Hung and T.J. Lane},
booktitle = {Rationality},
publisher = {Academic Press},
address = {San Diego},
pages = {227-240},
year = {2017},
isbn = {978-0-12-804600-5},
doi = {https://doi.org/10.1016/B978-0-12-804600-5.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012804600500012X},
author = {T.-W. Hung},
keywords = {practical rationality, procedural rationality, , human uniqueness, nonhuman rationality},
abstract = {If rationality is the defining characteristic of the human species, as Aristotle asserts, why is this trait rarely noticed in Eastern traditions? How should we interpret the reasoning ability in problem solving that is increasingly reported in other animals? In this chapter, I focus on descriptive-practical-procedural rationality (one’s action is described as rational if it is determined by internal processes that conform to logical or Bayesian rules). I argue that this rationality can be found in all organisms with adaptive capacity, including unicellular bacteria. To this end, I first review three seemingly true claims and explain why they lead to an inconsistency: (1) Escherichia coli are computational systems in a nontrivial sense, (2) E. coli are not creatures that can be rational or irrational, and (3) rationality is a matter of computational facts in that nontrivial sense, and organisms of the same computation are the type of creatures that can be rational or irrational. I then suggest rejecting claim (2) by examining recent microbiological data on E. coli, explaining the extent to which they satisfy this type of rationality. I also discuss some objections to and implications of this view. Instead of concluding that humans and bacteria both evolve with rationality at the same level, I argue that organisms’ rationality capacities comes in degree.}
}
@article{LEACH2024,
title = {The engineering legacy of the FIFA World Cup Qatar 2022: Al Janoub stadium},
journal = {Proceedings of the Institution of Civil Engineers - Structures and Buildings},
year = {2024},
issn = {0965-0911},
doi = {https://doi.org/10.1680/jstbu.22.00127},
url = {https://www.sciencedirect.com/science/article/pii/S0965091124000155},
author = {Jon Leach and Craig Sparrow and Federico Iori and Hamad N. Al-Nuaimi and Mohammed Z. E. B. Elshafie and Nasser A. Al-Nuaimi},
keywords = {buildings, structures & design, thermal effects, wind loading & aerodynamics},
abstract = {Al Janoub was the first new-build stadium designed for the FIFA World Cup Qatar 2022. This paper describes the journey of the engineering design of the 40 000 seat stadium, from the concept and detailed design development stages led by AECOM and Zaha Hadid Architects, through to the design and build contract on site. An architectural jewel located in Al Wakrah, just south of the city of Doha, the stadium was a world-first, using state-of-the-art computational analysis and physical modelling to create a safe, cooled environment that satisfies FIFA's requirements for both player and spectator comfort in the extreme temperatures of the region. A sustainable post-tournament legacy was also a key factor of the design, allowing it to be reduced to a 20 000-capacity stadium for the local football club and community. The task of integrating the stadium's stringent performance requirements on this path-finder project, including extensive scientific research and development, was a challenge that was overcome through close collaboration between the design team and the Supreme Committee's subject matter experts. The project's success as a test-bed helped it to set the standard for other stadia as part of the overall FIFA World Cup Qatar 2022 programme.}
}
@article{DRACK2011150,
title = {System approaches of Weiss and Bertalanffy and their relevance for systems biology today},
journal = {Seminars in Cancer Biology},
volume = {21},
number = {3},
pages = {150-155},
year = {2011},
note = {Why Systems Biology and Cancer?},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X11000307},
author = {Manfred Drack and Olaf Wolkenhauer},
keywords = {Paul Alfred Weiss, Ludwig von Bertalanffy, Organismic biology, System theory of life, Systems biology},
abstract = {System approaches in biology have a long history. We focus here on the thinking of Paul A. Weiss and Ludwig von Bertalanffy, who contributed a great deal towards making the system concept operable in biology in the early 20th century. To them, considering whole living systems, which includes their organisation or order, is equally important as the dynamics within systems and the interplay between different levels from molecules over cells to organisms. They also called for taking the intrinsic activity of living systems and the conservation of system states into account. We compare these notions with today's systems biology, which is often a bottom-up approach from molecular dynamics to cellular behaviour. We conclude that bringing together the early heuristics with recent formalisms and novel experimental set-ups can lead to fruitful results and understanding.}
}
@incollection{SIEGEL20223,
title = {Chapter 1 - Introduction: Defining the Role of Statistics in Business},
editor = {Andrew F. Siegel and Michael R. Wagner},
booktitle = {Practical Business Statistics (Eighth Edition)},
publisher = {Academic Press},
edition = {Eighth Edition},
pages = {3-18},
year = {2022},
isbn = {978-0-12-820025-4},
doi = {https://doi.org/10.1016/B978-0-12-820025-4.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200254000014},
author = {Andrew F. Siegel and Michael R. Wagner},
abstract = {We begin this chapter with an overview of the competitive advantage provided by a knowledge of statistical methods, followed by some basic facts about statistics and probability and their role in business. Statistical activities can be grouped into five main activities (designing, exploring, modeling, estimating, and hypothesis testing), and one way to clarify statistical thinking is to be able to match the business task at hand with the correct collection of statistical methods. This chapter sets the stage for the rest of the book, which follows up with many important detailed procedures for accomplishing business goals that involve these activities. Next follows an overview of data mining of Big Data (which involves these main activities) and its importance in business. Then we distinguish the field of probability (where, based on assumptions, we reach conclusions about what is likely to happen—a useful exercise in business where nobody knows for sure what will happen) from the field of statistics (where we know from the data what happened, from which we infer conclusions about the system that produced these data) while recognizing that probability and statistics will work well together in future chapters. The chapter concludes with some words of advice on how to integrate statistical thinking with other business viewpoints and activities.}
}
@article{JHA2025103700,
title = {Transitive reasoning: A high-performance computing model for significant pattern discovery in cognitive IoT sensor network},
journal = {Ad Hoc Networks},
volume = {167},
pages = {103700},
year = {2025},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2024.103700},
url = {https://www.sciencedirect.com/science/article/pii/S1570870524003111},
author = {Vidyapati Jha and Priyanka Tripathi},
keywords = {Transitive reasoning, Knowledge discovery, CIoT, Probabilistic clustering, Total variation regularization},
abstract = {Current research on the Internet of Things (IoT) has given rise to a new field of study called cognitive IoT (CIoT), which aims to incorporate cognition into the designs of IoT systems. Consequently, the CIoT inherits specific attributes and challenges from IoT. The CIoT applications generate vast, diverse, constantly changing, and time-dependent data due to the billions of devices involved. The efficient operation of these CIoT systems requires the extraction of valuable insights from vast data sources in a computationally efficient manner. Therefore, this study proposes transitive reasoning to glean significant concepts and patterns from a 21.25-year environmental dataset. To reduce the effects of missing entries, the proposed methodology includes a grouping of data using probabilistic clustering and applying total variance regularization in the alternate direction method of multipliers (ADMM) to regularize the sensory data. As a result, noisy entries will be less conspicuous. Afterward, it calculates the transitional plausibility value for each cluster using the transited value and then turns it into binary data to create concept lattices. In addition, each concept that is formed is assigned a weight, and the concept with the largest transitive strength value is chosen, followed by calculating the mean value. Therefore, this pattern is seen as significant. Experimental results on 21.25-year environmental data show an accuracy of over 99.5%, outperforming competing methods, as shown by cross-validation using multiple metrics.}
}
@incollection{NEWMAN2020183,
title = {Chapter 7 - Cognitive developmental theories},
editor = {Barbara M. Newman and Philip R. Newman},
booktitle = {Theories of Adolescent Development},
publisher = {Academic Press},
pages = {183-211},
year = {2020},
isbn = {978-0-12-815450-2},
doi = {https://doi.org/10.1016/B978-0-12-815450-2.00007-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128154502000073},
author = {Barbara M. Newman and Philip R. Newman},
keywords = {Problem solving, Equilibrium, Schemes, Adaptation, Egocentrism, Formal operational thought, Moral reasoning, Social reasoning, Metacognition, Education},
abstract = {The chapter focuses on cognitive developmental theories which address the emerging nature of concept formation, reasoning, planning, and problem solving, and the increasingly complex structures that support changing and flexible capacities for thinking about multidimensional problems with probabilistic outcomes. This chapter summarizes the work of Jean Piaget and the extension of his ideas among neo-Piagetian theorists including Deanna Kuhn, Paul Klacziynski, and Robbie Case. The following key concepts are explained: equilibrium, schemes, organization, adaptation, stages of development, and egocentrism. The stage of Formal Operational Reasoning and elaboration of cognitive capacities in adolescence are described in detail. Application of these theories to moral reasoning, social reasoning, metacognition, and educational initiatives are discussed. Experimental approaches and paper and pencil measures of cognitive reasoning are described. The strengths and limitations of cognitive developmental theories are reviewed.}
}
@article{BISWAL2024110150,
title = {Unlocking the potential of signature-based drug repurposing for anticancer drug discovery},
journal = {Archives of Biochemistry and Biophysics},
volume = {761},
pages = {110150},
year = {2024},
issn = {0003-9861},
doi = {https://doi.org/10.1016/j.abb.2024.110150},
url = {https://www.sciencedirect.com/science/article/pii/S0003986124002728},
author = {Sruti Biswal and Bibekanand Mallick},
keywords = {Cancer, Anticancer drug, Drug repurposing, Gene signature},
abstract = {Cancer is the leading cause of death worldwide and is often associated with tumor relapse even after chemotherapeutics. This reveals malignancy is a complex process, and high-throughput omics strategies in recent years have contributed significantly in decoding the molecular mechanisms of these complex events in cancer. Further, the omics studies yield a large volume of cancer-specific molecular signatures that promote the discovery of cancer therapy drugs by a method termed signature-based drug repurposing. The drug repurposing method identifies new uses for approved drugs beyond their intended initial therapeutic use, and there are several approaches to it. In this review, we discuss signature-based drug repurposing in cancer, how cancer omics have revolutionized this method of drug discovery, and how one can use the cancer signature data for repurposed drug identification by providing a step-by-step procedural handout. This modern approach maximizes the use of existing therapeutic agents for cancer therapy or combination therapy to overcome chemotherapeutics resistance, making it a pragmatic and efficient alternative to traditional resource-intensive and time-consuming methods.}
}
@article{IBEZIM2024e02226,
title = {Potential dual inhibitors of Hexokinases and mitochondrial complex I discovered through machine learning approach},
journal = {Scientific African},
volume = {24},
pages = {e02226},
year = {2024},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2024.e02226},
url = {https://www.sciencedirect.com/science/article/pii/S2468227624001728},
author = {Akachukwu Ibezim and Emmanuel Onah and Sochi Chinaemerem Osigwe and Peter Ukwu Okoroafor and Onyeoziri Pius Ukoha and Jair Lage {de Siqueira-Neto} and Fidele Ntie-Kang and Karuppasamy Ramanathan},
keywords = {Hexokinases, Mitochondrial complex I, Cancer, MACCS fingerprints, Boruta algorithms, Machine learning, Metabolic plasticity},
abstract = {Hexokinases (Hks) and mitochondrial complex I (MCI) are involved in the energy metabolism of cells; glycolysis/fermentation and oxidative phosphorylation. Both Hks and MCI are known to play critical roles in either division of metabolic plasticity which enables tumor progression and proliferation in the presence of chemotherapies. Therefore, targeting these enzymes are important in cancer drug resistance. Here, computational models for the prediction of inhibition of Hks were developed based on experimental data and an optimal feature subset that was selected by the Boruta algorithm (a wrapper feature selection algorithm coupled with random forest). Out of the seven models that were explored, a random forest classifier gave the best prediction (GA = 0.84, FNR = 0.12 and AUC = 0.96 for the external dataset). Fragmentation analysis led to the identification of the unique structural scaffolds that characterize hexokinase inhibitors and non-inhibitors. The best Hks inhibition model predicted that 23 molecules out of the 191 dataset of MCI actives (IC50 ≤ 10 µM) that were screened, have more than 60 % probability of exhibiting Hk inhibitory activity. Hence, they are possible dual inhibitors of both targets. Furthermore, the 23 molecules’ core structures are members of the scaffolds that are unique to Hk inhibitors earlier predicted by fragment analysis. The need for dual targeting agents in cancer therapy, particularly in combating cancer drug resistance, highlights the relevance of these findings.}
}
@article{DASILVA2024105785,
title = {Optimization of open web steel beams using the finite element method and genetic algorithms},
journal = {Structures},
volume = {60},
pages = {105785},
year = {2024},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2023.105785},
url = {https://www.sciencedirect.com/science/article/pii/S2352012423018738},
author = {Amilton Rodrigues {da Silva} and Gabriela Pereira Lubke},
keywords = {Open-web beams, Optimization, Genetic algorithm, Finite element method},
abstract = {Studies on structural optimization have gained prominence recently, and the search to consume resources in a more conscious and effective way encourages the use of such techniques in all fields. In this respect, this study aims to use computational optimization techniques to determine the maximum load-bearing capacity of hollow-core steel beams for two groups of different shear lines, one generating beams with opening in the shape of hexagons and the other having the shape of ellipses. The second group includes beams with circular openings as a particular case. A three-node triangular finite element for the analysis of structures in plane stress is used for the structural analysis of the beams. The design variables define the shape and number of opening in the beam, and a computational formulation using a genetic algorithm is presented to find the cut line that maximizes the load capacity of the beam considering different ultimate and service limit states. Numerical and experimental models in the literature are used to validate the implementations presented in this article, and the results of optimized hollow core beams are presented, demonstrating the efficiency of the formulations used.}
}
@article{PRADNYANA2025100307,
title = {An explainable ensemble model for revealing the level of depression in social media by considering personality traits and sentiment polarity pattern},
journal = {Online Social Networks and Media},
volume = {46},
pages = {100307},
year = {2025},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2025.100307},
url = {https://www.sciencedirect.com/science/article/pii/S2468696425000084},
author = {Gede Aditra Pradnyana and Wiwik Anggraeni and Eko Mulyanto Yuniarno and Mauridhi Hery Purnomo},
keywords = {Explainable ensemble model, Personality trait, Sentiment polarity pattern, RoBERTa, Hybrid RF-BiLSTM},
abstract = {Early detection of depression in mental health is crucial for better intervention. Social media has been extensively used to examine users’ behavior, motivating researchers to develop an automatic depression detection model. However, the accuracy and clarity of the reasons behind the detection results still need to be improved. Current research focuses primarily on syntactic and semantic information in user-posted texts, while other aspects of users’ psychological characteristics are often overlooked. Therefore, this study addresses the gap by proposing a novel model integrating personality traits and sentiment polarity patterns into an explainable ensemble model. Specifically, we developed two base learners for the averaged and meta-ensemble learning strategy. The first learner employed the Robustly Optimized BERT Pre-training Approach (RoBERTa). For the second learner, we combined the Random Forest and Bidirectional Long Short-Term Memory (RF-BiLSTM) methods to effectively handle the combination of personality traits and sequential information in sentiment polarity patterns. These additional features are obtained by performing domain adaptation for personality prediction and sentiment analysis using a lexicon-based model. Based on the experimental results, our ensemble model improved depression detection results by leveraging the strengths of each base learner. Our model advanced the state-of-the-art, outperforming existing models with an increase in accuracy and F1-score of 4.14% and 2.99%, respectively. The model successfully enhanced the interpretability of detection results, providing a more comprehensive understanding of the factors underlying depressive symptoms. This research highlights the potential of considering alternative additional features as a promising avenue for enhancing depression detection in social media.}
}
@article{VALJAK2023191,
title = {Functional modelling through Function Class Method: A case from DfAM domain},
journal = {Alexandria Engineering Journal},
volume = {66},
pages = {191-209},
year = {2023},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110016822007852},
author = {Filip Valjak and Nenad Bojčetić},
keywords = {Functional modelling, Function structure, Function class, Design for Additive Manufacturing},
abstract = {Functional modelling is an essential part of systematic design approaches and is often prescribed in engineering design textbooks. However, function models created with current function modelling techniques often lack formal and repeatable representation, limiting their use in computational reasoning. Therefore, this paper presents a new functional modelling method to support function models' creation with formal and repeatable representation. The key element of the proposed method is a Function Class – a function-modelling element that categorises defined functions on a function block level by specifying operating flow, input and output flows, and integrates primary rules for functional modelling such as conservation law. The formalisation on a function block level reduces the number of morphological errors and provides a theoretical framework for future computational processing of function models. This paper proposes a protocol for developing Function Classes and defines a theoretical function modelling framework through Function Class Method. The development and use of the Function Class Method are demonstrated through the development of Function Classes for the Design for Additive Manufacturing domain as the first step toward a universal function modelling approach.}
}
@article{WOLFENGAGEN2016353,
title = {Concordance in the Crowdsourcing Activity},
journal = {Procedia Computer Science},
volume = {88},
pages = {353-358},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.448},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916317045},
author = {Viacheslav E. Wolfengagen and Larisa Yu. Ismailova and Sergey Kosikov},
keywords = {crowdsourcing, Big Data, Thick Data, variable domains, cognition model, concordance, computational model},
abstract = {A concordance in cognition activity of possibly interrelated crowdsourcers aimed to property recognition in the voluminous data sources is considered. Data sources are of either usual nature or manually generated with the crowdsourcing. The proposed model is based on the variable domains assumption. A general layout is able to take into account an interaction of crowdsourcers and properties when they are varying with the evolving the events. The cognition model is of stage-by-stage type and has the representable functor. This model as may be shown is faithfully embedded into a category of indexed sets. Using the proposed neighborhood for cognition activity leads to a flexible computing model.}
}
@article{DOSSOU2021476,
title = {Development of a decision support tool for sustainable urban logistics optimization},
journal = {Procedia Computer Science},
volume = {184},
pages = {476-483},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.060},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921006918},
author = {Paul-Eric Dossou and Axel Vermersch},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Traffic flows are increasing in cities, partially due to congestions provoked by trucks. These congestions cause many problems such as pollution (gasoil, Carbon, etc.), noise, waste of time. Indeed, cities like Paris, Hamburg, Milan, and “Grand Paris Sud” conurbation are thinking about a sustainable alternative solution to road transportation. Then, a research based on co-creation methodology integrating all stakeholders (local authorities, companies, citizens) for elaborating an alternative solution to road transportation has been defined. This paper presents the architecture and the development of a decision aided tool for simulating and optimizing alternative solutions to road transportation}
}
@article{ESHAGHI2024107342,
title = {Methods for enabling real-time analysis in digital twins: A literature review},
journal = {Computers & Structures},
volume = {297},
pages = {107342},
year = {2024},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2024.107342},
url = {https://www.sciencedirect.com/science/article/pii/S0045794924000713},
author = {Mohammad Sadegh Es-haghi and Cosmin Anitescu and Timon Rabczuk},
abstract = {This paper presents a literature review on methods for enabling real-time analysis in digital twins, which are virtual models of physical systems. The advantages of digital twins are numerous, including cost reduction, risk mitigation, efficiency enhancement, and decision-making support. However, their implementation faces challenges such as the need for real-time data analysis, resource limitations, and data uncertainty. The paper focuses on methods for reducing computational demands, which have not been systematically discussed in the literature. The paper reviews and categorizes methods and tools for accelerating the modeling of physical phenomena and reducing the computational needs of digital twins.}
}
@article{LEITE20231,
title = {Interval incremental learning of interval data streams and application to vehicle tracking},
journal = {Information Sciences},
volume = {630},
pages = {1-22},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523002165},
author = {Daniel Leite and Igor Škrjanc and Sašo Blažič and Andrej Zdešar and Fernando Gomide},
keywords = {Granular machine learning, Online learning, Granular computing, Interval analysis, Data stream},
abstract = {This paper presents a method called Interval Incremental Learning (IIL) to capture spatial and temporal patterns in uncertain data streams. The patterns are represented by information granules and a granular rule base with the purpose of developing explainable human-centered computational models of virtual and physical systems. Fundamentally, interval data are either included into wider and more meaningful information granules recursively, or used for structural adaptation of the rule base. An Uncertainty-Weighted Recursive-Least-Squares (UW-RLS) method is proposed to update affine local functions associated with the rules. Online recursive procedures that build interval-based models from scratch and guarantee balanced information granularity are described. The procedures assure stable and understandable rule-based modeling. In general, the model can play the role of a predictor, a controller, or a classifier, with online sample-per-sample structural adaptation and parameter estimation done concurrently. The IIL method is aligned with issues and needs of the Internet of Things, Big Data processing, and eXplainable Artificial Intelligence. An application example concerning real-time land-vehicle localization and tracking in an uncertain environment illustrates the usefulness of the method. We also provide the Driving Through Manhattan interval dataset to foster future investigation.}
}
@article{GERSHMAN2020104394,
title = {Origin of perseveration in the trade-off between reward and complexity},
journal = {Cognition},
volume = {204},
pages = {104394},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104394},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720302134},
author = {Samuel J. Gershman},
keywords = {Decision making, Information theory, Reinforcement learning},
abstract = {When humans and other animals make repeated choices, they tend to repeat previously chosen actions independently of their reward history. This paper locates the origin of perseveration in a trade-off between two computational goals: maximizing rewards and minimizing the complexity of the action policy. We develop an information-theoretic formalization of policy complexity and show how optimizing the trade-off leads to perseveration. Analysis of two data sets reveals that people attain close to optimal trade-offs. Parameter estimation and model comparison supports the claim that perseveration quantitatively agrees with the theoretically predicted functional form (a softmax function with a frequency-dependent action bias).}
}
@article{QAMMAR2023e16230,
title = {Statistical analysis of the university sustainability in the higher education institution a case study from the Khyber Pakhtunkhwa province in Pakistan},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e16230},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e16230},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023034370},
author = {Naseha Wafa Qammar and Zohaib Ur {Rehman Afridi} and Shamaima Wafa Qammar},
keywords = {Sustainability, Higher education institute, Students and faculty, Pakistan},
abstract = {Educational institutions can incorporate the idea of sustainability at the grass root level for any society. This study is part of an effort to get insight into the campus sustainability in one of the Higher Education Institution (HEI) in the Khyber Pakhtunkhwa region of Pakistan. Aim is to investigation university students' and faculty members insight regarding sustainability. Thus, questionnaire-based survey followed by statistical inference was conducted for the potential outcomes. The questionnaire is comprised of 24 questions, 05 of which are on demographics and the remaining 19 are about sustainability. The sustainability related questions focused mostly on the respondents' knowledge, understanding, and interest in sustainability. A handful of the other questions in the questionnaire were tailored to the university input to achieve sustainability. The dataset is manipulated with basic statistical and computational approaches, and the results are analyzed using mean values. The mean values are further classified into flag values of 0 and 1. Flag value 1 indicates a good marker of the received response, while flag value 0 indicates the least amount of information included in responses. The results show that respondents' knowledge, awareness, interest, and engagement in sustainability are significantly sufficient, as we obtained a flag value of 1 for all questions about sustainability. The study's findings, on the other hand, indicated that the institution is lagging in terms of supporting, disseminating, and implementing campus-wide sustainability-related activities. This study is one of the first initiatives to provide a baseline dataset and substantial information to go a step further in achieving the bottom-line target of being and acting sustainable in the HEI.}
}
@article{CHAIARWUT2025101338,
title = {Enhancing executive mathematics problem-solving through a constructivist digital learning platform: Design, development and evaluation},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101338},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101338},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125000658},
author = {Supaluk Chaiarwut and Sanit Srikoon and Apirat Siritaratiwat and Parama Kwangmuang},
keywords = {Learning innovation, Digital platform, Mathematics problem solving},
abstract = {Recent international assessments have highlighted a global decline in mathematics performance, particularly among students in Thailand. This study aims to (1) design and evaluate a constructivist learning innovation model on a digital platform that promotes executive mathematics problem-solving and (2) develop and assess a prototype based on the designed model. A mixed-method research design was employed across two phases. Phase 1 involved designing and evaluating the learning model through document analysis and expert validation (n = 9). Phase 2 focused on developing and testing a prototype with experts (n = 15) and students (n = 30). Data collection utilized the Index of Item-Objective Congruence (IOC), System Usability Scale (SUS), and User Engagement Scale (UES). The model showed strong alignment with theoretical principles (IOC = 0.84). The prototype showed excellent usability (SUS = 91.0/100) and high engagement (UES = 4.26/5.00). Expert evaluations indicated high quality in content (M = 4.47, SD = 0.48), media (M = 4.40, SD = 0.50), and innovation design (M = 4.59, SD = 0.64). The findings validate the model's efficacy in promoting executive mathematics problem-solving skills through a constructivist digital platform approach.}
}
@article{199064,
title = {Natural languages: Berwick, R ‘Natural language computational complexity and generative capacity’ Comput. Artif. Intell. Vol 8 No 5 (1989) pp 423–441},
journal = {Knowledge-Based Systems},
volume = {3},
number = {1},
pages = {64},
year = {1990},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(90)90091-U},
url = {https://www.sciencedirect.com/science/article/pii/095070519090091U}
}
@article{YANG2010209,
title = {Creativity of student information system projects: From the perspective of network embeddedness},
journal = {Computers & Education},
volume = {54},
number = {1},
pages = {209-221},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2009.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360131509001997},
author = {Heng-Li Yang and Hsiu-Hua Cheng},
keywords = {Project team creativity, Network embeddedness, Affiliation network, Innovation climate, Centrality},
abstract = {Many companies have pursued innovation to obtain a competitive edge. Thus, educational reform focuses mainly on training creative students. This study adopted the concept of an affiliated network of projects to investigate how project embeddedness influences project team creativity. This work surveys 60 projects in a Management Information Systems Department of a University. Validity of the specific study hypotheses is tested by using moderate hierarchical regression analysis to determine how project embeddedness affects project team creativity and assess how the team innovation climate moderates the relationships between project embeddedness and project team creativity. Analytical results indicate a positive association between structural embeddedness and project team creativity, a negative relationship between positional embeddedness and project team creativity, and a positive influence of team innovation climate on the relationships between network embeddedness and project team creativity. An attempt is also made to understand the role of positional embeddedness by classifying the interactions based on the content of interactions. According to those results, positional embeddedness is positively related to project team creativity during problem–identification interaction; during solution–design interaction, positional embeddedness is negatively related to project team creativity. Results of this study explain the phenomena of divergent thinking and convergent thinking during creative development.}
}
@article{MARKMAN20181,
title = {Combining the Strengths of Naturalistic and Laboratory Decision-Making Research to Create Integrative Theories of Choice},
journal = {Journal of Applied Research in Memory and Cognition},
volume = {7},
number = {1},
pages = {1-10},
year = {2018},
issn = {2211-3681},
doi = {https://doi.org/10.1016/j.jarmac.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2211368117301778},
author = {Arthur B. Markman},
keywords = {Decision making, Naturalistic decision making, External validity, Internal validity},
abstract = {Naturalistic decision-making research contrasts with traditional laboratory research along a number of dimensions. It is typically more observational, more focused on expert performance, and more attentive to the context in which decisions are made than laboratory studies. This approach helps to shore up some of the weaknesses of laboratory research by providing incentive to develop integrative theories of choice and examining strong methods of problem solving in a choice domain. This paper contrasts the strengths and weaknesses of laboratory and naturalistic approaches to decision making. Then, it explores strategies for using both of these approaches as well as mathematical and computational modeling to find the optimal tradeoff between internal and external validity for research projects.}
}
@article{MURTAGH201637,
title = {Direct Reading Algorithm for Hierarchical Clustering},
journal = {Electronic Notes in Discrete Mathematics},
volume = {56},
pages = {37-42},
year = {2016},
note = {TCDM 2016 - 1st IMA Conference on Theoretical and Computational Discrete Mathematics, University of Derby},
issn = {1571-0653},
doi = {https://doi.org/10.1016/j.endm.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S157106531630213X},
author = {Fionn Murtagh and Pedro Contreras},
keywords = {Analytics, hierarchical clustering, ultrametric topology, p-adic and m-adic number representation, linear time computational complexity},
abstract = {Reading the clusters from a data set such that the overall computational complexity is linear in both data dimensionality and in the number of data elements has been carried out through filtering the data in wavelet transform space. This objective is also carried out after an initial transforming of the data to a canonical order. Including high dimensional, high cardinality data, such a canonical order is provided by row and column permutations of the data matrix. In our recent work, we induce a hierarchical clustering from seriation through unidimensional representation of our observations. This linear time hierarchical classification is directly derived from the use of the Baire metric, which is simultaneously an ultrametric. In our previous work, the linear time construction of a hierarchical clustering is studied from the following viewpoint: representing the hierarchy initially in an m-adic, m = 10, tree representation, followed by decreasing m to smaller valued representations that include p-adic representations, where p is prime and m is a non-prime positive integer. This has the advantage of facilitating a more direct visualization and hence interpretation of the hierarchy. In this work we present further case studies and examples of how this approach is very advantageous for such an ultrametric topological data mapping.}
}
@incollection{GORI2024339,
title = {Chapter 6 - Learning with constraints},
editor = {Marco Gori and Alessandro Betti and Stefano Melacci},
booktitle = {Machine Learning (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {339-442},
year = {2024},
isbn = {978-0-323-89859-1},
doi = {https://doi.org/10.1016/B978-0-32-389859-1.00013-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898591000131},
author = {Marco Gori and Alessandro Betti and Stefano Melacci},
keywords = {Support constraint machines, Learning from constraints, Lifelong learning, Constraint satisfaction, Penalty functions, Logic constraints, t-norms, Recurrent neural networks, Long short term memory networks (LSTM), Graphical models},
abstract = {This chapter provides a unified view of learning and inference in structured environments that are formally expressed as constraints that involve both data and tasks. A preliminary discussion has been put forward in Section 1.1.5, where we began proposing an abstract interpretation of the ordinary notion of constraint that characterizes human-based learning, reasoning, and decision processes. Here we make an effort to formalize those processes and explore the corresponding computational aspects. A first fundamental remark for the formulation of a sound theory is that most interesting real-world problems correspond with learning environments that are heavily structured, a feature that has been mostly neglected in the previous chapters on linear and kernel machines, as well as on deep networks. So far we have been mostly concerned with machine learning models where the agent takes a decision on patterns represented by x∈Rd, whereas we have mostly neglected the issue of constructing appropriate representations from the environmental information e∈E. The discussion in Section 1.1.5 has already stimulated the need of processing information organized as lists, trees, and graphs. Interestingly, in this chapter, it is shown that computational models, like recurrent neural networks and graph neural networks can also be regarded as a way for expressing appropriate constraints on environmental data by means of diffusion processes. In these cases the distinguishing feature of the computational model is that the focus is on uniform diffusion processes, whereas one can think of constraints that involve both data and tasks in a more general way. Basically, different vertexes of a graph that model the environment can be involved in different relations, thus giving rise to a different treatment. As a result, this yields richer computational mechanisms that involve the meaning attached to the different relations.}
}
@article{VANZUNDERT2010270,
title = {Effective peer assessment processes: Research findings and future directions},
journal = {Learning and Instruction},
volume = {20},
number = {4},
pages = {270-279},
year = {2010},
note = {Unravelling Peer Assessment},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2009.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959475209000814},
author = {Marjo {van Zundert} and Dominique Sluijsmans and Jeroen {van Merriënboer}},
keywords = {Peer assessment, Development of peer assessment skills, Attitudes towards peer assessment, Training of peer assessment skills},
abstract = {Despite the popularity of peer assessment (PA), gaps in the literature make it difficult to describe exactly what constitutes effective PA. In a literature review, we divided PA into variables and then investigated their interrelatedness. We found that (a) PA's psychometric qualities are improved by the training and experience of peer assessors; (b) the development of domain-specific skills benefits from PA-based revision; (c) the development of PA skills benefits from training and is related to students' thinking style and academic achievement, and (d) student attitudes towards PA are positively influenced by training and experience. We conclude with recommendations for future research.}
}
@article{RADTKE2022102355,
title = {Smart energy systems beyond the age of COVID-19: Towards a new order of monitoring, disciplining and sanctioning energy behavior?},
journal = {Energy Research & Social Science},
volume = {84},
pages = {102355},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102355},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004461},
author = {Jörg Radtke},
keywords = {Smart city, Smart energy governmentality, Social power framework, Energy transition conflict, Energy communities, Energy democracy, Michel Foucault},
abstract = {The Corona pandemic has led to the increased use of online tools throughout society, whether in business, education, or daily life. This shift to an online society has led social scientists to question the extent to which increased forms of control, surveillance and enforced conformity to ways of thinking, attitudes and behaviors can be promoted through online activities. This question arises overtly amidst a pandemic, but it also lurks behind the widespread diffusion of smart energy systems throughout the world and the increased use of smart meters in those systems. The extent to which forms of monitoring, disciplining and sanctioning of energy behavior and practices could come to reality is thus an important question to consider. This article does so using the ideas of Michel Foucault, together with research on smart energy systems and current trends in energy policy. The article closes with a discussion of energy democracy and democratic legitimacy in the context of possible effects of smart technologies on community energy systems.}
}
@incollection{JANELLE2015415,
title = {Time-Space in Geography},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {415-420},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.72070-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868720708},
author = {Donald G. Janelle},
keywords = {Activity patterns, Communication, Cyberinfrastructure, Geographic information systems, Human extensibility, Space-time path, Space-time prism, Time geography, Time-space compression, Time-space convergence, Time-space distanciation, Transportation, Travel},
abstract = {This article reviews the development of time-space perspectives in geography and exposes linkages between these perspectives and society's prevailing technologies for travel and communication. Special attention is given to the role of information, computation, and visualization technologies that shape the research practices that advance the potential to understand social organization and human activity behavior in a time-space context.}
}
@article{COMPANY2009592,
title = {Computer-aided sketching as a tool to promote innovation in the new product development process},
journal = {Computers in Industry},
volume = {60},
number = {8},
pages = {592-603},
year = {2009},
note = {Computer Aided Innovation},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2009.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S016636150900133X},
author = {Pedro Company and Manuel Contero and Peter Varley and Nuria Aleixos and Ferran Naya},
keywords = {Engineering design and innovation, CAI software, Computer-aided sketching},
abstract = {Sketching is an established part of engineering culture. Sketches assist product designers during the creative stages of design and help them to develop inventions. Paper-and-pencil sketching is highly useful but lacks functionalities, mainly because it is disconnected from the rest of the (computer-aided) design process. However, CAS tools are not yet as usable as paper-and-pencil, although they provide full integration with the subsequent phases of the design processes (CAD, CAE, CAM, etc.) and other interesting functionalities. We desire computer-aided sketching (CAS) tools which furnish users with the sketching environment they require to make full use of their conceptual design and innovation talents, while providing full integration with the subsequent phases of the design processes (CAD, CAE, CAM, etc.). In this paper we discuss the importance of sketching in conceptual design, we review the current situation of engineering sketching, and we then analyze the main characteristics which a successful and fully integrated CAS tool should include. We consider CAS, not as a single problem, but as at least three: thinking, prescriptive and talking sketches require different approaches to functionality. Finally, we present the current state of the art in CAS tools by describing the main features and outstanding problems of our own applications.}
}
@article{SREENATH1992121,
title = {A hybrid computation environment for multibody simulation},
journal = {Mathematics and Computers in Simulation},
volume = {34},
number = {2},
pages = {121-140},
year = {1992},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(92)90049-M},
url = {https://www.sciencedirect.com/science/article/pii/037847549290049M},
author = {N. Sreenath},
abstract = {A simulation architecture capable of generating the dynamical equations of a multibody system symbolically, automatically creating the computer code to simulate these equations numerically, run the simulation and display the results using animation and graphics is discussed. The power of object-oriented programming is used systematically to manipulate the symbolic, numeric and graphic modules and produce an effective tool for understanding the complicated motions of multibody systems. The architecture has been implemented in OOPSS (Object-Oriented Planar System Simulator) a software package written in a multilanguage (macsyma–fortran–lisp) environment. The package supports user interface capable of interactively modifying system parameters, change runtime initial conditions and introduce feedback control.}
}
@article{CARBONELL2016145,
title = {The role of metaphors in the development of technologies. The case of the artificial intelligence},
journal = {Futures},
volume = {84},
pages = {145-153},
year = {2016},
note = {SI: Metaphors in FS},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2016.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0016328715300902},
author = {Javier Carbonell and Antonio Sánchez-Esguevillas and Belén Carro},
keywords = {CLA, Metaphors, Artificial intelligence, Lakoff and Johnson},
abstract = {Technology plays a prominent role in configuring the way we live and work. In this paper we go further and think that it is a first level driver in the configuration of our deepest perceptions and has a paramount influence on shaping our worldviews and metaphors, though this aspect goes unnoticed for most of the population. In this paper we analyze how metaphors take action in the characterization of technologies, mainly emerging technologies, and in their evolution, and furthermore the impact of technologies and metaphors on the way we perceive our daily life. We analyze metaphors underlying brain nature and artificial intelligence, raising the connections between them and showing how metaphors in one of these fields impact on the way we understand the other. This fact has important consequences, for instance it conditions the evolution of computational systems, and we propose two scenarios for this evolution. This paper relies on the conceptual model and classification of metaphors proposed by Lakoff and Johnson in “Metaphors we live by”, from the orientational metaphors that show values and mantras, to the deepest structural metaphors that are reconfiguring how life is conceived. It also relies on CLA (Causal Layered Analysis) and to its reference book “CLA 2.0” in order to insert this analysis in a wider and future oriented framework and to analyze scenarios.}
}
@article{SULLIVAN2020246,
title = {Maritime 4.0 – Opportunities in Digitalization and Advanced Manufacturing for Vessel Development},
journal = {Procedia Manufacturing},
volume = {42},
pages = {246-253},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.078},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306430},
author = {Brendan P. Sullivan and Shantanoo Desai and Jordi Sole and Monica Rossi and Lucia Ramundo and Sergio Terzi},
keywords = {Maritime 4.0, Digitalization, Maritime Vessel Development, Industry 4.0},
abstract = {Maritime vessels are complex systems that generate and require the utilization of large amounts of data for maximum efficiency. The successful utilization of sensors and IoT in the industry requires a forward-thinking approach to leverage the benefits of Industry 4.0 in a more comprehensive manner. While processes and manufacturing processes can be improved and advanced through such efforts, in order the industry to be able to benefit from data generation, integrated approaches are necessary. In order to develop truly value-added vessels, we introduce a descriptive approach for understanding Maritime 4.0.}
}
@article{ZHANG2020259,
title = {Self-blast state detection of glass insulators based on stochastic configuration networks and a feedback transfer learning mechanism},
journal = {Information Sciences},
volume = {522},
pages = {259-274},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.02.058},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520301419},
author = {Qian Zhang and Weitao Li and Hua Li and Jianping Wang},
keywords = {Insulator self-blast state, Deep learning, Feedback transfer learning mechanism, Semantic error entropy},
abstract = {The self-blast state of a glass insulator directly affects the safety and reliability of transmission lines. To address the insufficient generalization ability of existing detection methods for insulator self-blast states and the drawbacks of deep neural network structures, the theories of transfer learning and closed-loop control are drawn upon to provide an intelligent detection method for the self-blast states of glass insulators. The method proposed in this paper is based on stochastic configuration networks and a feedback transfer learning mechanism. First, to reduce the redundancy of convolutional kernels in the channel extent, the interleaved group convolution strategy is employed to reconstruct the convolutional layers of the Inception network. Second, in view of the different feature applicabilities of different glass insulator images and based on the adaptive convolution module groups, the data structure of the dynamic feature space of insulator images is built with a certain mapping relationship from global to local. Then, the discriminative measure index is used to evaluate the discriminative information of the feature space to enhance the interpretability of the compact feature spance. Third, the fully connected feature vector of the compact feature space is sent to stochastic configuration networks (SCNs), which have universal approximation property to establish the classification criteria of the self-blast states of insulator images with generalization ability. Finally, an imitation of human thinking patterns is employed that exhibits repeated deliberation and comparison. Consequently, based on generalized error and entropy theories, the evaluation index of the objective function is established to evaluate the uncertain detection results of the self-blast states of glass insulator images in real time. Then, the dynamic transfer learning mechanism is constructed based on the constraint of the measurement index of uncertain detection results to realize self-optimizing regulation of the feature space that exhibits multihierarchy and discrimination and reconstructed classification criteria. The experimental results show that compared with other algorithms, the proposed method enhances the generalization ability and detection accuracy of the model.}
}
@article{LI2025125605,
title = {Traffic scenario frozen callback and adaptive neuro-fuzzy inference system based energy management strategy for connected fuel cell buses},
journal = {Applied Energy},
volume = {387},
pages = {125605},
year = {2025},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2025.125605},
url = {https://www.sciencedirect.com/science/article/pii/S0306261925003356},
author = {Menglin Li and Haoran Liu and Mei Yan and Boyu Guo and Jingda Wu and Guokai Jiang and Xupeng Fu},
keywords = {Connected fuel cell bus, Energy management strategy, Traffic scenario frozen callback, Adaptive neuro-fuzzy inference system},
abstract = {Exploring the full potential of energy savings for new energy vehicles in a future connected transportation system is a challenging task. To address how connected buses can leverage surrounding traffic information to improve their energy efficiency, an intelligent fuel cell bus energy management method based on traffic scenario frozen callback is proposed， which enables high real-time performance in online energy management. To tackle the issue of inconsistent data dimensions caused by random fluctuations in the number of vehicles in a fixed traffic flow, a traffic flow representation based on grid grayscale images is designed. Building upon this representation, a speed trajectory prediction model based on traffic scenario frozen callback is developed. Subsequently, offline historical global optimal data are used to construct a training dataset that links speed trajectories to optimal control sequences. An end-to-end energy management framework based on the adaptive neuro-fuzzy inference system (ANFIS) is presented and validated in scenarios that before entering bus station and after exiting bus station. Simulation results demonstrate that, the proposed energy management strategy (EMS) approaches the overall energy consumption of dynamic programming (DP), reaching 97.76 % and 98.82 % in the two kinds of scenarios of its performance, outperforms the other two comparative EMSs. In terms of timeliness, the computational time spent by the proposed EMS is only 0.2076 times and 0.1952 times that of traditional model predictive control (MPC)-based EMS in the separate scenario.}
}
@article{FAIRHALL2014ix,
title = {The receptive field is dead. Long live the receptive field?},
journal = {Current Opinion in Neurobiology},
volume = {25},
pages = {ix-xii},
year = {2014},
note = {Theoretical and computational neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2014.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0959438814000361},
author = {Adrienne Fairhall},
abstract = {Advances in experimental techniques, including behavioral paradigms using rich stimuli under closed loop conditions and the interfacing of neural systems with external inputs and outputs, reveal complex dynamics in the neural code and require a revisiting of standard concepts of representation. High-throughput recording and imaging methods along with the ability to observe and control neuronal subpopulations allow increasingly detailed access to the neural circuitry that subserves neural representations and the computations they support. How do we harness theory to build biologically grounded models of complex neural function?}
}
@article{SOPER2022126712,
title = {Quantifying the effect of solvent on the morphology of organic crystals using a statistical thermodynamics approach},
journal = {Journal of Crystal Growth},
volume = {591},
pages = {126712},
year = {2022},
issn = {0022-0248},
doi = {https://doi.org/10.1016/j.jcrysgro.2022.126712},
url = {https://www.sciencedirect.com/science/article/pii/S0022024822002007},
author = {Eleanor M. Soper and Radoslav Y. Penchev and Stephen M. Todd and Frank Eckert and Marc Meunier},
keywords = {A2. Solvent screening, A2. Particle engineering, A1. Surface chemistry, A1. Cosmo-RS, A1. Morphology},
abstract = {A method for predicting the effect of solvent on the morphology of organic crystals is presented, providing an efficient screening tool for identifying ideal crystallization solvents. The solvent effect is estimated by the computation of chemical potentials and activity coefficients of crystal surfaces using a first principles-based statistical thermodynamics approach. Density functional theory and COSMO-RS are utilized to determine the activity coefficients of the crystal growth faces of a selection of active pharmaceutical ingredients (APIs) in solvents across a broad range of polarities. The ability of COSMO-RS to predict and quantify the effects of solvent on crystal growth and morphology is assessed using hierarchical clustering to classify the solvents according to their overall interaction strength with the crystal faces. The COSMO-RS approach allows for a physical interpretation of the predictions in terms of surface polarity and is confirmed by comparison to published experimental data. Herein a methodology is reported for automated computation of the activity coefficients of all solvent-surface pairs directly from the drug crystal structure. The procedure goes beyond the traditional trial-and-error solvent selection process and has the potential to be used as a rapid computational screening tool in pharmaceutical drug development.}
}
@article{AQDA2011260,
title = {The impact of constructivist and cognitive distance instructional design on the learner’s creativity},
journal = {Procedia Computer Science},
volume = {3},
pages = {260-265},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910004199},
author = {Mahnaz Fatemi Aqda and Farideh Hamidi and Farhad Ghorbandordinejad},
keywords = {Instructional design, Distance education, E-learning, Creativity, Cognitivism, Constructivism},
abstract = {Creativity is at the heart of the 21st century educational work. Learner’ creativity or learner’s creative thinking skills are among the most important skills they need to be prepared for the knowledge society. The rapid development of technology in the modern era sheds light on the place and importance of creativity in education. Technology also has brought change in the way the students learn (collaboration strategy) and recently the computer-based instruction associated with electrical technologies has been a popular way of instruction that learning is no longer confined to classrooms (distance learning). Also in the case of the students if they want to take effective advantage of technology, they have to use the constructivist and cognitive skills (psychological learning theory). Recently education experts have tried to show how a distance instructional can be designed. The main question of this paper is what effects the distance instructional design based on the views of constructivism and cognitivism have on the learners’ creativity.The definition of distance education (e-learning), the instruction design based on constructivist view and its function in education and distance learning (e-learning), the instruction design based on cognitive view and its function in education and distance learning (e-learning), the factors affecting the creativity development and accommodation (comparison) the characteristics of the instructural context, and the impact of the appropriate learning on the creativity development according to these settings are among the other main points of this review.}
}
@article{KRAUSE20211094,
title = {The challenge of ensuring affordability, sustainability, consistency, and adaptability in the common metrics agenda},
journal = {The Lancet Psychiatry},
volume = {8},
number = {12},
pages = {1094-1102},
year = {2021},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(21)00122-X},
url = {https://www.sciencedirect.com/science/article/pii/S221503662100122X},
author = {Karolin Rose Krause and Sophie Chung and Maria da Luz {Sousa Fialho} and Peter Szatmari and Miranda Wolpert},
abstract = {Summary
Mental health research grapples with research waste and stunted field progression caused by inconsistent outcome measurement across studies and clinical settings, which means there is no common language for considering findings. Although recognising that no gold standard measures exist and that all existing measures are flawed in one way or another, anxiety and depression research is spearheading a common metrics movement to harmonise measurement, with several initiatives over the past 5 years recommending the consistent use of specific scales to allow read-across of measurements between studies. For this approach to flourish, however, common metrics must be acceptable and adaptable to a range of contexts and populations, and global access should be as easy and affordable as possible, including in low-income countries. Within a measurement landscape dominated by fixed proprietary measures and with competing views of what should be measured, achieving this goal poses a range of challenges. In this Personal View, we consider tensions between affordability, sustainability, consistency, and adaptability that, if not addressed, risk undermining the common metrics agenda. We outline a three-pronged way forward that involves funders taking more direct responsibility for measure development and dissemination; a move towards managing measure dissemination and adaptation via open-access measure hubs; and transitioning from fixed questionnaires to item banks. We argue that now is the time to start thinking of mental health metrics as 21st century tools to be co-owned and co-created by the mental health community, with support from dedicated infrastructure, coordinating bodies, and funders.}
}
@article{NAGANANDHINI2019548,
title = {Effective Diagnosis of Alzheimer’s Disease using Modified Decision Tree Classifier},
journal = {Procedia Computer Science},
volume = {165},
pages = {548-555},
year = {2019},
note = {2nd International Conference on Recent Trends in Advanced Computing ICRTAC -DISRUP - TIV INNOVATION , 2019 November 11-12, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.01.049},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920300570},
author = {S. Naganandhini and P. Shanmugavadivu},
keywords = {Alzheimer’s Disease, Feature Selection, Classification, Decision Tree, Early Detection, Hyper Parameter Tuning},
abstract = {Alzheimer’s disease (AD) is described as a severe form of the neural disorder that collectively degenerate the essential cognitive activities of a human being (thinking, memory retention, etc.,) in particular among the elderly individuals and eventually results in death. In addition to the adverse ill-health effects on the patients, AD imposes paramount responsibility and burden on the caretakers too. Several genetic and pathological traits and non-invasive diagnostic strategies are being vigorously investigated and explored to discover the early onset of this debilitating disease. The prognosis of AD assumes importance, as the deterioration of health due to its progression may be either contained or controlled. Moreover, early and accurate detection of AD helps medical practitioners to prescribe case-specific medical treatment procedure. Among the popular machine learning algorithms, decision tree technique is widely used for classification/prediction, due to its accuracy and speed.This research article presents a novel decision tree-based classification technique, with optimum hyper parametertuning, that is ideally suitable for AD diagnosis, even at the early stages of development. The performance of this newly proposed Decision Tree Classifier with Hyper Parameters Tuning (DTC-HPT) is validated on the Open Access Imaging Studies Series (OASIS) dataset that contains patients’ data on the different stages of AD. The DTC-HPT is designed with the primary objective to classify the nature of brain abnormality using the most relevantand potentially significant data attributes/parameters. The efficiency of DTC-HPT on AD classification is measured as Accuracy, Precision, Recall, and F1-Score. The correctness of AD classification by DTC-HPTwith an average accuracy of 99.10% endorse that this classification technique can be used for AD detection on the AD clinical datasets.}
}
@article{LOU2023541,
title = {Human Creativity in the AIGC Era},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {4},
pages = {541-552},
year = {2023},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872624000054},
author = {Yongqi Lou},
keywords = {AIGC, Artificial intelligence, Meaning-making, Paradigmatic innovation, Human values},
abstract = {Recent advances in artificial intelligence raise profound questions for humanity. Is the artificial intelligence-generated content (AIGC) technology merely a tool? Or is AIGC developing a level of creativity comparable to that of human beings? This essay explores the challenges and opportunities that AIGC technology brings to creativity, industry, and the ways of living of people around the world. These questions involve scale, authenticity, choice, and wisdom. Further, this essay addresses the core capabilities of future creative workers in the era of AIGC. The author believes that the ability to create meaning—meaning making—is and will remain a distinctive strength of human creativity in the AIGC era. To build on this strength, human beings must focus on six key areas: human-centered values, paradigmatic innovation, holistic experiences, cultural awareness, situational connections, and narrative reasoning. The best outcome for the AIGC is to make machines more machine-like and humans more human. Achieving this goal requires a cultural renaissance. We must break through the limits of computational rationality with the brilliance of humanity.}
}
@article{LILWALL1989268,
title = {Seismological Algorithms, Computational Methods and Computer Programs: Durk J. Doornbos (Editor), Academic Press/Harcourt Brace Jovanovich, 1988, 469 pp., £39.50, ISBN 0-12-220770-X},
journal = {Physics of the Earth and Planetary Interiors},
volume = {58},
number = {2},
pages = {268-269},
year = {1989},
issn = {0031-9201},
doi = {https://doi.org/10.1016/0031-9201(89)90062-9},
url = {https://www.sciencedirect.com/science/article/pii/0031920189900629},
author = {R.C. Lilwall}
}
@article{NEMETH2024101385,
title = {The interplay between subcortical and prefrontal brain structures in shaping ideological belief formation and updating},
journal = {Current Opinion in Behavioral Sciences},
volume = {57},
pages = {101385},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101385},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624000366},
author = {Dezső Németh and Teodóra Vékony and Gábor Orosz and Zoltán Sarnyai and Leor Zmigrod},
abstract = {History illustrates that economic crises and other sociopolitical threats often lead to a rise of polarization and radicalism, whereby people become more susceptible to intolerant political messages, including propaganda and ideological rhetoric. Political science, sociology, economics, and psychology have explored many dimensions of this phenomenon, yet a critical piece of the puzzle is still missing: what cognitive and neural mechanisms in the brain mediate between these threats and responsiveness to political messages? To answer this question, here, we present a theory that combines cognitive neuroscience theories, namely stress-induced memory shift and competitive cognitive processes, with political science. Our Threat-based Neural Switch Theory posits that the processing of political information, similarly to other information processing, is shaped by the competitive interaction between goal-directed and habitual processes. Threats, including resource overload or scarcity, can shift neural networks toward receptiveness to oversimplified political messages. This theory sets out a research program aimed at discovering the cognitive and neural underpinning of how situational factors alter brain functions and modify political information processing.}
}
@article{YU2022102230,
title = {Spatial processing rather than logical reasoning was found to be critical for mathematical problem-solving},
journal = {Learning and Individual Differences},
volume = {100},
pages = {102230},
year = {2022},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2022.102230},
url = {https://www.sciencedirect.com/science/article/pii/S1041608022001170},
author = {Mingxin Yu and Jiaxin Cui and Li Wang and Xing Gao and Zhanling Cui and Xinlin Zhou},
keywords = {Logical reasoning, Spatial processing, Mathematical problem-solving},
abstract = {Students' ability to solve mathematical problems is a standard mathematical skill; however, its cognitive correlates are unclear. Thus, this study aimed to examine whether spatial processing (mental rotation, paper folding, and the Corsi blocks test) and logical reasoning (abstract and concrete syllogisms) were correlated with mathematical problem-solving (word problems and geometric proofing) for college students. The regression results showed that after controlling for gender, age, general IQ, language processing, cognitive processing (visual perception, attention, and memory skills), and number sense and arithmetic computation skills, spatial processing skills still predicted mathematical problem-solving and geometry skills in Chinese college students. Contrastingly, logical reasoning measures related to syllogisms did not predict after controlling for these variables. Further, notably, it did not correlate significantly with geometry performance when no control variables were included. Our results suggest that spatial processing is a significant component of math skills involving word and geometry problems (even after controlling for multiple key cognitive factors).}
}
@article{PIERCE2024,
title = {Identifying Factors Associated With Heightened Anxiety During Breast Cancer Diagnosis Through the Analysis of Social Media Data on Reddit: Mixed Methods Study},
journal = {JMIR Cancer},
volume = {10},
year = {2024},
issn = {2369-1999},
doi = {https://doi.org/10.2196/52551},
url = {https://www.sciencedirect.com/science/article/pii/S2369199924000569},
author = {Joni Pierce and Mike Conway and Kathryn Grace and Jude Mikal},
keywords = {breast cancer, anxiety, NLP, natural language processing, mixed methods study, cancer diagnosis, social media apps, descriptive analysis, diagnostic progression, patient-centered care},
abstract = {Background
More than 85% of patients report heightened levels of anxiety following breast cancer diagnosis. Anxiety may become amplified during the early stages of breast cancer diagnosis when ambiguity is high. High levels of anxiety can negatively impact patients by reducing their ability to function physically, make decisions, and adhere to treatment plans, with all these elements combined serving to diminish the quality of life.
Objective
This study aimed to use individual social media posts about breast cancer experiences from Reddit (r/breastcancer) to understand the factors associated with breast cancer–related anxiety as individuals move from suspecting to confirming cancer diagnosis.
Methods
We used a mixed method approach by combining natural language processing–based computational methods with descriptive analysis. Our team coded the entire corpus of 2170 unique posts from the r/breastcancer subreddit with respect to key variables, including whether the post was related to prediagnosis, diagnosis, or postdiagnosis concerns. We then used Linguistic Inquiry and Word Count (LIWC) to rank-order the codified posts as low, neutral, or high anxiety. High-anxiety posts were then retained for deep descriptive analysis to identify key themes relative to diagnostic progression.
Results
After several iterations of data analysis and classification through both descriptive and computational methods, we identified a total of 448 high-anxiety posts across the 3 diagnostic categories. Our analyses revealed that individuals experience higher anxiety before a confirmed cancer diagnosis. Analysis of the high-anxiety posts revealed that the factors associated with anxiety differed depending on an individual’s stage in the diagnostic process. Prediagnosis anxiety was associated with physical symptoms, cancer-related risk factors, communication, and interpreting medical information. During the diagnosis period, high anxiety was associated with physical symptoms, cancer-related risk factors, communication, and difficulty navigating the health care system. Following diagnosis, high-anxiety posts generally discussed topics related to treatment options, physical symptoms, emotional distress, family, and financial issues.
Conclusions
This study has practical, theoretical, and methodological implications for cancer research. Content analysis reveals several possible drivers of anxiety at each stage (prediagnosis, during diagnosis, and postdiagnosis) and provides key insights into how clinicians can help to alleviate anxiety at all stages of diagnosis. Findings provide insights into cancer-related anxiety as a process beginning before engagement with the health care system: when an individual first notices possible cancer symptoms. Uncertainty around physical symptoms and risk factors suggests the need for increased education and improved access to trained medical staff who can assist patients with questions and concerns during the diagnostic process. Assistance in understanding technical reports, scheduling, and patient-centric clinician behavior may pinpoint opportunities for improved communication between patients and providers.}
}
@article{BELABES2015639,
title = {Designing Islamic Finance Programmes in a Competitive Educational Space: The Islamic Economics Institute Experiment},
journal = {Procedia - Social and Behavioral Sciences},
volume = {191},
pages = {639-643},
year = {2015},
note = {The Proceedings of 6th World Conference on educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.300},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815025604},
author = {Abderrazak Belabes and Ahmed Belouafi and Mohamed Daoudi},
keywords = {Curricula design, glocalization, competitiveness, Islamic finance, Islamic Economics Institute},
abstract = {This paper aims at exploring the experiment of the Islamic Economics Institute (IEI) of King Abdulaziz University in the design of the first ever Islamic finance higher educational programme at a Saudi Public University. An evaluative analytical framework has been utilized to meet this goal. Results show that the Institute has pursued a ‘glocalization’; thinking globally and acting locally approach in designing the programme. This approach aims at providing learners with ‘cutting-edge’ skills that will enhance their chances for employment at the local as well as regional markets. What are the advantages of this approach? And how can the Institute preserve its ‘distinctive research’ positioning that it has gained over the years, at the same time, being able to provide ‘world-class’ educational programmes?}
}
@article{STROBL2024104585,
title = {Counterfactual formulation of patient-specific root causes of disease},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104585},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104585},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000030},
author = {Eric V. Strobl},
keywords = {Root cause analysis, Causal inference, Precision medicine, Causal discovery, Computational medicine},
abstract = {Objective:
Root causes of disease intuitively correspond to root vertices of a causal model that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. We seek a definition of patient-specific root causes of disease that models the intuitive procedure routinely utilized by physicians to uncover root causes in the clinic.
Methods:
We use structural equation models, interventional counterfactuals and the recently developed mathematical formalization of backtracking counterfactuals to propose a counterfactual formulation of patient-specific root causes of disease matching clinical intuition.
Results:
We introduce a definition of patient-specific root causes of disease that climbs to the third rung of Pearl’s Ladder of Causation and matches clinical intuition given factual patient data and a working causal model. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence.
Conclusion:
The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.}
}
@article{FRADKIN20231013,
title = {Theory-Driven Analysis of Natural Language Processing Measures of Thought Disorder Using Generative Language Modeling},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {1013-1023},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223001258},
author = {Isaac Fradkin and Matthew M. Nour and Raymond J. Dolan},
keywords = {Computational psychiatry, GPT-2, Natural language processing, Psychosis, Schizophrenia, Thought disorder},
abstract = {Background
Natural language processing (NLP) holds promise to transform psychiatric research and practice. A pertinent example is the success of NLP in the automatic detection of speech disorganization in formal thought disorder (FTD). However, we lack an understanding of precisely what common NLP metrics measure and how they relate to theoretical accounts of FTD. We propose tackling these questions by using deep generative language models to simulate FTD-like narratives by perturbing computational parameters instantiating theory-based mechanisms of FTD.
Methods
We simulated FTD-like narratives using Generative-Pretrained-Transformer-2 by either increasing word selection stochasticity or limiting the model’s memory span. We then examined the sensitivity of common NLP measures of derailment (semantic distance between consecutive words or sentences) and tangentiality (how quickly meaning drifts away from the topic) in detecting and dissociating the 2 underlying impairments.
Results
Both parameters led to narratives characterized by greater semantic distance between consecutive sentences. Conversely, semantic distance between words was increased by increasing stochasticity, but decreased by limiting memory span. An NLP measure of tangentiality was uniquely predicted by limited memory span. The effects of limited memory span were nonmonotonic in that forgetting the global context resulted in sentences that were semantically closer to their local, intermediate context. Finally, different methods for encoding the meaning of sentences varied dramatically in performance.
Conclusions
This work validates a simulation-based approach as a valuable tool for hypothesis generation and mechanistic analysis of NLP markers in psychiatry. To facilitate dissemination of this approach, we accompany the paper with a hands-on Python tutorial.}
}
@article{PARIKH2024138,
title = {A comprehensive study on epigenetic biomarkers in early detection and prognosis of Alzheimer's disease},
journal = {Biomedical Analysis},
volume = {1},
number = {2},
pages = {138-153},
year = {2024},
issn = {2950-435X},
doi = {https://doi.org/10.1016/j.bioana.2024.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2950435X24000167},
author = {Dhruv Parikh and Manan Shah},
keywords = {Alzheimer’s Disease, Biomarkers, Detection, Epigenetics},
abstract = {Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by beta-amyloid plaques and tau tangles, disrupting brain cell communication, causing atrophy, and leading to cognitive decline. It poses a substantial global health challenge, necessitating urgent research. Molecular biomarkers, reflecting AD progression, have been identified in diverse bodily tissues. Notably, emerging epigenetic biomarkers introduce a novel dimension to AD pathophysiology. However, their precise role in early AD detection and prognosis remains unclear. This review classifies various epigenetic biomarkers, emphasizing their potential in early detection and prognosis. Various epigenetic biomarkers like DNA methylation, non-coding RNAs, histone modification, OMICS, and many more get significantly altered during AD; these biomarkers being distinctly expressed in normal conditions to AD offer a huge therapeutic benefit to stop the progression or worsening it. We explore the therapeutic implications and propose integration with existing diagnostic methods to intervene in AD progression, mitigating exacerbation. Addressing challenges, we envision the future scope of these biomarkers, emphasizing their synergy with computational approaches for enhanced AD detection. This review contributes to the field by proposing a multifaceted approach that combines epigenetic markers with computational analysis to improve early detection and facilitate timely therapeutic interventions. Furthermore, we discuss the economic implications of these biomarkers, proposing that their early application could significantly reduce the financial burden of AD by delaying the progression and severity of the disease.}
}
@article{TSIGKINOPOULOU2017518,
title = {Respectful Modeling: Addressing Uncertainty in Dynamic System Models for Molecular Biology},
journal = {Trends in Biotechnology},
volume = {35},
number = {6},
pages = {518-529},
year = {2017},
note = {Special Issue: Computation and Modeling},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2016.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167779916302311},
author = {Areti Tsigkinopoulou and Syed Murtuza Baker and Rainer Breitling},
abstract = {Although there is still some skepticism in the biological community regarding the value and significance of quantitative computational modeling, important steps are continually being taken to enhance its accessibility and predictive power. We view these developments as essential components of an emerging ‘respectful modeling’ framework which has two key aims: (i) respecting the models themselves and facilitating the reproduction and update of modeling results by other scientists, and (ii) respecting the predictions of the models and rigorously quantifying the confidence associated with the modeling results. This respectful attitude will guide the design of higher-quality models and facilitate the use of models in modern applications such as engineering and manipulating microbial metabolism by synthetic biology.}
}
@article{BATTAGLIA2025197,
title = {The paradox of the self-studying brain},
journal = {Physics of Life Reviews},
volume = {52},
pages = {197-204},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524001787},
author = {Simone Battaglia and Philippe Servajean and Karl J. Friston},
keywords = {Theoretical neuroscience, Consciousness, Perception, Introspection, Neurophenomenology, Brain},
abstract = {The paradox of a brain trying to study itself presents a conundrum, raising questions about self-reference, consciousness, psychiatric disorders, and the boundaries of scientific inquiry. By which means can this complex organ shift the focus of study towards itself? We aim at unpacking the intricacies of this paradox. Historically, this question has been raised by philosophers under different frameworks. Thanks to the development of novel techniques to study the brain on a functional and structural level - as well as neurostimulation protocols that can modulate its activity in selected areas - we now possess advanced methods to progress this intricate inquiry. Nonetheless, the broader implications of the brain's pursuit of understanding itself remain unclear to this day. Ultimately, the need to employ both perception and introspection has led to different formulations of consciousness. This creates a challenge, as evidence supporting one formulation does not necessarily support the other. By deconstructing the paradoxical nature of self understanding - from a philosophical and neuroscientific point of view - we may gain insights into the human brain, which could lead to improved understanding of self-awareness and consciousness.}
}
@article{VALLEETOURANGEAU2016195,
title = {Insight with hands and things},
journal = {Acta Psychologica},
volume = {170},
pages = {195-205},
year = {2016},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2016.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0001691816301755},
author = {Frédéric Vallée-Tourangeau and Sune Vork Steffensen and Gaëlle Vallée-Tourangeau and Miroslav Sirota},
keywords = {Problem solving, Insight, Task ecology, Enactivism, Methodological individualism},
abstract = {Two experiments examined whether different task ecologies influenced insight problem solving. The 17 animals problem was employed, a pure insight problem. Its initial formulation encourages the application of a direct arithmetic solution, but its solution requires the spatial arrangement of sets involving some degree of overlap. Participants were randomly allocated to either a tablet condition where they could use a stylus and an electronic tablet to sketch a solution or a model building condition where participants were given material with which to build enclosures and figurines. In both experiments, participants were much more likely to develop a working solution in the model building condition. The difference in performance elicited by different task ecologies was unrelated to individual differences in working memory, actively open-minded thinking, or need for cognition (Experiment 1), although individual differences in creativity were correlated with problem solving success in Experiment 2. The discussion focuses on the implications of these findings for the prevailing metatheoretical commitment to methodological individualism that places the individual as the ontological locus of cognition.}
}
@article{OMIDI20231,
title = {Molecular dynamic study of perovskite with improved thermal and mechanical stability for solar cells application: Calculation the final strength of the modeled atomic structures and the Young's modulus},
journal = {Engineering Analysis with Boundary Elements},
volume = {156},
pages = {1-7},
year = {2023},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2023.07.037},
url = {https://www.sciencedirect.com/science/article/pii/S0955799723003922},
author = {Mohammad Omidi and Zahra Karimi and Shirin Rahmani and Ali {Naderi Bakhtiyari} and Mahmood {Karimi Abdolmaleki}},
keywords = {Molecular dynamic, LAMMPS, Mechanical properties, Stress-strain, Solar cell},
abstract = {The Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) software is used to do molecular dynamics simulations, which entail modeling atom behavior over time using interatomic potentials. This approach is used to calculate perovskite structures' mechanical characteristics. For testing purposes, stress-strain curves are completed in the X, Y, and Z directions to represent the material's reaction to applied stress in terms of strain. The simulated structures are deformed inside the computational experiments using the loads and deform approaches command to get the stress-strain curves. The mechanical data of the structures may be retrieved by producing a deformation. These stress-strain curves are then compared in three axes of X, Y, and Z for XSnO3 (X= Cs, Rb, and K) at varied temperature and pressure settings. Finally, we applied this material to solar cell devices to find the performance of perovskite materials and calculated the efficiency.}
}
@incollection{GRANJOU20161,
title = {1 - The Time Beast},
editor = {Céline Granjou},
booktitle = {Environmental Changes},
publisher = {Elsevier},
pages = {1-43},
year = {2016},
isbn = {978-1-78548-026-3},
doi = {https://doi.org/10.1016/B978-1-78548-026-3.50001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480263500015},
author = {Céline Granjou},
keywords = {Analogism, Anthropophagy, Doctrine of the Apocalypse, Environmental change, Environmental humanities, Evolution, Multi-species ethnography, Nature/society partition, Plate tectonics, Sociology},
abstract = {Abstract:
This chapter will give insights into the historical shaping of the very peculiar notion of a nature without any future. We will retrace some of its roots in the secularization of Christian apocalypse, Newtonian physics and Linnean classification – while at the same time, Cartesian, Kantian and Hegelian philosophies perceived humans as subjects of reason, emancipation and civilization. We will revisit the way that, in the 19th Century, the Darwinian theory of evolution and, more recently, the development of geophysics, both contributed in the thinking of nature itself as able to instigate and create new futures.}
}
@incollection{GOMILA20121,
title = {1 - Introduction: Language as the Key Factor to Human Singularity},
editor = {Antoni Gomila},
booktitle = {Verbal Minds},
publisher = {Elsevier},
address = {London},
pages = {1-4},
year = {2012},
isbn = {978-0-12-385200-7},
doi = {https://doi.org/10.1016/B978-0-12-385200-7.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852007000011},
author = {Antoni Gomila},
keywords = {Cognitive architecture, verbal minds, cognitive control, flexibility, language},
abstract = {Publisher Summary
This chapter explains language as a distinctive human trait characterized by flexibility and creativity of human brains. Language is the sole symbol of communication and representation of individuality and sociality. The role of language on human thinking is to define the cultural and behavioral novelties of human thoughts. Lately, language is going through pendulum dynamics from past 30 years because the communicative approaches are becoming hegemonic. There has been a lot of new evidence to support the constitutive approach and its becoming mainstream, however, the modern critics of the cognitive view of language react in a paradoxical manner and do not support a cognitive role of language. This chapter aims at providing a defined role of language in human cognition to analyze the different ways, in which the relation between language and cognition has been conceived, to review the evidence amassed in recent years on this relationship, and to conclude multiple ways to conceive of the relationship at its best accounts for the facts.}
}
@article{XI2025106930,
title = {Depression detection based on the temporal-spatial-frequency feature fusion of EEG},
journal = {Biomedical Signal Processing and Control},
volume = {100},
pages = {106930},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106930},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424009881},
author = {Yang Xi and Ying Chen and Tianyu Meng and Zhu Lan and Lu Zhang},
keywords = {Depression detection, EEG, Temporal-spatial-frequency feature, Channel selection, Attention mechanism},
abstract = {Depression is a prevalent affective psychiatric disorder projected to be the leading contributor to the world’s disease burden by 2030. Due to its high prevalence and low recognition rate, an objective and effective detection method is urgently needed. Deep learning methods based on electroencephalography (EEG) have shown significant potential in depression detection. However, excessive channels can increase redundancy and computational complexity in EEG, while irrelevant channels may reduce accuracy. Additionally, existing models often overlook the complementarity between the temporal-, spatial-, and frequency-domain features of EEG, limiting their detection capabilities. To address these issues, we propose a method that fuses the temporal, spatial, and frequency domain features of EEG to enhance the detection accuracy while eliminating redundant channels. We introduce an EEG channel selection method based on frequency domain weighting that automatically adjusts the channel weights to select the EEG channels that best capture spatial information across the delta, theta, alpha, beta, and gamma bands, thereby optimizing the extraction of spatial-frequency features. In addition, we designed a multiscale spatiotemporal convolutional attention network to extract the spatiotemporal features of EEG. In this network, the multiscale convolutional attention module enhanced the model’s ability to capture spatial features, whereas the temporal trend-aware self-attention module extracted long-term temporal features by analyzing global correlations across different time points. Experimental results on the MODMA dataset show that our method achieved a 97.24% detection accuracy, surpassing current state-of-the-art models. This study offers a novel approach for constructing depression detection models, providing a foundation for future research and application.}
}
@article{JIANG2023104680,
title = {Using sequence mining to study students’ calculator use, problem solving, and mathematics achievement in the National Assessment of Educational Progress (NAEP)},
journal = {Computers & Education},
volume = {193},
pages = {104680},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104680},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522002512},
author = {Yang Jiang and Gabrielle A. Cayton-Hodges and Leslie {Nabors Oláh} and Ilona Minchuk},
keywords = {Calculator, Assessment, Problem solving, Sequence mining, Process data, Mathematics},
abstract = {Using appropriate tools strategically to aid in problem solving is a crucial skill identified in K-12 mathematics curriculum standards. As more assessments transition from paper-and-pencil to digital formats, a variety of interactive tools have been made available to test takers in digital testing platforms. Using onscreen calculators as an example, this study illustrates how process data obtained from student interactions with a digitally-based large-scale assessment can be leveraged to explore how and how well test takers use interactive tools and unveil their mathematical problem-solving processes and strategies. Specifically, sequence mining techniques using the longest common subsequence were applied on process data collected from a nationally representative sample who took the National Assessment of Educational Progress (NAEP) mathematics assessment to examine patterns of eighth-grade students’ calculator-use behaviors and the content of calculator input across a series of items. Sequences of keystrokes executed on the onscreen calculator by test takers were compared to reference sequences identified by content experts as proficient and efficient use to infer how well and how consistently the calculator was used. Results indicated that calculator-use behaviors and content differed by item characteristics. Students were more likely to use calculators on calculation-demanding items that involve intensive and complex computations than on items that involve simple or no computation. Using the calculator on more calculation-demanding items and using it in a manner that is more efficient and more similar to reference sequences on these items were related to higher mathematical proficiency. Findings have implications for assessment design and can be used in educational practices to provide educators with actionable process-related information on tool use and problem solving.}
}
@article{HOBBS2019100055,
title = {Estimating peak water demand: Literature review of current standing and research challenges},
journal = {Results in Engineering},
volume = {4},
pages = {100055},
year = {2019},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2019.100055},
url = {https://www.sciencedirect.com/science/article/pii/S2590123019300556},
author = {Ian Hobbs and Martin Anda and Parisa A. Bahri},
keywords = {Fixture unit, Peak water demand, Modified wistort method, Exhaustive enumeration method, Water demand calculator, Loading unit normalisation method},
abstract = {Since the 1940s, the models used to estimate peak water demand has been based largely upon variations and refinements of the probabilistic ‘fixture unit’ model. An approach originally advanced by Hunter (1940) in the United States of America (USA). Seeking an improved approach to the 'fixture unit' model, now widely recognised as outdated, is the key driving force behind the current work. Boosted by the development of computing power, the plumbing industry, researchers, and academics have, over the last decade, developed computational models as a means of estimating peak water demand. This paper builds on computational models embracing the estimation of peak water demand. A brief outline of the fixture unit and its limitations is provided with key developments in computational modeling comprising current developments from the USA and UK. A brief outline of computational models is presented: Modified Wistort Method (MWM); the Exhaustive Enumeration Method (EEM), and the Water Demand Calculator (WDC). Also presented, from the UK, is the Loading Unit Normalisation Assessment method (LUNA) aimed at an improved model to size domestic hot and cold-water systems. The analysis of the computational models suggests the WDC model is conceivably the most compatible with that of the plumbing industry's design requirements. Suggesting this model could easily be adapted to meet the requirements across international borders. Challenges for the international acceptance of the WDC are the field study requirements to determine p (probability of use) and q (fixture flow rate) values for all types of buildings.}
}
@article{SELIG2025105923,
title = {Using the kinematics of the RC linkage to find the degree of the adjoint representation of SE(3)},
journal = {Mechanism and Machine Theory},
volume = {206},
pages = {105923},
year = {2025},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2025.105923},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X25000126},
author = {J.M. Selig},
keywords = {Adjoint representation, Birational mappings, Study quadric, Assembly configurations},
abstract = {This work studies the projective algebraic variety formed from the closure of the adjoint representation of the group of rigid-body displacements, SE(3). This is motivated by asking how many assembly configurations a mechanism would have in general, if it was designed to keep six given lines in six linear line complexes. The main result is to find the degree of the variety defined by the adjoint representation and hence answer the motivating question. A simple special case is discussed, a mechanism that maintains a single given line reciprocal to three fixed lines from the regulus of a cylindrical hyperboloid of one sheet. The three dimensional variety defined in this way can be realised by an RC linkage. More specifically, the variety splits into two components each of which can be realised by an RC linkage. The homology of these 3-dimensional varieties, as subvarieties of the Study quadric, is found and used to determine the degree of the adjoint representation as an algebraic variety. The possible equations defining the variety determined by the adjoint representation of SE(3), are also discussed but no definitive result is found.}
}
@article{CUFFARO201235,
title = {Many worlds, the cluster-state quantum computer, and the problem of the preferred basis},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {43},
number = {1},
pages = {35-42},
year = {2012},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2011.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1355219811000694},
author = {Michael E. Cuffaro},
keywords = {Quantum computation, Quantum mechanics, Many worlds, Everettian interpretation, Quantum parallelism, Quantum speed-up, Cluster state, Measurement-based, One-way, Preferred basis problem},
abstract = {I argue that the many worlds explanation of quantum computation is not licensed by, and in fact is conceptually inferior to, the many worlds interpretation of quantum mechanics from which it is derived. I argue that the many worlds explanation of quantum computation is incompatible with the recently developed cluster state model of quantum computation. Based on these considerations I conclude that we should reject the many worlds explanation of quantum computation.}
}
@article{DAEMS2019101110,
title = {Building communities. Presenting a model of community formation and organizational complexity in southwestern Anatolia},
journal = {Journal of Anthropological Archaeology},
volume = {56},
pages = {101110},
year = {2019},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2019.101110},
url = {https://www.sciencedirect.com/science/article/pii/S027841651830237X},
author = {Dries Daems},
keywords = {Social complexity, Community formation, Sagalassos, Anatolian archaeology, Social interaction, Organizational complexity},
abstract = {In this paper, a model of community formation and organizational complexity is presented, focusing on the fundamental role of social interactions and information transmission for the development of complex social organisation. The model combines several approaches in complex systems thinking which has garnered increasing attention in archaeology. It is then outlined how this conceptual model can be applied in archaeology. In the absence of direct observations of constituent social interactions, archaeologists study the past through material remnants found in the archaeological record. People used their material surroundings to shape, structure and guide social interactions and practices in various ways. The presented framework shows how dynamics of social organisation and community formation can be inferred from these material remains. The model is applied on a case study of two communities, Sagalassos and Düzen Tepe, located in southwestern Anatolia during late Achaemenid to middle Hellenistic times (fifth to second centuries BCE). It is suggested that constituent interactions and practices can be linked to the markedly different forms of organizational structures and material surroundings attested in both communities. The case study illustrates how the presented model can help understand trajectories of socio-political structures and organizational complexity on a community level.}
}
@article{WADHWA2022101177,
title = {Most significant hotspot detection using improved particle swarm optimizers},
journal = {Swarm and Evolutionary Computation},
volume = {75},
pages = {101177},
year = {2022},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2022.101177},
url = {https://www.sciencedirect.com/science/article/pii/S2210650222001444},
author = {Ankita Wadhwa and Manish Kumar Thakur},
keywords = {Hotspot detection, Emergency response planning, Scan statistics, Improved particle swarm optimization, Geospatial data},
abstract = {Significant circular hotspot detection (SCHD) aims at identifying those circular regions in a spatial space where the occurrence of a particular activity is uncommonly higher than the surrounding areas. Plentiful societal applications make SCHD a problem of utmost interest. In many domains, detection of the most significant circular hotspot (MSCHD) is of further usefulness. Well-timed detection of the most significant hotspot helps crucially for short term response planning (STRP) in situations like a disease outbreak, police vigilance, etc. State of the art methods like SaTScan, identify circular hotspots by listing all possible circles in the search area, followed by a statistical significance test, making it a very high computational cost problem. Considering their high costs, these methods are inefficient in applications related to STRP. To reduce the computational time of SaTScan, two randomized versions of the SaTScan algorithm are presented in this paper. Further, the MSCHD problem is modeled as an optimization problem and three improved variants of Particle Swarm Optimizer (PSO) namely I-PSO, HCL-PSO and Ensemble PSO are applied to detect the most significant hotspots. The comparative and sensitivity analysis are performed using synthetic datasets. The comparative analysis of SaTScan and the presented PSO based schemes is made in terms of the quality of identified hotspots and the computational time. Results reveal that the PSO based schemes (I-PSO, HCL-PSO, and Ensemble PSO) are promising and far efficient than randomized and traditional SaTScan algorithms. Further, for the datasets containing only a single hotspot, the performance of all PSO based schemes is at par with each other. However, for datasets with more than one hotspot in the study area, HCL-PSO has lower average rank than I-PSO and Ensemble PSO schemes and hence seems more promising for MSCHD. The superiority of HCL-PSO based hotspot detection is also validated using Friedman Test. Finally, the presented schemes are applied to the case study of Chicago city for identification of different types of crime hotspots.}
}
@article{GARBOCZI2001455,
title = {Elastic moduli of a material containing composite inclusions: effective medium theory and finite element computations},
journal = {Mechanics of Materials},
volume = {33},
number = {8},
pages = {455-470},
year = {2001},
issn = {0167-6636},
doi = {https://doi.org/10.1016/S0167-6636(01)00067-9},
url = {https://www.sciencedirect.com/science/article/pii/S0167663601000679},
author = {E.J. Garboczi and J.G. Berryman},
keywords = {Fnite element, Effective medium theory, Concrete, Microstructure, Random elastic},
abstract = {Concrete is a good example of a composite material in which the inclusions (rocks and sand) are surrounded by a thin shell of altered matrix material and embedded in the normal matrix material. Concrete, therefore, may be viewed as consisting of a matrix material containing composite inclusions. Assigning each of these phases different linear elastic moduli results in a complicated effective elastic moduli problem. A new kind of differential effective medium theory (D-EMT) is presented in this paper that is intended to address this problem. The key new idea is that each inclusion particle, surrounded by a shell of another phase, is mapped onto an effective particle of uniform elastic moduli. The resulting simpler composite, with a normal matrix, is then treated in usual D-EMT. Before use, however, the accuracy of this method must be determined, as effective medium theory of any kind is an uncertain approximation. One good way to assess the accuracy of effective medium theory is to compare to exact results for known microstructures and phase moduli. Exact results, however, only exist for certain microstructures (e.g., dilute limit of inclusions) or special choices of the moduli (e.g., equal shear moduli). Recently, a special finite element method has been developed that can compute the linear elastic moduli of an arbitrary digital image in 2D or 3D. If a random microstructure can be represented with enough resolution by a digital image, then its elastic moduli can be readily computed. This method is used, after proper error analysis, to provide stringent tests of the new D-EMT equations, which are found to compare favorably to numerically exact finite element simulations, in both 2D and 3D, with varying composite inclusion particle size distributions.}
}
@article{CASTILLO2018165,
title = {In search of missing time: A review of the study of time in leadership research},
journal = {The Leadership Quarterly},
volume = {29},
number = {1},
pages = {165-178},
year = {2018},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2017.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1048984317300632},
author = {Elizabeth A. Castillo and Mai P. Trinh},
keywords = {Leadership, Time, Process, Computational science, Agent-based model},
abstract = {Many studies describe leadership as a dynamic process. However, few examine the passage of time as a critical dimension of that dynamism. This article illuminates this knowledge gap by conducting a systematic review of empirical studies on temporal effects of leadership to identify if and how time has been considered as a factor. After synthesizing key findings from the review, the article discusses methodological implications. We propose that a computational science approach, particularly agent-based modeling, is a fruitful path for future leadership research. This article contributes to leadership scholarship by shedding light on a missing variable (time) and offering a novel way to investigate the temporal, dynamic, emergent, and recursive aspects of leadership. We demonstrate the usefulness of agent-based modeling with an example of leader-member exchange relationship development.}
}
@article{MALINVERNI2025100727,
title = {Scaffolding Children's critical reflection on intelligent technologies: Opportunities from speculative fiction},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100727},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2025.100727},
url = {https://www.sciencedirect.com/science/article/pii/S2212868925000078},
author = {Laura Malinverni and Marie-Monique Schaper and Elisa Rubegni and Mariana Aki Tamashiro},
keywords = {AI literacy, Critical reflection, Speculative fiction, Children, Reflective AI literacy},
abstract = {Current technological development of Artificial Intelligence (AI) requires educational practices that address the social and ethical implications derived from these emerging technologies. To this end, an increasing number of educational practices are pursuing the goal of supporting children's critical reflection on these topics. Our research aims at understanding how speculative fiction-based resources can meet and respond to the goals of supporting children's critical reflection on AI technologies and their impact on society. Through revisiting relevant literature on these topics and critically analyzing our own practices in three different settings, we identify a set of opportunities and challenges oriented at guiding the design of resources capable of taking advantage of speculative fiction as a way to support critical reflection.}
}
@article{CHERNYSHOV20117408,
title = {System Identification Technique Application to Revealing Human-Operator Skills},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {7408-7413},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.00077},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016447968},
author = {K.R. Chernyshov},
keywords = {Human factors and errors, Identification, Information correlation, Sampled data, Skills, Stochastic systems},
abstract = {Abstract
A new approach to abnormal situations with regard for the heuristic regularities of human-operator thinking process is proposed. The regularities are revealed on basis of recording the motions of the human-operator eyes over the information field of the control board and processing the experimental data obtained. For data processing, a probability theoretical approach is utilized. Such an approach is based on involving the notion of consistency of measures of dependence of random variables. Within the approach, a set of the so called information correlations has been proposed to serve as a quantitative performance index of human-operator skills.}
}
@article{NAARANOJA2015611,
title = {Multi-ontology Sense Making – Decision Making of Project Core Team},
journal = {Procedia Manufacturing},
volume = {3},
pages = {611-617},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.280},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915002814},
author = {Marja Naaranoja},
keywords = {Sense making, Decision making, Ontology, Project, Core team, Construction industry},
abstract = {In order to understand core team's management task this paper studies the landscape of the decision making of the construction project core team. This paper uses multi-ontology sense making framework developed by Snowden. The four described situation illustrate the use of this framework. Firstly, a project core team create a project plan –timetable and cost estimate, that is supposed to be followed (rules and order) when making investment decision. Secondly, a project core team uses the plan but since the plan cannot be followed due to an unexpected situation the team changes the plan by calculating an optimal solution. In other words the team uses heuristic thinking when they change the rule (heuristics and order). Thirdly, the design group guides the design process by rules to get information for designing new facilities (rules and un-order). Fourthly, there are situations when the stakeholders have different kind of opinions in crisis and team cannot follow the preset orderly way of working (heuristics and un-order).}
}
@article{BALAKRISHNAN2025109810,
title = {Alzheimer's Disease detection and classification using optimized neural network},
journal = {Computers in Biology and Medicine},
volume = {187},
pages = {109810},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.109810},
url = {https://www.sciencedirect.com/science/article/pii/S001048252500160X},
author = {Nair Bini Balakrishnan and Anitha S. Pillai and Jisha {Jose Panackal} and P.S. Sreeja},
keywords = {Recurrent neural network, Moth flame optimization, Deep reinforcement learning, Alzheimer's detection},
abstract = {Alzheimer's disease (AD) is a degenerative neurological condition characterized by a progressive decline in cognitive abilities, resulting in memory impairment and limitations in performing daily tasks. Timely and precise identification of AD holds paramount importance for prompt intervention and enhanced patient prognosis. In this research, a novel approach to AD mechanism was developed by combining Deep Reinforcement Learning (DRL) with a Moth Flame Optimized Recurrent Neural Network (MFORNN). Initially, the brain MRI samples are gathered and preprocessed to discard the noise features and to improve their quality. Consequently, the MFO algorithm captures and selects the most informative and highly correlative features from the preprocessed images, making it easier for Recurrent Neural Networks (RNNs) to learn the temporal dependencies and patterns differentiating normal and AD-affected images. The DRL component fine-tunes the parameters of RNN through its reward-based mechanism, ensuring that the classifier produces accurate outcomes and reduces computational complexity. The Python tool was utilized to implement the outlined framework, with the outcomes showcased that the designed algorithm attained an accuracy of 99.31 %, precision of 99.24 %, recall of 99.43 %, and f-measure of 99.35 %. Ultimately, a comparative analysis was performed against established classifier models, affirming the superior performance of the proposed technique over conventional algorithms.}
}
@article{GUZMANURBINA2022109295,
title = {FIEMA, a system of fuzzy inference and emission analytics for sustainability-oriented chemical process design},
journal = {Applied Soft Computing},
volume = {126},
pages = {109295},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109295},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622004859},
author = {Alexander Guzman-Urbina and Kakeru Ouchi and Hajime Ohno and Yasuhiro Fukushima},
keywords = {Sustainability engineering, Emission analytics, Fuzzy systems, Data clustering},
abstract = {In the quest to achieve sustainable development goals, developments in sustainability-oriented chemical process design are key to innovation in the chemical industry, especially important for processes aiming for sustainable fuels. One of the greatest challenges is the difficulty of modeling the highly complex interactions among the design variables, such as catalyst technology attributes, and greenhouse gas emissions. Most of the computational aids crucial to deal with the complexity of chemical processes require data that is either unavailable or uncertain at an early stage of design. The multistage integrated system for sustainable design proposed in this paper boosts these computational aids by applying data science techniques to allow uncertainty to be handled more efficiently, thereby facilitating the modeling of the interactions between the properties of new materials or processes and sustainability indicators. In this system, current data connectivity methods are used to find paths of correlation among catalysts properties and greenhouse gas emissions. The key feature of the proposed system relies on the integration through multiple stages of Fuzzy Inference systems and a data-driven technique for Emissions Analytics, FIEMA.11FIEMA: Fuzzy Inference and Emission Analytics. The algorithm in FIEMA provides a semi-supervised learning approach to emission analytics: it determines data clusters by a C-means algorithm and subsequently builds fuzzy sets for multiple stages of input–output inference. The proposed FIEMA system was demonstrated in an effort to determine the optimal configurations of the properties of catalysts to minimize the probability of associated greenhouse gas emissions for a methanol production process. The results showed the potential of this approach to reduce the search space of catalyst material designs by suggesting promising configurations for oxygen storage capacity, mechanical strength, lifetime, size, and poisoning level. The research impacts of this study contribute to the development of clean fuels by a computationally-efficient system for early design, and by the determination of catalysts development paths that assure an actual reduction of the life-cycle emissions.}
}
@article{OLTEANU20161,
title = {Opportunity to communicate: The coordination between focused and discerned aspects of the object of learning},
journal = {The Journal of Mathematical Behavior},
volume = {44},
pages = {1-12},
year = {2016},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231231630116X},
author = {Lucian Olteanu},
keywords = {Algebra, Communication, Experience, Critical aspects, Opportunity to communicate},
abstract = {There are extensive concerns pertaining to the idea that students do not develop sufficient communication abilities in algebra and in mathematics more generally. This problem is at least partially related to their algebraic thinking. Although teaching should give students the opportunity to develop their ability to communicate, there are limited research insights as to why some forms of communication work better than others, and how and why instruction influences such communication. Two case studies are reported on in this article. The analysis of the opportunity to communicate was grounded in variation theory. Differences between focused aspects and discerned aspects of the object of learning are described. The results show that the coordination between the aspects focused on by the teacher and discerned by the students provides students with the opportunity to successfully communicate the content in algebra. In addition, the structure of the lesson influences the opportunity to communicate aspects of the content.}
}
@article{HO2024124656,
title = {Unraveling the complexity of amorphous solid as direct ingredient for conventional oral solid dosage form: The story of Elagolix Sodium},
journal = {International Journal of Pharmaceutics},
volume = {665},
pages = {124656},
year = {2024},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2024.124656},
url = {https://www.sciencedirect.com/science/article/pii/S0378517324008901},
author = {Raimundo Ho and Richard S. Hong and Joseph Kalkowski and Kevin C. Spence and Albert W. Kruger and Jayanthy Jayanth and Nandkishor K. Nere and Samrat Mukherjee and Ahmad Y. Sheikh and Shailendra V. Bordawekar},
keywords = {Amorphous drug substance, Impinging jet precipitation, Scale-up, Glass transition, Microstructure, Physical property control, Multi-scale modeling},
abstract = {Conventional solid oral dosage form development is not typically challenged by reliance on an amorphous drug substance as a direct ingredient in the drug product, as this may result in product development hurdles arising from process design and scale-up, control of physical quality attributes, drug product processability and stability. Here, we present the Chemistry, Manufacturing and Controls development journey behind the successful commercialization of an amorphous drug substance, Elagolix Sodium, a first-in-class, orally active gonadotropin-releasing hormone antagonist. The reason behind the lack of crystalline state was assessed via Molecular Dynamics (MD) at the molecular and inter-molecular level, revealing barriers for nucleation due to prevalence of intra-molecular hydrogen bond, repulsive interactions between active pharmaceutical ingredient (API) molecules and strong solvation effects. To provide a foundational basis for the design of the API manufacturing process, we modeled the solvent-induced plasticization behavior experimentally and computationally via MD for insights into molecular mobility. In addition, we applied material science tetrahedron concepts to link API porosity to drug product tablet compressibility. Finally, we designed the API isolation process, incorporating computational fluid dynamics modeling in the design of an impinging jet mixer for precipitation and solvent-dependent glass transition relationships in the cake wash, blow-down and drying process, to enable the consistent manufacture of a porous, non-sintered amorphous API powder that is suitable for robust drug product manufacturing.}
}
@article{KOSIKOV2021492,
title = {Data Enrichment in the Information Graphs Environment Based on a Specialized Architecture of Information Channels},
journal = {Procedia Computer Science},
volume = {190},
pages = {492-499},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013922},
author = {Sergey Kosikov and Larisa Ismailova and Viacheslav Wolfengagen},
keywords = {data enrichment, information channels, conceptual constructions, informational graph, applicative computations, semantics},
abstract = {The paper considers the possibility of constructing a specialized computing system oriented at the transmission of data through information channels, that are determined taking into account the semantics of the selected data. In the process of computations the data is connected with semantic characteristics that describe the channel of computations, which can be considered as a method of semantic data enrichment. The system of information channels as a whole can be considered as an information graph describing the structuring of the processed data. The information graph supports the data model in the form of a network, the framework of which are objects and the relationships between them. The paper proposes language tools for determining the information graph and interpretation tools that provide practical computations. The set of information channels that make up the information graph can be considered as a low-level tool for data enrichment. The paper studies the possibility of determining tools of higher level. An applicative type language is proposed for defining information graphs, the syntax and semantics of the language are specified. The proposed language can be considered as an intermediate level tool for defining semantics. A procedure is proposed for compiling the language into a low-level construct, preserving the semantics of the language. The supporting system for the proposed computing system includes a low-level language interpreter, as well as an intermediate-level language compiler into a low-level language. The supporting system is implemented in an applicative programming environment. Some elements of the supporting system were tested when developing applied information systems in the field of jurisprudence.}
}
@article{DODERO20221227,
title = {Ship design assessment through virtual prototypes},
journal = {Procedia Computer Science},
volume = {200},
pages = {1227-1236},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003325},
author = {Matteo Dodero and Serena Bertagna and Luca Braidotti and Alberto Marinò and Vittorio Bucci},
keywords = {Ship design, early stage design, Virtual Prototype, Product Model Program},
abstract = {The traditional design process has been developed, through time, by trial and error, following an evolutive approach. By following this procedure, the design team focused its attention on only one conceptual design alternative at a time, which is perfected step by step until the expected outcome is obtained. Nevertheless nowadays, due to the high complexity of ships and increasingly stringent operational requirements, this approach appears to be obsolete in a market where cost and time reduction is a fundamental parameter. Indeed, to be competitive in the shipbuilding market, very accurate information should be available since the beginning of the process, to allow the design team a 360-degree exploration of a high number of alternatives and then identify the best design solution in no time. In this paper a new, rational, design process, necessary to raise efficiency and effectiveness of ship design, is presented. By using a multi-purpose design software, the authors were able to create a Virtual Prototype of a case study ship with ease and little training, obtaining, since early-stage design phases, some outputs of interest (such as longitudinally weight distribution of ship structures, preliminary midship section, GZ curves and powering curves) without great computational efforts. The most important benefit of using only one multipurpose software instead of multiple specific ones lies in the elimination of remarking activities for switching from one software to another, reducing loss of data’s risks during the process.}
}
@article{GHABOUSSI201275,
title = {Unifying Principles for Sudden Transitions in All Systems},
journal = {Procedia Computer Science},
volume = {8},
pages = {75-80},
year = {2012},
note = {Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912000178},
author = {Jamshid Ghaboussi},
keywords = {Complex systems, Sudden transitions, System properties, Tipping points},
abstract = {All physical, natural, biological and socio-economic systems – also referred to as complex systems - have system-level properties that result from the interactions between their components. In most cases it is not possible to determine the complete system-level properties with the current state of our knowledge. Where there are system-level properties, the uncoupled form of those properties is the eigen-system consisting of system eigenvalues and eigenfunctions, even though at the present we are not able to determine them through modelling or observation. All systems operate in equilibrium states; small perturbations cause small changes. While these systems normally undergo gradual changes in their system-level properties, they can also undergo sudden transitions to new equilibrium states. New insights into these important transitions are proposed in this paper. In some mechanical systems transition occur when the smallest system eigenvalue goes to zero. It is proposed that the same principles apply to all systems. Transitions in all systems occur when at least one system eigenvalue goes to zero. Generalization of these principles to all systems will encourage new ways of thinking about systems and will suggest new research directions in studying these important major transitions, potentially leading to reliable methods for predicting their onset.}
}
@article{CELLERIER1990159,
title = {Psychology and computation: A response to Bunge},
journal = {New Ideas in Psychology},
volume = {8},
number = {2},
pages = {159-175},
year = {1990},
issn = {0732-118X},
doi = {https://doi.org/10.1016/0732-118X(90)90006-N},
url = {https://www.sciencedirect.com/science/article/pii/0732118X9090006N},
author = {G. Cellerier and J.-J. Ducret}
}
@article{GILL2019556,
title = {Holons on the Horizon: Re-Understanding Automation and Control},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {556-561},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.605},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319325261},
author = {Karamjit S Gill},
keywords = {data science, human-machine interaction, cybernetics, systems architecture, holon, symbiosis, valorisation, cultural architectures},
abstract = {In Re-Understanding Automation and Control in the era of digital automation of societal systems, we need to understand the inter-connected relations between knowledge, culture, technology and society. This in turn demands the exploration of social and cultural architectures, which facilitate them. Whilst computational model of data systems is built upon the bottom-up architecture, it is the top-down architecture of social and cultural contexts that synchronises the processing and outcomes of data systems, may they relate to organisational systems, heath and welfare systems, or institutional systems. It is this notion of the inter-dependence of the bottom-up and top-down architectures that makes us act beyond the linear gaze worldview of automation and control of production systems, and explore the multiplicity of interconnections between and across societal systems. In these horizons, we see the inter-connectedness between the unit and the whole, between the horizontal and vertical, and a symbiosis of hand and brain- an augmentation of the human and the machine. The ideas of inter-connectedness, augmentation and symbiosis lie at the core of holonic horizons. These horizons allow us to transcend the limit of the calculation and control model of automation, and enable the design of human-centred systems that valorise differences whilst utilising the richness and diversity of human-machine collaborations. When we envision these interactions and collaborations as a systems developmental process, we begin to visualise systems design from an interdependent perspective, which goes beyond the linear gaze of “utility”. The paper explores the ways holonic architectures engage us in the design process.}
}
@article{POLETTI20141803,
title = {Adverse childhood experiences worsen cognitive distortion during adult bipolar depression},
journal = {Comprehensive Psychiatry},
volume = {55},
number = {8},
pages = {1803-1808},
year = {2014},
issn = {0010-440X},
doi = {https://doi.org/10.1016/j.comppsych.2014.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010440X14001825},
author = {Sara Poletti and Cristina Colombo and Francesco Benedetti},
abstract = {Background
Cognitive distortion is a central feature of depression, encompassing negative thinking, dysfunctional personality styles and dysfunctional attitudes. It has been hypothesized that ACEs could increase the vulnerability to depression by contributing to the development of a stable negative cognitive style. Nevertheless, little research has been carried out on possible associations between adverse childhood experiences (ACEs) and cognitive distortion, and whether any gender differences exist.
Aim
The aim of this study was to examine the association between ACEs and cognitive distortions and possible differences between genders in a sample of patients affected by bipolar disorder.
Method
130 patients with bipolar disorder (BD) (46 men and 84 females), completed the Risky Family Questionnaire to assess ACEs and the Cognition Questionnaire (CQ) to assess cognitive distortions.
Results
A positive association was found between ACE and the CQ total score. Investigating the 5 dimensions assessed through the CQ, only the dimension “generalization across situations” was significantly associated to ACE. An interaction between ACE and gender was found for “generalization across situations”, while no differential effect among females and males was found for CQ total score.
Conclusion
This is the first study to report a relationship between negative past experiences and depressive cognitive distortions in subjects affected by BD. Growing in a family environment affected by harsh parenting seems to a cognitive vulnerability to depression; this effect is especially strong in females.}
}
@incollection{HANEES202523,
title = {Chapter 2 - The evolution of healthcare: bridging conventional and quantum computing},
editor = {Gayathri Nagasubramanian and S. Rakesh Kumar and Valentina {Emilia Balas}},
booktitle = {Quantum Computing for Healthcare Data},
publisher = {Academic Press},
pages = {23-42},
year = {2025},
series = {Advances in Biomedical Informatics},
isbn = {978-0-443-29297-2},
doi = {https://doi.org/10.1016/B978-0-443-29297-2.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292972000113},
author = {Ahamed Lebbe Hanees and Elakkiya Elango and Gnanasankaran Natarajan and Gayathri Nagasubramanian},
keywords = {Traditional computing, quantum computing healthcare, AI (artificial intelligence), medical diagnostics, real-time data analysis, personalized medicine, treatment optimization},
abstract = {Quantum computing is going to completely transform the medical field, replacing traditional computing. Pattern identification and predictive analysis are two types of jobs that quantum computing may expedite significantly. In contrast, classical computing, which is fueled by artificial intelligence techniques including machines learning and deep learning, primarily uses enormous datasets to feed these types of operations. This development is anticipated to enable real-time visualization of complex medical records, leading to faster and more accurate diagnoses via genetics and imaging information. Through leveraging the mathematical capabilities of quantum computing, healthcare providers may anticipate significant advancements in individualized medicine, therapy optimizing, and entire patient care, that will improve the standards for the delivery of medical services. Innovative and inventive collaborations exist involving Quantum Computing and the healthcare sector. Thus it was merely an extension of decades when the field of healthcare was drastically changed by quantum computing. The development of quantum technology means that an entirely new phase of computation is about to begin. Despite being a purely scientific subject, the laws of quantum mechanics and technologies have the power to completely transform a variety of industries, including healthcare. Quantum convergence presents enormous opportunities throughout the medical sector. Furthermore, technologies in general and AI in particular have made major improvements to the healthcare sector. These advances in technology are being used and transforming the healthcare industry to provide better care, assistance, and diagnosis. In the same way, quantum computing hopes to revolutionize how it is used in the field of healthcare. These days, personalized medicine that utilizes pharmaceutical kinetics human physiology and genomics is the standard. Therefore quantum computing is an ideal way to achieve this.}
}
@article{TSUTSUI201856,
title = {A Bayesian network model for supporting the formation of PSS design knowledge},
journal = {Procedia CIRP},
volume = {73},
pages = {56-60},
year = {2018},
note = {10th CIRP Conference on Industrial Product-Service Systems, IPS2 2018, 29-31 May 2018, Linköping, Sweden},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305171},
author = {Yusuke Tsutsui and Yosuke Kubota and Yoshiki Shimomura},
keywords = {Product-service systems, Design knowledge, Bayesian network},
abstract = {Recently, product-service systems (PSS) have drawn the interest of the manufacturing industry. Designing PSS to enhance the value of their core products, manufacturers should assume that their products are their strength or constraint and also derive the service solution logically. However, PSS design knowledge to determine the services suitable for manufacturers’ core products is unclear. As a result, determining a service solution that is compatible with their core products is difficult. This difficulty consequently prevents the manufacturing industry from realising high-quality PSS. To form PSS design knowledge efficiently, this study aims to support the analysis of the complicated and diverse relationships between product characteristics and service contents. Specifically, a Bayesian network model that represents the logical structure between the product characteristics and service contents common among existing PSS cases is constructed through computational learning based on statistical data on PSS cases.}
}
@article{YAN2025129868,
title = {SPRInT: Scaling Programmatic Reasoning for INstruction Tuning in mathematics},
journal = {Neurocomputing},
volume = {634},
pages = {129868},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129868},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005405},
author = {Yan Yan and Lin Li and Bo-Wen Zhang},
keywords = {Programmatic mathematical reasoning, Data augmentation, Data synthesis, Decoupled numeric dependencies, Logical inconsistencies},
abstract = {We present SPRInT, a novel approach for large-scale, cost-effective synthesis of instruction-tuning datasets, leveraging Program-of-Thoughts (PoT) to enhance mathematical reasoning capabilities. Through the SPRInT framework, we synthesized data from seven high-quality open-source math datasets (including GSM8K, MATH, AQuA), and developed InfinityMATH-a dataset containing over 100,000 samples generated from QA pairs, offering extensive coverage across various mathematical domains. The SPRInT model series, fine-tuned on InfinityMATH using open-source language and code models such as Llama2-7B, Mistral-7B, and CodeLlama-7B, achieved remarkable improvements in mathematical reasoning, with performance gains between 184.7% and 514.3%. In zero-shot settings, our SPRInT-CodeLlama-7B model surpassed MAmmoTH-Coder on widely-used benchmarks, including GSM8K (65.80% vs. 56.86%) and MATH (34.06% vs. 29.88%). To assess logical consistency in numerical transformations, we created the GSM8K+ and MATH＋ test sets by modifying the numerical values in the original datasets. While traditional models struggled with these alterations, the SPRInT models exhibited superior robustness. The InfinityMATH dataset is publicly available at https://huggingface.co/datasets/BAAI/InfinityMATH.}
}
@article{MCLEAN2020,
title = {Simulation Modeling as a Novel and Promising Strategy for Improving Success Rates With Research Funding Applications: A Constructive Thought Experiment},
journal = {JMIR Nursing},
volume = {3},
number = {1},
year = {2020},
issn = {2562-7600},
doi = {https://doi.org/10.2196/18983},
url = {https://www.sciencedirect.com/science/article/pii/S2562760020000125},
author = {Allen McLean and Wade McDonald and Donna Goodridge},
keywords = {simulation modeling, computational science, funding application, grant funding, grant writing, nursing, research, thought experiment, persuasive technology, peripheral vascular disease},
abstract = {Writing a successful grant or other funding applications is a requirement for continued employment, promotion, and tenure among nursing faculty and researchers. Writing successful applications is a challenging task, with often uncertain results. The inability to secure funding not only threatens the ability of nurse researchers to conduct relevant health care research but may also negatively impact their career trajectories. Many individuals and organizations have offered advice for improving success with funding applications. While helpful, those recommendations are common knowledge and simply form the basis of any well-considered, well-formulated, and well-written application. For nurse researchers interested in taking advantage of innovative computational methods and leading-edge analytical techniques, we propose adding the results from computer-based simulation modeling experiments to funding applications. By first conducting a research study in a virtual space, nurse researchers can refine their study design, test various assumptions, conduct experiments, and better determine which elements, variables, and parameters are necessary to answer their research question. In short, simulation modeling is a learning tool, and the modeling process helps nurse researchers gain additional insights that can be applied in their real-world research and used to strengthen funding applications. Simulation modeling is well-suited for answering quantitative research questions. Still, the design of these models can benefit significantly from the addition of qualitative data and can be helpful when simulating the results of mixed methods studies. We believe this is a promising strategy for improving success rates with funding applications, especially among nurse researchers interested in contributing new knowledge supporting the paradigm shift in nursing resulting from advances in computational science and information technology.}
}
@article{HONG2015671,
title = {Free will: A case study in reconciling phenomenological philosophy with reductionist sciences},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {671-727},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715001212},
author = {Felix T. Hong},
keywords = {Free will, Determinism, Quantum indeterminacy, Downward causation, Naturalizing phenomenology, Visual thinking},
abstract = {Phenomenology aspires to philosophical analysis of humans' subjective experience while it strives to avoid pitfalls of subjectivity. The first step towards naturalizing phenomenology — making phenomenology scientific — is to reconcile phenomenology with modern physics, on the one hand, and with modern cellular and molecular neuroscience, on the other hand. In this paper, free will is chosen for a case study to demonstrate the feasibility. Special attention is paid to maintain analysis with mathematical precision, if possible, and to evade the inherent deceptive power of natural language. Laplace's determinism is re-evaluated along with the concept of microscopic reversibility. A simple and transparent version of proof demonstrates that microscopic reversibility is irreconcilably incompatible with macroscopic irreversibility, contrary to Boltzmann's claim. But the verdict also exalts Boltzmann's statistical mechanics to the new height of a genuine paradigm shift, thus cutting the umbilical cord linking it to Newtonian mechanics. Laplace's absolute determinism must then be replaced with a weaker form of causality called quasi-determinism. Biological indeterminism is also affirmed with numerous lines of evidence. The strongest evidence is furnished by ion channel fluctuations, which obey an indeterministic stochastic phenomenological law. Furthermore, quantum indeterminacy is shown to be relevant in biology, contrary to the opinion of Erwin Schrödinger. In reconciling phenomenology of free will with modern sciences, three issues — alternativism, intelligibility and origination — of free will must be accounted for. Alternativism and intelligibility can readily be accounted for by quasi-determinism. In order to account for origination of free will, the concept of downward causation must be invoked. However, unlike what is commonly believed, there is no evidence that downward causation can influence, shield off, or overpower low-level physical forces already known to physicists. Quasi-determinism offers an escape route: The possibility that downward causation arising from hierarchical organization of biological structures can modify dispersions of physical laws remains open. Empirical evidence in support of downward causation is scanty but nevertheless exists. Still, origination of free will must be considered an unsolved problem at present. It is demonstrated that objectivity does not guarantee scientific rigor in the study of complex phenomena, such as human creativity. In its replacement, universality and overall consistency between a theory and empirical evidence must be maintained. Visual thinking is proposed as a reasoning tool to ensure universality and overall consistency through inference to the best explanation.}
}
@article{SMITH2020108208,
title = {Imprecise action selection in substance use disorder: Evidence for active learning impairments when solving the explore-exploit dilemma},
journal = {Drug and Alcohol Dependence},
volume = {215},
pages = {108208},
year = {2020},
issn = {0376-8716},
doi = {https://doi.org/10.1016/j.drugalcdep.2020.108208},
url = {https://www.sciencedirect.com/science/article/pii/S0376871620303732},
author = {Ryan Smith and Philipp Schwartenbeck and Jennifer L. Stewart and Rayus Kuplicki and Hamed Ekhtiari and Martin P. Paulus},
keywords = {Substance use disorders, Computational modeling, Active inference, Learning rate, Explore-exploit dilemma, Directed exploration},
abstract = {Background
Substance use disorders (SUDs) are a major public health risk. However, mechanisms accounting for continued patterns of poor choices in the face of negative life consequences remain poorly understood.
Methods
We use a computational (active inference) modeling approach, combined with multiple regression and hierarchical Bayesian group analyses, to examine how treatment-seeking individuals with one or more SUDs (alcohol, cannabis, sedatives, stimulants, hallucinogens, and/or opioids; N = 147) and healthy controls (HCs; N = 54) make choices to resolve uncertainty within a gambling task. A subset of SUDs (N = 49) and HCs (N = 51) propensity-matched on age, sex, and verbal IQ were also compared to replicate larger group findings.
Results
Results indicate that: (a) SUDs show poorer task performance than HCs (p = 0.03, Cohen’s d = 0.33), with model estimates revealing less precise action selection mechanisms (p = 0.004, d = 0.43), a lower learning rate from losses (p = 0.02, d = 0.36), and a greater learning rate from gains (p = 0.04, d = 0.31); and (b) groups do not differ significantly in goal-directed information seeking.
Conclusions
Findings suggest a pattern of inconsistent behavior in response to positive outcomes in SUDs combined with a tendency to attribute negative outcomes to chance. Specifically, individuals with SUDs fail to settle on a behavior strategy despite sufficient evidence of its success. These learning impairments could help account for difficulties in adjusting behavior and maintaining optimal decision-making during and after treatment.}
}
@article{WANG2024109848,
title = {An effective DOA estimation method for low SIR in small-size hydrophone array},
journal = {Applied Acoustics},
volume = {217},
pages = {109848},
year = {2024},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2023.109848},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X23006461},
author = {Wenbo Wang and Ye Li and TongSheng Shen and Feng Liu and DeXin Zhao},
abstract = {The estimation ability of traditional direction of arrival (DOA) estimation methods is relatively fragile in small-size hydrophone arrays with limited space. Especially in low signal to interference ratio (SIR), the strong interference signals may submerge some weak signals of interest (SOI) and make DOA estimation difficult in response to this issue. This paper introduces an improved sparse DOA estimation method for practical multi-objective DOA estimation in complex scenarios. The main work is to introduce a noise weight constraint in the sparse iterative covariance process. It leads the algorithm to output sparse peaks and smooth spatial energy spectra and achieve faster fitting while reducing the probability of false peaks. The algorithm can complete DOA estimation of the multi-target reliably without prior information of sources. Then, we propose a fast region grid refinement method based on allocation reconstruction to increase angle resolution. The method increases the accuracy of multi-objective DOA estimation while reducing computational costs. Finally, simulation and experiment have verified the method's effectiveness.}
}
@article{SEOW2021436,
title = {How Local and Global Metacognition Shape Mental Health},
journal = {Biological Psychiatry},
volume = {90},
number = {7},
pages = {436-446},
year = {2021},
note = {BPS 90/7Pharmacologic Prevention and Treatment of Posttraumatic Stress Disorder},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2021.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0006322321013299},
author = {Tricia X.F. Seow and Marion Rouault and Claire M. Gillan and Stephen M. Fleming},
keywords = {Confidence, Mental health, Metacognition, Self-beliefs, Self-efficacy, Transdiagnostic psychiatry},
abstract = {Metacognition is the ability to reflect on our own cognition and mental states. It is a critical aspect of human subjective experience and operates across many hierarchical levels of abstraction—encompassing local confidence in isolated decisions and global self-beliefs about our abilities and skills. Alterations in metacognition are considered foundational to neurologic and psychiatric disorders, but research has mostly focused on local metacognitive computations, missing out on the role of global aspects of metacognition. Here, we first review current behavioral and neural metrics of local metacognition that lay the foundation for this research. We then address the neurocognitive underpinnings of global metacognition uncovered by recent studies. Finally, we outline a theoretical framework in which higher hierarchical levels of metacognition may help identify the role of maladaptive metacognitive evaluation in mental health conditions, particularly when combined with transdiagnostic methods.}
}
@article{DUAN2023103365,
title = {Mining multigranularity decision rules of concept cognition for knowledge graphs based on three-way decision},
journal = {Information Processing & Management},
volume = {60},
number = {4},
pages = {103365},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103365},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323001024},
author = {Jiangli Duan and Guoyin Wang and Xin Hu and Deyou Xia and Di Wu},
keywords = {Granular computing, Cognitive intelligence, Concept cognition, Knowledge graph, Three-way decision},
abstract = {Machine understanding and thinking require prior knowledge consisting of explicit and implicit knowledge. The current knowledge base contains various explicit knowledge but not implicit knowledge. As part of implicit knowledge, the typical characteristics of the things referred to by the concept are available by concept cognition for knowledge graphs. Therefore, this paper attempts to realize concept cognition for knowledge graphs from the perspective of mining multigranularity decision rules. Specifically, (1) we propose a novel multigranularity three-way decision model that merges the ideas of multigranularity (i.e., from coarse granularity to fine granularity) and three-way decision (i.e., acceptance, rejection, and deferred decision). (2) Based on the multigranularity three-way decision model, an algorithm for mining multigranularity decision rules is proposed. (3) The monotonicity of positive or negative granule space ensured that the positive (or negative) granule space from coarser granularity does not need to participate in the three-classification process at a finer granularity, which accelerates the process of mining multigranularity decision rules. Moreover, the experimental results show that the multigranularity decision rule is better than the two-way decision rule, frequent decision rule and single granularity decision rule, and the monotonicity of positive or negative granule space can accelerate the process of mining multigranularity decision rules.}
}
@incollection{TVERSKY197817,
title = {2 - Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty},
editor = {PETER DIAMOND and MICHAEL ROTHSCHILD},
booktitle = {Uncertainty in Economics},
publisher = {Academic Press},
pages = {17-34},
year = {1978},
isbn = {978-0-12-214850-7},
doi = {https://doi.org/10.1016/B978-0-12-214850-7.50008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780122148507500085},
author = {Amos Tversky and Daniel Kahneman},
abstract = {Publisher Summary
Many decisions are based on beliefs concerning the likelihood of uncertain events such as the outcome of an election, the guilt of a defendant, or the future value of the dollar. Occasionally, beliefs concerning uncertain events are expressed in numerical form as odds or subjective probabilities. In general, the heuristics are quite useful, but sometimes they lead to severe and systematic errors. The subjective assessment of probability resembles the subjective assessment of physical quantities such as distance or size. These judgments are all based on data of limited validity, which are processed according to heuristic rules. However, the reliance on this rule leads to systematic errors in the estimation of distance. This chapter describes three heuristics that are employed in making judgments under uncertainty. The first is representativeness, which is usually employed when people are asked to judge the probability that an object or event belongs to a class or event. The second is the availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development, and the third is adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available.}
}
@article{CHIU2024100282,
title = {Developing and validating measures for AI literacy tests: From self-reported to objective measures},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100282},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100282},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000857},
author = {Thomas K.F. Chiu and Yifan Chen and King Woon Yau and Ching-sing Chai and Helen Meng and Irwin King and Savio Wong and Yeung Yam},
keywords = {AI literacy, Instrument, K-12 education, AI education, Co-design process, Measures},
abstract = {The majority of AI literacy studies have designed and developed self-reported questionnaires to assess AI learning and understanding. These studies assessed students' perceived AI capability rather than AI literacy because self-perceptions are seldom an accurate account of true measures. International assessment programs that use objective measures to assess science, mathematical, digital, and computational literacy back up this argument. Furthermore, because AI education research is still in its infancy, the current definition of AI literacy in the literature may not meet the needs of young students. Therefore, this study aims to develop and validate an AI literacy test for school students within the interdisciplinary project known as AI4future. Engineering and education researchers created and selected 25 multiple-choice questions to accomplish this goal, and school teachers validated them while developing an AI curriculum for middle schools. 2390 students in grades 7 to 9 took the test. We used a Rasch model to investigate the discrimination, reliability, and validity of the items. The results showed that the model met the unidimensionality assumption and demonstrated a set of reliable and valid items. They indicate the quality of the test items. The test enables AI education researchers and practitioners to appropriately evaluate their AI-related education interventions.}
}
@article{CHANG2025100364,
title = {Co-designing AI with youth partners: Enabling ideal classroom relationships through a novel AI relational privacy ethical framework},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100364},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100364},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000049},
author = {Michael Alan Chang and Mike Tissenbaum and Thomas M. Philip and Sidney K. D’Mello},
keywords = {Architectures for educational technology systems, Cooperative/collaborative learning, Cultural and social implications, Interdisciplinary studies, participatory design, AI-supported collaboration},
abstract = {In recent years, the design of AI-based tools for educational spaces have been largely driven by researchers who impart their past expertises, experiences, and perspectives in the design process. While this typically leads to technically feasible designs and are often well-grounded in theories of learning, youth agency is typically limited in this process. In this paper, we argue that designers have a significant ethical responsibility to incorporate youth voices – in particular, their dreams and concerns – into the design of AI tools starting from conception. This need is particularly important as new applications for AI, such as AI-supported collaboration, introduce new surveillance vectors into classroom spaces. Drawing from recent scholarship which advances ethics and relationality in participatory co-design with youth, we introduce a co-design methodology in which youth are supported in imagining expansive technical possibilities for K-12 public schools, grounded within affordances, limitations, and tradeoffs of AI and machine learning techniques. This approach is demonstrated through our Learning Futures Workshop, which brought together 30 historically minoritized youth in conversation with experts in both education and technology. Through detailed case study on the enactment of the workshop, including a thematic analysis of the activities the youth engaged in and their outputs, we identified new, expansive relational possibilities for AI, ethical commitments to support the design, and finally, developed a novel AI Relational Privacy ethical framework that supports the design of new collaborative AI platforms. We conclude by connecting these findings and frameworks to the design of newly enacted AI-based applications and underlying data infrastructures.}
}
@article{YEAP1988297,
title = {Towards a computational theory of cognitive maps},
journal = {Artificial Intelligence},
volume = {34},
number = {3},
pages = {297-360},
year = {1988},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(88)90064-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370288900641},
author = {Wai K. Yeap},
abstract = {A computational theory of cognitive maps is developed which can explain some of the current findings about cognitive maps in the psychological literature and which provides a coherent framework for future development. The theory is tested with several computer implementations which demonstrate how the shape of the environment is computed and how one's conceptual representation of the environment is derived. We begin with the idea that the cognitive mapping process should be studied as two loosely coupled modules: The first module, known as the raw cognitive map, is computed from information made explicit in Marr's 212-D sketch and not from high-level descriptions of what we perceive. The second module, known as the full cognitive map, takes the raw cognitive map as input and produces different “abstract representations” for solving high-level spatial tasks faced by the individual.}
}
@article{KRAJNAK2021132976,
title = {Reactive islands for three degrees-of-freedom Hamiltonian systems},
journal = {Physica D: Nonlinear Phenomena},
volume = {425},
pages = {132976},
year = {2021},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2021.132976},
url = {https://www.sciencedirect.com/science/article/pii/S0167278921001330},
author = {Vladimír Krajňák and Víctor J. García-Garrido and Stephen Wiggins},
keywords = {Phase space of Hamiltonian systems, Stable and unstable manifolds, Normally hyperbolic invariant manifolds, Reactive islands, Spherinders, Lagrangian descriptors},
abstract = {We develop the geometrical, analytical, and computational framework for reactive island theory for three degrees-of-freedom time-independent Hamiltonian systems. In this setting, the dynamics occurs in a 5-dimensional energy surface in phase space and is governed by four-dimensional stable and unstable manifolds of a three-dimensional normally hyperbolic invariant sphere. The stable and unstable manifolds have the geometrical structure of spherinders and we provide the means to investigate the ways in which these spherinders and their intersections determine the dynamical evolution of trajectories. This geometrical picture is realized through the computational technique of Lagrangian descriptors. In a set of trajectories, Lagrangian descriptors allow us to identify the ones closest to a stable or unstable manifold. Using an approximation of the manifold on a surface of section we are able to calculate the flux between two regions of the energy surface.}
}
@article{MAYER2024115725,
title = {Site heterogeneity and broad surface-binding isotherms in modern catalysis: Building intuition beyond the Sabatier principle},
journal = {Journal of Catalysis},
volume = {439},
pages = {115725},
year = {2024},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2024.115725},
url = {https://www.sciencedirect.com/science/article/pii/S002195172400438X},
author = {James M. Mayer},
abstract = {Learning the science of heterogeneous catalysis and electrocatalysis always starts with the simple case of a flat, uniform surface with an ideal adsorbate. It has of course been recognized for a century that real catalysts are more complicated. For the increasingly complex catalysts of the 21st century, this Perspective argues that surface heterogeneity and non-ideal binding isotherms are central features, and their implications need to be incorporated in current thinking. A variety of systems are described herein where catalyst complexity leads to broad, non-Langmuirian surface isotherms for the binding of hydrogen atoms – and this occurs even for ideal, flat Pt(111) surfaces. Modern catalysis employs nanoscale materials whose surfaces have substantial step, edge, corner, impurity, and other defect sites, and they increasingly have both metallic and non-metallic elements MnXm, including metal oxides, chalcogenides, pnictides, carbides, doped carbons, etc. The surfaces of such catalysts are often not crystal facets of the bulk phase underneath, and they typically have a variety of potential active sites. Catalytic surfaces in operando are often non-stoichiometric, amorphous, dynamic, and impure, and often vary from one part of the surface to another. Understanding of the issues that arise at such nanoscale, multi-element catalysts is just beginning to emerge. Yet these catalysts are widely discussed using Brønsted/Bell-Evans-Polanyi (BEP) relations, volcano plots, Tafel slopes, the Butler-Volmer equation, and other linear free energy relations (LFERs), which all depend on the implicit assumption that the active sites are “similar” and that surface adsorption is close to ideal. These assumptions underly the ubiquitous intuition based on the Sabatier Principle, that the fastest catalysis will occur when key intermediates have free energies of adsorption that are not too strong nor too weak. Current catalysis research often aims to minimize the complexity of non-ideal isotherms through experimental and computational design (e.g., the use of single crystal surfaces), and these studies are the foundation of the field. In contrast, this Perspective argues that the heterogeneity of binding sites and binding energies is an inherent strength of these catalysts. This diversity makes many nanoscale catalysts inherently a high-throughput screen wrapped in a tiny package. Only by making the heterogeneity part of the foundation of catalysis models, sorting the types of active sites and dissecting non-ideal binding isotherms, will modern catalysis learn to harness the inherent diversity of real catalysts. Controlling and exploiting diversity rather than avoiding it will help to optimize complex modern catalysts and catalytic conditions.}
}
@article{RIVIERE2024637,
title = {Proceedings from the inaugural Artificial Intelligence in Primary Immune Deficiencies (AIPID) conference},
journal = {Journal of Allergy and Clinical Immunology},
volume = {153},
number = {3},
pages = {637-642},
year = {2024},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0091674924000332},
author = {Jacques G. Rivière and Pere {Soler Palacín} and Manish J. Butte},
keywords = {Artificial intelligence, machine learning, large language models, natural language processing, electronic health records, inborn errors of immunity, diagnosis, ethics},
abstract = {Here, we summarize the proceedings of the inaugural Artificial Intelligence in Primary Immune Deficiencies conference, during which experts and advocates gathered to advance research into the applications of artificial intelligence (AI), machine learning, and other computational tools in the diagnosis and management of inborn errors of immunity (IEIs). The conference focused on the key themes of expediting IEI diagnoses, challenges in data collection, roles of natural language processing and large language models in interpreting electronic health records, and ethical considerations in implementation. Innovative AI-based tools trained on electronic health records and claims databases have discovered new patterns of warning signs for IEIs, facilitating faster diagnoses and enhancing patient outcomes. Challenges in training AIs persist on account of data limitations, especially in cases of rare diseases, overlapping phenotypes, and biases inherent in current data sets. Furthermore, experts highlighted the significance of ethical considerations, data protection, and the necessity for open science principles. The conference delved into regulatory frameworks, equity in access, and the imperative for collaborative efforts to overcome these obstacles and harness the transformative potential of AI. Concerted efforts to successfully integrate AI into daily clinical immunology practice are still needed.}
}
@article{MACLENNAN2015410,
title = {Living science: Science as an activity of living beings},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {410-419},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715001224},
author = {Bruce J. MacLennan},
keywords = {Philosophy of science, Phenomenology, Embodied cognition, Causality, Analytical psychology, Goethe},
abstract = {The philosophy of science should accommodate itself to the facts of human existence, using all aspects of human experience to adapt more effectively, as individuals, species, and global ecosystem. This has several implications: (1) Our nature as sentient beings interacting with other sentient beings requires the use of phenomenological methods to investigate consciousness. (2) Our embodied, situated, purposeful physical interactions with the world are the foundation of scientific understanding. (3) Aristotle's four causes are essential for understanding living systems and, in particular, the final cause aids understanding the role of humankind, and especially science, in the global ecosystem. (4) In order to fulfill this role well, scientists need to employ the full panoply of human faculties. These include the consciousness faculties (thinking, sensation, feeling, intuition), and therefore, as advocated by many famous scientists, we should cultivate our aesthetic sense, emotions, imagination, and intuition. Our unconscious faculties include archetypal structures common to all humans, which can guide scientific discovery. By striving to engage the whole of human nature, science will fulfill better its function for humans and the global ecosystem.}
}
@article{OSMAN2013188,
title = {21st Century Biology: An Interdisciplinary Approach of Biology, Technology, Engineering and Mathematics Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {102},
pages = {188-194},
year = {2013},
note = {6th International Forum on Engineering Education (IFEE 2012)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2013.10.732},
url = {https://www.sciencedirect.com/science/article/pii/S1877042813042687},
author = {Kamisah Osman and Lee Chuo Hiong and Rian Vebrianto},
keywords = {interdisciplinary, BTEM (Biology, Technology, Engineering, Mathematics), inquiry-discovery, 21st century skills},
abstract = {The principal goal of interdisciplinary approach for Biology, Technology, Engineering and Mathematics (BTEM) is to cultivate scientific inquiry that requires coordination of both knowledge and skills simultaneously. The dominant activity for BTEM is inquiry-discovery on the authentic problems. This is intended to enhance the students’ abilities to construct their own knowledge through the relevant hands-on and minds-on activities. The essence of engineering is inventive problem solving. The Integration of advanced information communication technologies believed to be able to fulfill current Net Generation learning styles. Mathematics plays an important role as computational tools. The expected outcome of BTEM implementation is the inculcation of 21st century skills.}
}
@article{ATANASIU2023e20698,
title = {On the utility of Colour in shape analysis: An introduction to Colour science via palaeographical case studies},
journal = {Heliyon},
volume = {9},
number = {10},
pages = {e20698},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e20698},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023079069},
author = {Vlad Atanasiu and Peter Fornaro},
keywords = {Colour science, Colour processing, Colour perception, Image processing, Image enhancement, Palaeography},
abstract = {In this article, we explore the use of colour for the analysis of shapes in digital images. We argue that colour can provide unique information that is not available from shape alone, and that familiarity with the interdisciplinary field of colour science is essential for unlocking the potential of colour. Within this perspective, we offer an illustrated overview of the colour-related aspects of image management and processing, perceptual psychology, and cultural studies, using for exemplary purposes case studies focused on computational palaeography. We also discuss the changing roles of colour in society and the sciences, and provide technical solutions for using digital colour effectively, highlighting the impact of human factors. The article concludes with an annotated bibliography. This work is a primer, and its intended readership are scholars and computer scientists unfamiliar with colour science.}
}