@article{LU2024112309,
title = {The physical information LSTM surrogate model for establishing a digital twin model of reciprocating air compressors},
journal = {Applied Soft Computing},
volume = {167},
pages = {112309},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112309},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624010834},
author = {Yingkang Lu and Yanfei Li and Gaocai Fu and Yu Jiang and Yuzhe Huang and Jiaxing Zhu and Buyun Sheng},
keywords = {Reciprocating air compressors, Digital twin, Surrogate model, Pressure prediction, Long short-term memory neural network},
abstract = {Reciprocating air compressors play a crucial role in industrial production processes. However, due to the complex structure and long operating time of reciprocating air compressors, real-time monitoring to grasp the operating status of reciprocating air compressors has become particularly important. Digital twin is a technology that can reflect the behavior of physical entities in real time, accurately predicting and evaluating the operation status of reciprocating air compressors. However, the establishment of a digital twin model for reciprocating air compressors requires a significant amount of computational resources, which can result in the inability to meet the requirements of real-time performance evaluation. To overcome this limitation, this paper proposes a method for constructing a digital twin model of reciprocating air compressors based on a surrogate model. The surrogate model is constructed based on a long short-term memory neural network with physical information(PILSTM). This model can accurately describe the changes in cylinder pressure by combining physical information. According to the characteristics of cylinder pressure changes, regularization formulas are added to ensure the smoothness of the predicted pressure. The experimental results show that the digital twin model based on the surrogate model has high prediction accuracy and real-time performance. Therefore, this model provides a new method for monitoring the operating status of reciprocating air compressors.}
}
@article{KUO2013510,
title = {Cultural Evolution Algorithm for Global Optimizations and its Applications},
journal = {Journal of Applied Research and Technology},
volume = {11},
number = {4},
pages = {510-522},
year = {2013},
issn = {1665-6423},
doi = {https://doi.org/10.1016/S1665-6423(13)71558-X},
url = {https://www.sciencedirect.com/science/article/pii/S166564231371558X},
author = {H.C. Kuo and C.H. Lin},
keywords = {Cultural Algorithm, Genetic Algorithm, Nelder-Mead’s simplex method, Global optimization},
abstract = {The course of socio-cultural transition can neither be aimless nor arbitrary, instead it requires a clear direction. A common goal of social species’ evolution is to move towards an advanced spiritual and conscious state. This study aims to develop a population-based algorithm on the basis of cultural transition goal. In this paper, the socio-cultural model based on a system thought framework could be used to develop a cultural evolution algorithm (CEA). CEA leverage four strategies, each consists of several search methods with similar thinking. Seven benchmark functions are utilized to validate the search performance of the proposed algorithm. The results show that all of the four strategies of cultural evolution algorithm have better performance when compared with relevant literatures. Finally, the CEA was then applied to optimize two different reliability engineering problems, a Serial-Parallel System design and a Bridge System design. For the Serial-Parallel System design, the CEA achieved the exact solution with ease, and for the Bridge System design, the solution obtained by the CEA is superior to those from other literatures.}
}
@article{BARBOSA2025100864,
title = {An interchangeable editor to create generic and adaptable decision trees for versatile applications and game development scenarios},
journal = {Entertainment Computing},
volume = {52},
pages = {100864},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100864},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002325},
author = {Rafael Garcia Barbosa and Maria Andréia Formico Rodrigues},
keywords = {Decision tree editor, Standard interchange format, Interoperability and adaptability, Personalized learning, Game engine integration, Game development scenarios},
abstract = {This paper introduces a novel software tool developed to serve as an editor for the manual construction of decision trees, characterized by their generic nature, flexibility, and adaptability across a wide range of applications and game development scenarios. The editor enables straightforward definition and modification of decision tree elements and data, dynamically updating to meet changing needs and contexts. A key feature of this tool is its capability to export decision trees in a standardized interchange format, enhancing interoperability by allowing seamless integration with other computational platforms, including game engines. We demonstrate the utility and versatility of our editor with three distinct and comprehensive use cases, highlighting its potential as a significant contribution to interactive technology. The tool facilitates the development of decision trees, enhancing informed decision-making and strategic planning, and supports personalized learning to improve engagement and outcomes.}
}
@article{SATPUTE2024109583,
title = {Exploring large language models for microstructure evolution in materials},
journal = {Materials Today Communications},
volume = {40},
pages = {109583},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.109583},
url = {https://www.sciencedirect.com/science/article/pii/S2352492824015642},
author = {Prathamesh Satpute and Saurabh Tiwari and Maneet Gupta and Supriyo Ghosh},
keywords = {Large language models, Materials science, Phase-field models, Microstructure evolution, Partial differential equations},
abstract = {There is a significant potential for coding skills to transition fully to natural language in the future. In this context, large language models (LLMs) have shown impressive natural language processing abilities to generate sophisticated computer code for research tasks in various domains. We report the first study on the applicability of LLMs to perform computer experiments on microstructure pattern formation in model materials. In particular, we exploit LLM’s ability to generate code for solving various types of phase-field-based partial differential equations (PDEs) that integrate additional physics to model material microstructures. The results indicate that LLMs have a remarkable capacity to generate multi-physics code and can effectively deal with materials microstructure problems up to a certain complexity. However, for complex multi-physics coupled PDEs for which a detailed understanding of the problem is required, LLMs fail to perform the task efficiently, since much more detailed instructions with many iterations of the same query are required to generate the desired output. Nonetheless, at their current stage of development and potential future advancements, LLMs offer a promising outlook for accelerating materials education and research by supporting beginners and experts in their physics-based methodology. We hope this paper will spur further interest to leverage LLMs as a supporting tool in the integrated computational materials engineering (ICME) approach to materials modeling and design.}
}
@article{NGUYEN2022108381,
title = {Knowledge mapping of digital twin and physical internet in Supply Chain Management: A systematic literature review},
journal = {International Journal of Production Economics},
volume = {244},
pages = {108381},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108381},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321003571},
author = {Tiep Nguyen and Quang Huy Duong and Truong Van Nguyen and You Zhu and Li Zhou},
keywords = {, , , },
abstract = {Physical Internet (PI) is an open global logistics system of which components are hyperconnected for increased efficiency and sustainability. Digital twin (DT), referring to the virtual representation of a physical object, is well-perceived as a key driver in the development of PI-based Supply Chain Management (SCM). Due to the capabilities of real-time monitoring and evaluation of large-scale complex systems, significant research efforts have been made to exploit values of PI/DT in SCM. Despite this, the current literature remained largely unstructured and scattered due to a lack of systematic literature reviews to synergise research findings, analyse the evolution of research fronts and extract emerging trends in the field. To address this issue, the paper deploys a bibliometric knowledge mapping approach to provide a bird's eye view of the current research status in the PI/DT-SCM area. Using CiteSpace's keyword co-occurrence network, 518 journal articles are clustered into 10 key research streams on PI/DT applications in: job shop scheduling, smart manufacturing design, PI-based SCM, manufacturing virtualisation, information management, sustainability development, data analytics, manufacturing operations management, simulation and optimisation, and assembly process planning. Based on citation burst rate, keywords representing research frontiers of the PI/DT are detected and their temporal evolutions are discussed. Likewise, some identified emerging research trends are production process and system, robotics, computer architecture, and cost. Finally, seven future research directions are suggested, which emphasise on several PI/DT-related issues, including business ecosystem, sustainability development, SC downstream management, cognitive thinking in Industry 5.0, citizen twin in digital society, and SC resilience.}
}
@article{ISOLAN2024111786,
title = {Monte Carlo analysis of dosimetric issues in space exploration},
journal = {Radiation Physics and Chemistry},
volume = {221},
pages = {111786},
year = {2024},
issn = {0969-806X},
doi = {https://doi.org/10.1016/j.radphyschem.2024.111786},
url = {https://www.sciencedirect.com/science/article/pii/S0969806X24002780},
author = {Lorenzo Isolan and Valentina Sumini and Marco Sumini},
keywords = {Space habitat, MCNP6, Unstructured mesh, Topology optimization, Radiation protection},
abstract = {The Radiation protection is of paramount importance in the planning of human exploration activities in space. The related risks must be considered with respect to two aspects: devising a proper shielding and providing answers to the requirement of an effective dosimetry evaluation in astronaut's activities. Both aspects have been considered using the Monte Carlo (MC) code MCNP 6.2 as the reference tool. As case study an application devised for the National Aeronautics and Space Administration (NASA) Artemis program has been chosen. The project aims to establish a sustainable human presence on the Moon, envisioning the realization of an outpost that will serve as a steppingstone for space exploration endeavors. A Class III shelter, in situ resource utilization (ISRU) built habitat for the Moon, has been designed through computational methods and topology optimization techniques, and analyzed in terms of radiation shielding performances and the strictly related structural behavior. The outpost must be able to withstand temperature variations, micrometeorite impacts, and the absence of a substantial atmosphere. Any solution studied to respect the constraints must devise robust and innovative materials and techniques to create habitats that have as goal the shielding from the Galactic Cosmic Rays (GCR) and from the solar flares to provide a safe and habitable environment at the time scales scheduled for the missions. Moreover, the outpost design must incorporate strategies for extracting and utilizing local resources. Overcoming such challenges will pave the way for the establishment of a sustainable human presence on the Moon and serve as a crucial leap for future space exploration missions.}
}
@article{ZHANG2022786,
title = {Research on Graph Neural Network in Stock Market},
journal = {Procedia Computer Science},
volume = {214},
pages = {786-792},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.242},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019524},
author = {Wenjun Zhang and Zhensong Chen and Jianyu Miao and Xueyong Liu},
keywords = {Graph neural networks, stocks, forecasts},
abstract = {The stock market is a very important part of the financial field, and the prediction of the stock market has a great relationship with the returns and risk safety of the entire financial field. With the continuous mature application of machine learning and deep learning in other fields, such as image processing and text analysis, people begin to focus on the use of different models so as to predict stock volatility. However, in view of the unique multi-source and heterogeneous characteristics of stock information, the artificial neural network relying on deep learning cannot make a good prediction on it. At this time, the graph neural network that can well analyze the graph structure data is gradually favored by scholars at home and abroad, and the research thinking is also expanding. This dissertation examines the purpose of deeply analyzing the methods of different graph neural network models on stock prediction through an inductive study of amount of relevant literature. In this paper, we not only classify the literature by various graph neural network models, but also describe objectively the models and ideas presented in each paper. By referring to literature, this paper summarizes the previous research results, analyzes the applicability and results of different methods, and lays a foundation for better stock prediction in the future.}
}
@article{VAKILI2021112687,
title = {The development of a transdisciplinary policy framework for shipping companies to mitigate underwater noise pollution from commercial vessels},
journal = {Marine Pollution Bulletin},
volume = {171},
pages = {112687},
year = {2021},
issn = {0025-326X},
doi = {https://doi.org/10.1016/j.marpolbul.2021.112687},
url = {https://www.sciencedirect.com/science/article/pii/S0025326X21007219},
author = {Seyedvahid Vakili and Aykut I. Ölçer and Fabio Ballini},
keywords = {Energy Efficiency Design Index, Energy Efficiency Existing Ship Index, Enhanced Ship Energy Efficiency Management Plan, Policy, Transdisciplinary, Underwater noise pollution},
abstract = {One of the newly emerging environmental issues is underwater noise pollution. It has both negative environmental and socio-economic impacts and threatens sustainable shipping. While other types of shipping pollutants have been regulated and societal awareness has been raised, due to the intangible characteristics of underwater noise pollution, there is neither societal awareness nor an international legally binding instrument to mitigate underwater noise pollution. This paper aims to raise awareness of ship owners regarding UWN pollution by introducing the sources of UWN pollution, as well as proposing a transdisciplinary policy for shipping companies to mitigate UWN pollution from their ships. The proposed policy is aligned with IMO's initial GHG strategy, especially the Energy Efficiency Design Index, Energy Efficiency Existing Ship Index, and Enhanced Ship Energy Efficiency Management Plan. This multi-dimensional approach will make stakeholders more enthusiastic to tackle underwater noise pollution while enhancing the efficient use of capacities and resources.}
}
@article{KRUIJVER2022102748,
title = {The number of alleles in DNA mixtures with related contributors},
journal = {Forensic Science International: Genetics},
volume = {61},
pages = {102748},
year = {2022},
issn = {1872-4973},
doi = {https://doi.org/10.1016/j.fsigen.2022.102748},
url = {https://www.sciencedirect.com/science/article/pii/S1872497322000898},
author = {Maarten Kruijver and James M. Curran},
keywords = {DNA mixtures, Probabilistic genotyping},
abstract = {The maximum allele count (MAC) across loci and the total allele count (TAC) are often used to gauge the number of contributors to a DNA mixture. Computational strategies that predict the total number of alleles in a mixture arising from a certain number of contributors of a given population have been developed. Previous work considered the restricted case where all of the contributors to a mixture are unrelated. We relax this assumption and allow mixture contributors to be related according to a pedigree. We introduce an efficient computational strategy. This strategy based on first determining a probability distribution on the number of independent alleles per locus, and then conditioning on this distribution to compute a distribution of the number of distinct alleles per locus. The distribution of the number of independent alleles per locus is obtained by leveraging the Identical by Descent (IBD) pattern distribution which can be computed from the pedigree. We explain how allelic dropout and a subpopulation correction can be accounted for in the calculations.}
}
@article{OFOSUAMPONG2024100127,
title = {Artificial intelligence research: A review on dominant themes, methods, frameworks and future research directions},
journal = {Telematics and Informatics Reports},
volume = {14},
pages = {100127},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000136},
author = {Kingsley Ofosu-Ampong},
keywords = {Artificial intelligence, Classification, Literature review, Technological issues, Research agenda},
abstract = {This article presents an analysis of artificial intelligence (AI) in information systems and innovation-related journals to determine the current issues and stock of knowledge in AI literature, research methodology, frameworks, level of analysis and conceptual approaches. By doing this, the article aims to identify research gaps that can guide future investigations. A total of 85 peer-reviewed articles from 2020 to 2023 were used in the analysis. The findings show that extant literature is skewed towards the prevalence of technological issues and highlights the relatively lower focus on other themes, such as contextual knowledge co-creation issues, conceptualisation, and application domains. While there have been increasing technological issues with artificial intelligence, the three identified areas of security concern are data security, model security and network security. Furthermore, the review found that contemporary AI, which continually drives the boundaries of computational capabilities to tackle increasingly intricate decision-making challenges, distinguishes itself from earlier iterations in two primary aspects that significantly affect organisational learning in dealing with AI's potential: autonomy and learnability. This study contributes to AI research by providing insights into current issues, research methodology, level of analysis and conceptual approaches, and AI framework to help identify research gaps for future investigations.}
}
@article{LI2022101701,
title = {A framework and method for Human-Robot cooperative safe control based on digital twin},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101701},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101701},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001604},
author = {Hao Li and Wenfeng Ma and Haoqi Wang and Gen Liu and Xiaoyu Wen and Yuyan Zhang and Miying Yang and Guofu Luo and Guizhong Xie and Chunya Sun},
keywords = {Human-robot collaboration, Digital twin, Safety control, Machine vision, Convolutional neural network},
abstract = {Human-robot collaboration (HRC) combines the robot’s mechanical properties and predictability with human experience, logical thinking, and strain capabilities to alleviate production efficiency. However, ensuring the safety of the HRC process in-real time has become an urgent issue. Digital twin extends functions of virtual models in the design phase of the physical counterpart in the production phase through virtual-real interactive feedback, data fusion analysis, advanced computational features, etc. This paper proposes an HRC safety control framework and corresponding method based on the digital twin. In the design phase, virtual simulation and virtual reality technology are integrated to construct virtual twins of various HRC scenarios for testing and analyzing potential safety hazards. In the production phase, the safety distance between humans and robots of the HRC scene is monitored and calculated by an iterative algorithm according to machine vision and a convolutional neural network. Finally, the virtual twin is driven based on real-scene data, real-time online visual monitoring, and optimization of the HRC’s overall process. A case study using ABB-IRB1600 is presented to verify the feasibility of the proposed approach.}
}
@article{KOCHEN1958267,
title = {The acquisition and utilization of information in problem solving and thinking},
journal = {Information and Control},
volume = {1},
number = {3},
pages = {267-288},
year = {1958},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(58)80005-4},
url = {https://www.sciencedirect.com/science/article/pii/S0019995858800054},
author = {Manfred Kochen and Eugene H. Galanter},
abstract = {Some of the logical consequences of drawing a distinction between the following two aspects of problem-solving behavior are explored: (a) actions directed toward the acquisition of information to guide future actions toward valuable goals; (b) actions directed toward the utilization of accumulated information to attain a valuable goal. An experimental paradigm accomplishing this separation is described for the case of an environment of periodic sequences of binary events. A general way of describing behavioral strategies is developed in terms of: (a) a plan for when to acquire information, to guess an outcome, or to guess at the solution; and (b) a program for how to compute guesses from the information accumulated. The structure of the binary environmental sequences, the structure of these behavioral strategies, and the relations between them are analyzed, and certain strategies which maximize value are suggested. Computing machine interpretations of certain specific strategies for a restricted kind of experiment are displayed, and predictions from these are compared with experimental data from pilot studies performed with human subjects.}
}
@incollection{CANCES20033,
title = {Computational quantum chemistry: A primer},
series = {Handbook of Numerical Analysis},
publisher = {Elsevier},
volume = {10},
pages = {3-270},
year = {2003},
booktitle = {Special Volume, Computational Chemistry},
issn = {1570-8659},
doi = {https://doi.org/10.1016/S1570-8659(03)10003-8},
url = {https://www.sciencedirect.com/science/article/pii/S1570865903100038},
author = {Eric Cancès and Mireille Defranceschi and Werner Kutzelnigg and Claude {Le Bris} and Yvon Maday},
abstract = {Publisher Summary
This chapter discusses basic modeling. The chapter illustrates that quantum chemistry aims at understanding the properties of matter through the modeling of its behavior at a subatomic scale, where matter is described as an assembly of nuclei and electrons. At this scale, the equation that rules the interactions between these constitutive elements is the Schrödinger equation. It can be considered as a universal model for at least three reasons. First, it contains all the physical information of the system under consideration so that any of the properties of this system can be deduced in theory from the Schrödinger equation associated to it. Second, the Schrödinger equation does not involve any empirical parameter, except some fundamental constants of Physics; it can thus be written for any kind of molecular system provided its chemical composition, in terms of natures of nuclei and number of electrons, is known. Third, this model enjoys remarkable predictive capabilities, as confirmed by comparisons with a large amount of experimental data of various types.}
}
@article{MONTEIRO2023100076,
title = {Environmental assessment in concrete pole industries},
journal = {CEMENT},
volume = {13},
pages = {100076},
year = {2023},
issn = {2666-5492},
doi = {https://doi.org/10.1016/j.cement.2023.100076},
url = {https://www.sciencedirect.com/science/article/pii/S2666549223000221},
author = {Nathalie Barbosa Reis Monteiro and José Machado {Moita Neto} and Elaine Aparecida {da Silva}},
keywords = {Concrete poles, Life cycle, Environmental impact},
abstract = {Purpose
Companies that manufacture poles generate several negative environmental impacts, whose extent needs to be assessed to find ways to mitigate them.
Methods
In this research, Life Cycle Assessment (LCA) was used as a methodology to measure the potential environmental impacts throughout the poles' life cycle. Primary data (amount of cement, gravel, sand, steel rebars, energy, water) were collected from industries located in Teresina, Piauí, Brazil, and information from the Ecoinvent 3.7.1 database (transport, solid waste, liquid effluents, particulate matter) was used.
Results and discussion
The literature addresses pole production from a different perspective, making this study relevant to disseminate the life cycle thinking in concrete pole production. However, the literature points to a correlation trend for ecotoxicity and human toxicity indicators, as well as the results found in this research. Waste disposal stands out as an important source of impact for these industries, confirming the necessity of efficient management of these materials at the end of their lifespan and during the production process. The scenario analysis showed that is possible to reduce the potential impacts of these industries.
Conclusion
The reuse of waste within the industry itself is feasible (using a shredder for this purpose) and can contribute to decreasing the extraction of natural deposits in various production processes related to the poles' life cycle and reducing their accumulation in the environment. The use of inputs from closer suppliers is a strategy that contributes to mitigating the potential impact of gaseous emissions, reducing the impact that generates global warming and climate change. In addition, other papers show viable alternatives in different scenarios, based on complex laboratory studies. Nevertheless, his approach shows how impacts can be mitigated with the adoption of simple actions such as the reuse of effluents and residues from these industries. It is possible to redefine the production process through a scenario close to the ideal, bringing environmental sustainability to the sector.}
}
@article{POURFOULADI2025107722,
title = {PoliBrick plugin as a parametric tool for digital stereotomy modelling},
journal = {Computers & Structures},
volume = {311},
pages = {107722},
year = {2025},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2025.107722},
url = {https://www.sciencedirect.com/science/article/pii/S004579492500080X},
author = {Mohammad Pourfouladi and Natalia Pingaro and Marco Valente},
keywords = {PoliBrick Plugin, Software Development, Masonry Construction, Brick Pattern, Stereotomy, Single and Double Curvature Vaults},
abstract = {This paper presents the development of a new plugin that is both simple and user-friendly for the digital modelling of multiple brick patterns in 3D on any surface, from simple flat walls to complex single and double curvature geometries. The plugin, named PoliBrick, is specifically conceived to assist in modelling different types of brickwork shells with intricate patterns, such as masonry arches and vaults. It excels in streamlining parametric modelling across a broad spectrum of free-form curved surfaces, standing out from existing tools. Developed for Rhino software within the Grasshopper environment, PoliBrick features an intuitive interface and comprises only six essential components. Its parametric method makes it competitive with any procedure documented in the literature, as it can accurately replicate brick assemblies on all types of free-form shells. PoliBrick can reproduce, with immediate feedback, any brick arrangement, including patterns like basket weave, stretcher bond, herringbone bond, and many others. Such a functionality addresses a significant gap in current software tools, which cannot often handle curved geometries with complex brick layouts. Moreover, the new plugin can be integrated into a variety of software tools to enable pre-analysis capabilities for the structural evaluation of single and double curvature vaulted structures, using preferred methods like finite element or distinct element approaches. It also supports robotic fabrication processes by considering paths and construction order, enhancing its practical utility in modern construction techniques. PoliBrick is benchmarked on some case studies to demonstrate the validity of the developed procedure and the robustness of the proposed algorithm, which is expected to be effective in markedly reducing the computational effort in pre-structural analysis modelling phases and allows designers to take into account the non-negligible role of stereotomy in curved structures.}
}
@article{LABO2018185,
title = {Application of low-invasive techniques and incremental seismic rehabilitation to increase the feasibility and cost-effectiveness of seismic interventions},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {185-193},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301264},
author = {S. Labò and E. Casprini and C. Passoni and J. Zanni and A. Belleri and A. Marini and P. Riva},
keywords = {Incremental rehabilitation, seismic retrofit, renovation strategy, low-invasive techniques, life cycle thinking, diagrid, school buildings},
abstract = {The high seismic risk connected to the existing construction heritage requires a wide-scale renovation action to ensure structural resilience and avoid future human and economic losses. Given the urgency and the scale of the problem and the lack of available resources, a new strategy for the renovation of the obsolete European building stock should be envisioned, accounting for both safety and environmental, social and economic sustainability. This research aims at exploring new cost-effective seismic retrofit solutions based on the principles of low-invasiveness and incremental seismic rehabilitation, as envisioned by FEMA P-420 (2009). The incremental rehabilitation approach allows to plan repair and retrofit actions along with the maintenance works expected during the building's lifetime, thereby spreading them in time and reducing costs. In addition, low-invasiveness of the solutions is required to reduce the impacts on the functionality of the building, thus cutting the costs connected to downtime. A possible solution is represented by the introduction of an exoskeleton entirely carried out from outside. In this paper, a new sustainable technique is proposed, where the existing structure is connected to a self-supporting exoskeleton adopting demountable dry techniques, which may be assembled and activated in different phases of the building lifetime. As a proof of concept, the approach is then applied to a school building.}
}
@article{DING2024120338,
title = {Next generation of computer vision for plant disease monitoring in precision agriculture: A contemporary survey, taxonomy, experiments, and future direction},
journal = {Information Sciences},
volume = {665},
pages = {120338},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120338},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524002512},
author = {Weiping Ding and Mohamed Abdel-Basset and Ibrahim Alrashdi and Hossam Hawash},
keywords = {Computer vision, Deep learning, Convolutional neural networks (CNNs), Vision transformers, Vision MLPs, Plant recognition, Precision agriculture},
abstract = {Efficient and rational monitoring of plant health is an essential prerequisite for ensuring optimal crop production and resource management in the field of agriculture. Computer vision techniques have revolutionized visual disease monitoring with their exceptional visual recognition performance. However, despite the outstanding results, the widespread acceptance of these methods in agriculture practice is still in its early stages. This study presents a comprehensive survey of the next generation of computer vision models applied to plant disease monitoring in precision agriculture. Our study begins by tracing the evolution of agricultural computer vision research over the past decade, encompassing legacy methods such as convolutional neural networks (CNNs), progressing to newer techniques like vision transformers (ViTs), and culminating in cutting-edge vision multi-layer perceptrons (MLPs). Next, our study embraces both qualitative and quantitative approaches, supporting a profound review of literature and classifying methodologies and experimental approaches. A significant contribution lies in our comprehensive taxonomy, offering a fine-grained categorization of current computer vision models. This taxonomy meticulously highlights the potentials and limitations of these models while explaining their roles in improving plant disease management. Moreover, extensive experimental comparisons are conducted on PlantVillage dataset to evaluate the performance of state-of-the-art computer-vision models for plant recognition data. The obtained results are then utilized to draw insightful conclusions about the behavior of these models and provide guidance for selecting the most suitable one for specific tasks at hand. Additionally, we discuss open research avenues and future directions of computer-vision models in plant disease management including challenges related to the data scarcity, the computational efficiency, need for explainability, and multi-modal analysis.}
}
@article{KUMAR202415,
title = {Hybrid approach of type-2 fuzzy inference system and PSO in asthma disease},
journal = {Clinical eHealth},
volume = {7},
pages = {15-26},
year = {2024},
issn = {2588-9141},
doi = {https://doi.org/10.1016/j.ceh.2024.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2588914124000017},
author = {Tarun Kumar and Anirudh {Kumar Bhargava} and M.K. Sharma and Nitesh Dhiman and Neha Nain},
keywords = {Asthma, Type-2 fuzzy set, Type-2 fuzzy optimized system, Particle swarm optimization, Medical diagnostic},
abstract = {This research work presents a hybrid approach combining a type-2 fuzzy inference system with particle swarm optimization (PSO) to develop a type-2 fuzzy optimized inference system, specifically tailored for asthma patient data. Addressing the inherent uncertainty in medical diagnostics, this model enhances traditional type-1 fuzzy logic by incorporating ambiguity into linguistic variables and utilizing type-2 fuzzy if-then rules. The system is trained to minimize diagnostic error in asthma disease identification. Applied to a dataset comprising eight medical entities from asthma patients, the model demonstrates substantial accuracy improvements. Numerical computations validate the system, showing a decrease in error rate from 1.445 to 0.03, indicating a significant enhancement in diagnostic precision. These results underscore the potential of our model in medical diagnostic problems, providing a novel and effective tool for tackling the complexities of asthma diagnosis.}
}
@article{XUE2025129449,
title = {Lightweight visual backbone network with enhanced comprehensive strength through context-aware dual attention mechanism},
journal = {Neurocomputing},
volume = {624},
pages = {129449},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129449},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225001213},
author = {Jianxin Xue and Yaohua Hu and Sicheng Hua and Minyu Chen and Ling-I Wu and Xi Chang and Guoqiang Li},
keywords = {Deep learning, Attention mechanism, Backbone network, Computer vision},
abstract = {Recent studies have increasingly concentrated on integrating visual neural networks into edge devices, especially for robots and drones that require effective real-time decision-making and autonomous path planning. Achieving this necessitates lightweight visual networks with robust overall performance. In this study, we adopt a small-scale architecture based on convolutional neural networks (CNNs) and propose a Context-Aware Decoupled Fully Connected (CADFC) attention mechanism to enhance the performance of existing CNN-based visual networks. The core concept of the CADFC attention mechanism is to incorporate both remote and local contexts, enabling the network to capture local features and their relevant remote features. Moreover, we design a novel bottleneck that integrates CADFC attention with depthwise convolution. This design thinking is to accumulate focused regions, learning from depthwise convolution, alongside their surrounding features based on CADFC attention. Experimental results demonstrate that CADFC MobileNet outperforms the recent SOTA network GhostNetV2, achieving a top-1 accuracy of 76.8% with 15% fewer parameters, surpassing GhostNetV2’s 75.3% accuracy by 1.5%, even in resource-constrained environments.}
}
@article{TAPIA2019170,
title = {Design of biomass value chains that are synergistic with the food–energy–water nexus: Strategies and opportunities},
journal = {Food and Bioproducts Processing},
volume = {116},
pages = {170-185},
year = {2019},
issn = {0960-3085},
doi = {https://doi.org/10.1016/j.fbp.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0960308519300641},
author = {John Frederick D. Tapia and Sheila Samsatli and Stephen S. Doliente and Elias Martinez-Hernandez and Wan Azlina Binti Wan Ab Karim Ghani and Kean Long Lim and Helmi Zulhaidi Mohd Shafri and Nur Shafira Nisa Binti Shaharum},
keywords = {Biomass value chains (BVCs), Food–energy–water (FEW) nexus, Mathematical modelling, Biomass supply chains, Optimisation, Process systems engineering, Sustainable land use, Bioenergy},
abstract = {Humanity’s future sustainable supply of energy, fuels and materials is aiming towards renewable sources such as biomass. Several studies on biomass value chains (BVCs) have demonstrated the feasibility of biomass in replacing fossil fuels. However, many of the activities along the chain can disrupt the food–energy–water (FEW) nexus given that these resource systems have been ever more interlinked due to increased global population and urbanisation. Essentially, the design of BVCs has to integrate the systems-thinking approach of the FEW nexus; such that, existing concerns on food, water and energy security, as well as the interactions of the BVCs with the nexus, can be incorporated in future policies. To date, there has been little to no literature that captures the synergistic opportunities between BVCs and the FEW nexus. This paper presents the first survey of process systems engineering approaches for the design of BVCs, focusing on whether and how these approaches considered synergies with the FEW nexus. Among the surveyed mathematical models, the approaches include multi-stage supply chain, temporal and spatial integration, multi-objective optimisation and uncertainty-based risk management. Although the majority of current studies are more focused on the economic impacts of BVCs, the mathematical tools can be remarkably useful in addressing critical sustainability issues in BVCs. Thus, future research directions must capture the details of food–energy–water interactions with the BVCs, together with the development of more insightful multi-scale, multi-stage, multi-objective and uncertainty-based approaches.}
}
@article{WELLMAN1991205,
title = {The ecology of computation: B.A. Huberman, ed.},
journal = {Artificial Intelligence},
volume = {52},
number = {2},
pages = {205-218},
year = {1991},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(91)90044-K},
url = {https://www.sciencedirect.com/science/article/pii/000437029190044K},
author = {Michael P. Wellman}
}
@article{ULRICH1988309,
title = {Computation and conceptual design},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {4},
number = {3},
pages = {309-315},
year = {1988},
note = {Special Issue Manufacturing Systems and Technology of the Future},
issn = {0736-5845},
doi = {https://doi.org/10.1016/0736-5845(88)90002-6},
url = {https://www.sciencedirect.com/science/article/pii/0736584588900026},
author = {Karl Ulrich and Warren Seering},
abstract = {Design is the transformation between a functional and a structural description of a device. Conceptual design is the initial stage of this transformation. We hypothesize that most new design are derived from knowledge of existing designs. We identify a special case of this process and call it novel combination. By describing an implemented program which designs novel mechanical fasteners, we explain how knowledge of existing devices can be represented and used. We highlight the issues arising from this implementation and propose four areas of future research. This work is important for establishing a fundamental understanding of conceptual design, leading to enhanced design teaching and better design tools.}
}
@article{FEIZABADI2024103461,
title = {When and under what conditions ambidextrous supply chains prove effective? Insights from simulation and empirical studies},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {183},
pages = {103461},
year = {2024},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103461},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524000516},
author = {Javad {Feiz Abadi} and David M. Gligor and Somayeh {Alibakhshi Motlagh} and Raj Srivastava},
keywords = {Supply Chain Archetype, Ambidexterity, NK modeling, Paradoxes},
abstract = {Our research delves into the impact of ambidextrous supply chain activity configurations on performance, particularly in the dynamic and complex contexts of today's business landscape. Drawing from the rich literature on paradox theory, we aim to unravel the efficacy of ambidextrous supply chain setup in mitigating the tensions inherent in managing dynamism, complexities, munificence, and, as well as understanding the contextual factors that modulate this efficacy. To accomplish this, we construct a computational model that captures the resource allocation and search behavior of the ambidextrous supply chain archetype within the ever-shifting terrain of performance. Our findings reveal that ambidextrous supply chain configurations excel at reconciling paradoxical tensions stemming from high complexity, limited resource abundance, and turbulent market conditions. Empirical data substantiate these findings.}
}
@article{LIU2023100050,
title = {Literature review of digital twin technologies for civil infrastructure},
journal = {Journal of Infrastructure Intelligence and Resilience},
volume = {2},
number = {3},
pages = {100050},
year = {2023},
issn = {2772-9915},
doi = {https://doi.org/10.1016/j.iintel.2023.100050},
url = {https://www.sciencedirect.com/science/article/pii/S2772991523000257},
author = {Cheng Liu and Peining Zhang and Xuebing Xu},
keywords = {Digital Twin, Civil Infrastructure, Bridges, High-speed Railway},
abstract = {Currently, there are numerous drawbacks associated with infrastructure health monitoring and management, such as inefficiency, subpar real-time functionality, demanding data requirements, and high cost. Digital twin (DT), a hybrid of a computational simulation and an actual physical system, has been proposed to overcome these challenges and become increasingly popular for modeling civil infrastructure systems. This literature review summarized different methods to build digital twins in civil infrastructure. In addition, this review also introduced the current progress of digital twins in different infrastructure sectors, including smart cities and urban spaces, transport systems, and energy systems, along with detailed examples of their various applications. Finally, the current challenges in digital twin technologies for civil infrastructure are also highlighted.}
}
@article{SANGA2025104241,
title = {Stories, simulations and narratives: Collaboratively exploring food security and agricultural innovation in sub-Saharan Africa},
journal = {Agricultural Systems},
volume = {224},
pages = {104241},
year = {2025},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2024.104241},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X24003913},
author = {Udita Sanga and Maja Schlüter},
keywords = {Food security, Narratives, Storytelling, Sub-Saharan Africa, Agent-based models},
abstract = {CONTEXT
Food insecurity remains a global challenge, with differing narratives shaping interventions in sub-Saharan Africa. The “crisis narrative,” favored by aid agencies, links insecurity to production issues, advocating agricultural innovations. Meanwhile, the “chronic poverty narrative,” reflected in African policy, ties insecurity to farmer poverty, emphasizing livelihood and economic solutions. Narrative subjectivity can lead to uncritical privileging of certain understandings and solutions, necessitating a critical exploration of contexts, causes, and solutions to food insecurity in the region. Our research addresses the need to understand and illustrate the complex problem of food insecurity in the region.
OBJECTIVE
This study employs a mixed-method approach, combining collaborative storytelling, model exploration, and scenario analysis, to investigate food security, agricultural innovation, and climate adaptation in Mali, West Africa.
METHODS
We developed a three-stage methodology represented as a story arc: beginning (exposition and problem statement), development (action), and completion (solution), providing a cohesive narrative framework. The arc unfolds with the story exposition introducing characters, plot, and problem statement. The story development includes participant-led model simulations and modeler-led scenario analysis. The story completion integrates insights from model simulations and scenario analysis to develop the collective understanding of the narratives surrounding food (in)security.
RESULTS AND CONCLUSIONS
This study generates several insights that highlight the inherent complexities within agricultural innovation systems that emerge from the non-linear dynamic interaction of actors operating across scales that contribute to food insecurity. We redirect the focus of narratives of causes (and subsequent solutions) of food insecurity from solely climate-driven production losses and poverty to the complex interplay of climate, agroecology, innovation networks, risk perception, innovation beliefs, desires, and knowledge transmission. A shared narrative emerges, characterizing food security as a complex adaptive system influenced by factors such as climate-induced production variability, agroecological heterogeneity, network structures and climate risk perception. The study underscores the methodological value of collaborative storytelling and model simulation to enable a structured and reflective exploration of these complex systems. By transforming participants into co-creators of knowledge, this methodology fosters systems thinking, turning abstract systemic relationships into tangible, actionable insights.
SIGNIFICANCE
Our study demonstrates the need to critically reevaluate the role of narratives in shaping agricultural innovation systems and their capacity to transform food systems toward enhanced sustainability and food security. Our participatory and systems-driven approach offers a pathway to more adaptive and effective interventions in the face of complex, dynamic challenges.}
}
@article{COIERA2022100860,
title = {Evidence synthesis, digital scribes, and translational challenges for artificial intelligence in healthcare},
journal = {Cell Reports Medicine},
volume = {3},
number = {12},
pages = {100860},
year = {2022},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2022.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2666379122004244},
author = {Enrico Coiera and Sidong Liu},
keywords = {evidence-based medicine, evidence synthesis, patient safety, research replication, machine learning, algorithmic transportability, deep learning, clinical trial registries},
abstract = {Summary
Healthcare has well-known challenges with safety, quality, and effectiveness, and many see artificial intelligence (AI) as essential to any solution. Emerging applications include the automated synthesis of best-practice research evidence including systematic reviews, which would ultimately see all clinical trial data published in a computational form for immediate synthesis. Digital scribes embed themselves in the process of care to detect, record, and summarize events and conversations for the electronic record. However, three persistent translational challenges must be addressed before AI is widely deployed. First, little effort is spent replicating AI trials, exposing patients to risks of methodological error and biases. Next, there is little reporting of patient harms from trials. Finally, AI built using machine learning may perform less effectively in different clinical settings.}
}
@article{ROBSON2022101018,
title = {Searching for the principles of a less artificial A.I.},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101018},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101018},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001617},
author = {B. Robson and G. Ochoa-Vargas},
keywords = {AI, Algorithms, X factor, Emergent properties, Consciousness, Quantum effects},
abstract = {What would it take to build a computer physician that can take its place amongst human peers? Currently, Neural Nets, especially as so-called “Deep Learning” nets, dominate what is popularly called “Artificial Intelligence”, but to many critics they seem to be little more than powerful data-analytic tools inspired by some of the more basic functions and regions of the human brain such as those involved in early processes in biological vision, classification, and categorization. The deeper nature of human intelligence as the term is normally meant, including relating to consciousness, has been the domain of philosophers, psychologists, and some neuroscientists. Now, attention is turning to neuronal mechanisms in humans and simpler organisms as a basis of a truer AI with far greater potential. Arguably, the approach required should be rooted in information theory and algorithmic science. But as discussed in this paper, caution is required: “just any old information” might not do. The information might need to be of a particular dynamical and actioning nature, and that might significantly impact the kind of computation and computer hardware required. Overall, however, the authors do not favor emergent properties such as those based on complexity and quantum effects. Despite the possible difficulties, such studies could, in return, have substantial benefits for biology and medicine beyond the computational tools that they produce to serve those disciplines.}
}
@incollection{MENAMADATHIL202463,
title = {Chapter 4 - Machine learning approach for vaccine development-fundamentals},
editor = {Jayashankar Das and Sushma Dave and Siomar de Castro Soares and Sandeep Tiwari},
booktitle = {Reverse Vaccinology},
publisher = {Academic Press},
pages = {63-85},
year = {2024},
isbn = {978-0-443-13395-4},
doi = {https://doi.org/10.1016/B978-0-443-13395-4.00025-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443133954000253},
author = {Dhanalakshmi Menamadathil and Kajari Das and Sushma Dave and Jayashankar Das},
keywords = {Artificial intelligence, machine learning, support vector machine, logistic regression, extreme gradient boosting, convolutional neural network, recurrent neural networks, reverse vaccinology},
abstract = {Artificial intelligence (AI)–assisted vaccine creation has emerged as a significant development among the cutting-edge technologies that will help society in the 21st century. AI and machine learning technologies have brought answers to issues that have arisen as a result of the advent of recurring and emerging infectious diseases and the rise in antibiotic resistance. The rapid discovery of effective vaccines has been crucial in preparation for outbreaks, including epidemics and pandemics. The urgent requirement for precise vaccine creation in a short duration has led the way for researchers to investigate diverse vaccine-development technologies such as computational biology, structure-based antigen design, protein engineering, gene synthesis, and novel manufacturing platforms, With the advent of whole-genome sequencing and big data analytic platforms aided by AI, omics-based vaccine design has emerged, which is also known as reverse vaccinology (RV). RV accomplishes comprehensive immunogenicity profiling employing proteome and structural data. With the advancement of AI and deep learning algorithms, a range of modeling tools for accurate and precise prediction of immune-recognition patterns have been created, which may be utilized to generate novel vaccine candidates. Given that vaccinations are available for a few infectious illnesses, there is an urgent need for the quick development of vaccines for numerous lethal and developing infections, which can give prominence to RV. Within the course of this chapter, a thorough view of AI -employed algorithms and their role in RV is offered, with a particular emphasis on immunoinformatic and AI methods utilized in it.}
}
@article{WANG2022799,
title = {BMW-TOPSIS: A generalized TOPSIS model based on three-way decision},
journal = {Information Sciences},
volume = {607},
pages = {799-818},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522006004},
author = {Yumei Wang and Peide Liu and Yiyu Yao},
keywords = {Three-way decision, TOPSIS, Reference point, Multiple attribute decision making},
abstract = {TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) uses a pair of a positive ideal solution and a negative ideal solution as two reference points to rank a set of decision alternatives. In some situations, a trade-off of the distances to the two extreme reference points may not necessarily be meaningful. Inspired by the theory of three-way decision as thinking in threes (e.g., two opposite poles and a third middle), in this paper we generalize the classical TOPSIS by adding a third middle reference point. We use a common setting for investigating systematically reference-point-based TOPSIS-style multi-criteria decision-making methods. In particular, we examine three classes of approaches: a) a best reference point based model (i.e., B-TOPSIS) and a worst reference point based model (i.e., W-TOPSIS), b) the classical best and worst reference points based model (i.e., BW-TOPSIS), and c) a new best, mean, and worst reference points based model (i.e., BMW-TOPSIS). The three classes are one-way TOPSIS, two-way TOPSIS, and three-way TOPSIS, respectively. Based on one-way and two-way TOPSISs, we give two specific methods of three-way TOPSIS. The experimental results, compared with the existing TOPSIS methods, show that the BMW-TOPSIS model is feasible and effective.}
}
@article{AICHA2022107933,
title = {A mathematical formulation for processing time computing in disassembly lines and its optimization},
journal = {Computers & Industrial Engineering},
volume = {165},
pages = {107933},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.107933},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222000031},
author = {Mahdi Aicha and Imen Belhadj and Moncef Hammadi and Nizar Aifaoui},
keywords = {DP Evaluation, Index of Quality, Operating time, Optimization, Lean thinking},
abstract = {Disassembly is the first practice in maintenance and recycling of industrial products. For productivity and efficiency, it is necessary to optimize its operative manners by reducing: change of tools and directions, process variation, wastes, etc. The simulation of Disassembly Plan (DP) allows the detection and identification of difficulties from the design stage in order to avoid them. This paper proposes a mathematical formulation which combines two principal indicators: the index of Quality (Qi) and the index of processing Time (Ti) in order to choose the best and feasible disassembly plan. The Failure Mode, Effects and Criticality Analysis method is implemented to compute Qi. Ti is obtained according to real manufacturing constraints (workspace, layout, tools, machines, etc.). Based on 5S method, the workspace can be optimized which directly impacts the timing index and contribute to the selection of the best DP. A gear box is used to show up the efficiency of the proposed approach.}
}
@article{SHARMA2024100944,
title = {Towards intelligent industrial systems: A comprehensive survey of sensor fusion techniques in IIoT},
journal = {Measurement: Sensors},
volume = {32},
pages = {100944},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2023.100944},
url = {https://www.sciencedirect.com/science/article/pii/S2665917423002805},
author = {Deepak sharma and Anuj kumar and Nitin Tyagi and Sunil S. Chavan and Syam Machinathu Parambil Gangadharan},
keywords = {Sensor fusion, Machine learning, Fault tolerance, Fault prediction, Neural network},
abstract = {Industrial Internet of Things (IIoT) is systems aim to facilitate human monitoring and the direction of efficient production of goods in industrial settings by linking a wide variety of intelligent devices such as sensors, actuators, and controllers. This is achieved by utilizing Internet of Things (IoT) to diagnose a problem with a specific IIoT part is to employ a basic diagnostic technique that's based on models and data. Physical models, signal patterns, and machine-learning strategies must be adequately built to account for system challenges. Another factor that could lead to an exponential rise in complexity is the ever-increasing interconnections between different electronic hardware. The knowledge-based defect diagnosis methods boost interoperability in the operation. Users don't need to be experts in the field to benefit from the system's high-level thinking and response to their queries. So, in advanced IIoT systems, a knowledge-based fault diagnostic approach is favored over traditional model-based and data-driven diagnosis methods. The goal of this study is to evaluate recent improvements in the design of knowledge-based defect detection in the context of IIoT systems, deductive and inductive reasoning, and many other forms of logical reasoning. IIoT-based systems have revolutionized industrial settings by connecting intelligent devices such as sensors, actuators, and controllers to enable efficient production and human monitoring. In this survey paper, we explore machine learning-based sensor fusion techniques within the realm of Industrial Internet of Things (IIoT), addressing critical challenges in fault detection and diagnosis.}
}
@article{ZONG2024120192,
title = {Parameter estimation of multivariable Wiener nonlinear systems by the improved particle swarm optimization and coupling identification},
journal = {Information Sciences},
volume = {661},
pages = {120192},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120192},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524001051},
author = {Tiancheng Zong and Junhong Li and Guoping Lu},
keywords = {System identification, Multivariable Wiener system, Particle swarm optimization, Coupling identification, Auxiliary model},
abstract = {This paper investigates the parameter estimation of multivariable Wiener nonlinear systems. To solve the inconsistency problem of the parameter vector and the parameter matrix, the coupling identification concept is applied. Combined with particle swarm optimization (PSO) and an auxiliary model, the partially coupled improved particle swarm optimization (PC-IPSO) method is proposed. In this algorithm, the adaptive feedback inertia weight is improved to accelerate the convergence speed, and the retirement update mechanism is introduced to improve the optimization ability of the basic PSO algorithm. To verify the performance of PC-IPSO, we also derive a multivariable improved PSO (M-IPSO) method for comparison. The computational complexity analysis shows that the PC-IPSO algorithm requires less computational resources than the M-IPSO algorithm. Then, the convergence of the improved PSO method is analyzed. The simulation results indicate that the PC-IPSO method has a faster convergence speed and higher identification accuracy than the M-IPSO and several existing state-of-the-art methods for multivariable Wiener system identification.}
}
@article{RICAURTE2020102,
title = {Project-based learning as a strategy for multi-level training applied to undergraduate engineering students},
journal = {Education for Chemical Engineers},
volume = {33},
pages = {102-111},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772820300464},
author = {Marvin Ricaurte and Alfredo Viloria},
keywords = {Project-based learning, Multi-level, Undergraduate students, Process engineering},
abstract = {This study presents a project-based learning methodology whose particularity is the inclusion of training at different levels of undergraduate engineering programs, which allows for the interaction among students from different semesters who work together on a common project. To show the applicability of the proposed methodology, a project for the industrial production of ethanol from sugar cane was considered. Students enrolled in Process Design (9th semester) and Computer-Assisted Technical Design (5th semester), courses included in the engineering programs offered by the Department of Chemical Engineering at Yachay Tech University (Ecuador), jointly developed it. The details of the project were presented to the students of the Introduction to Engineering course (3rd semester) to boost their interest in the engineering as applied science. The activities carried out in each of the courses are described in detail together with a description of how the learning outcomes were achieved thanks to the implementation of a multi-level training strategy. Teamwork and collaborative-integrated learning are the elements highlighted by the students who participated in the project. Some of the innovative aspects of the proposed methodology include professional training and multi-level learning, the development of logical thinking typical of engineers, the knowledge handover associated with the professional activities of process engineers engaged with real-world projects. Additionally, this methodology prizes the industrial experience that professors at the undergraduate level may have by allowing them to contribute with an engineering vision to the training of young people in engineering projects. This study was inspired by the principle of Constructive Alignment and by goal # 4 (quality education) of the 2030 Agenda for Sustainable Development.}
}
@article{MARIN2021100015,
title = {Human macrophage polarization in the response to Mycobacterium leprae genomic DNA},
journal = {Current Research in Microbial Sciences},
volume = {2},
pages = {100015},
year = {2021},
issn = {2666-5174},
doi = {https://doi.org/10.1016/j.crmicr.2020.100015},
url = {https://www.sciencedirect.com/science/article/pii/S2666517420300171},
author = {Alberto Marin and Kristopher {Van Huss} and John Corbett and Sangjin Kim and Jonathon Mohl and Bo-young Hong and Jorge Cervantes},
keywords = {RNAseq, , Leprosy, Macrophage polarization},
abstract = {Infection with Mycobacterium leprae, the causative organism of leprosy, is still endemic in numerous parts of the world including the southwestern United States. The broad variation of symptoms in the leprosy disease spectrum range from the milder tuberculoid leprosy (paucibacillary) to the more severe and disfiguring lepromatous leprosy (multibacillary). The established thinking in the health community is that host response, rather than M. leprae strain variation, is the reason for the range of disease severity. More recent discoveries suggest that macrophage polarization also plays a significant role in the spectrum of leprosy disease but to what degree it contributes is not fully established. In this study, we aimed to analyze if different strains of M. leprae elicit different transcription responses in human macrophages, and to examine the role of macrophage polarization in these responses. Genomic DNA from three different strains of M. leprae DNA (Strains NHDP, Br4923, and Thai-53) were used to stimulate human macrophages under three polarization conditions (M1, M1-activated, and M2). Transcriptome analysis revealed a large number of differentially expressed (DE) genes upon stimulation with DNA from M. leprae strain Thai-53 compared to strains NHDP and Br4923, independent of the macrophage polarization condition. We also found that macrophage polarization affects the responses to M. leprae DNA, with up-regulation of numerous interferon stimulated genes. These findings provide a deeper understanding of the role of macrophage polarization in the recognition of M. leprae DNA, with the potential to improve leprosy treatment strategies.}
}
@article{CLEMENTZ2020808,
title = {Testing Psychosis Phenotypes From Bipolar–Schizophrenia Network for Intermediate Phenotypes for Clinical Application: Biotype Characteristics and Targets},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {5},
number = {8},
pages = {808-818},
year = {2020},
note = {Understanding the Nature and Treatment of Psychopathology: Letting the Data Guide the Way},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2020.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S2451902220301002},
author = {Brett A. Clementz and Rebekah L. Trotti and Godfrey D. Pearlson and Matcheri S. Keshavan and Elliot S. Gershon and Sarah K. Keedy and Elena I. Ivleva and Jennifer E. McDowell and Carol A. Tamminga},
keywords = {Biomarkers, Computational neuroscience, Neurobiological, Precision medicine, Psychopathology, Transdiagnostic},
abstract = {Background
Psychiatry aspires to the molecular understanding of its disorders and, with that knowledge, to precision medicine. Research supporting such goals in the dimension of psychosis has been compromised, in part, by using phenomenology alone to estimate disease entities. To this end, we are proponents of a deep phenotyping approach in psychosis, using computational strategies to discover the most informative phenotypic fingerprint as a promising strategy to uncover mechanisms in psychosis.
Methods
Doing this, the Bipolar–Schizophrenia Network for Intermediate Phenotypes (B-SNIP) has used biomarkers to identify distinct subtypes of psychosis with replicable biomarker characteristics. While we have presented these entities as relevant, their potential utility in clinical practice has not yet been demonstrated.
Results
Here we carried out an analysis of clinical features that characterize biotypes. We found that biotypes have unique and defining clinical characteristics that could be used as initial screens in the clinical and research settings. Differences in these clinical features appear to be consistent with biotype biomarker profiles, indicating a link between biological features and clinical presentation. Clinical features associated with biotypes differ from those associated with DSM diagnoses, indicating that biotypes and DSM syndromes are not redundant and are likely to yield different treatment predictions. We highlight 3 predictions based on biotype that are derived from individual biomarker features and cannot be obtained from DSM psychosis syndromes.
Conclusions
In the future, biotypes may prove to be useful for targeting distinct molecular, circuit, cognitive, and psychosocial therapies for improved functional outcomes.}
}
@article{LEVESQUE201427,
title = {On our best behaviour},
journal = {Artificial Intelligence},
volume = {212},
pages = {27-35},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000356},
author = {Hector J. Levesque},
keywords = {IJCAI Research Excellence},
abstract = {The science of AI is concerned with the study of intelligent forms of behaviour in computational terms. But what does it tell us when a good semblance of a behaviour can be achieved using cheap tricks that seem to have little to do with what we intuitively imagine intelligence to be? Are these intuitions wrong, and is intelligence really just a bag of tricks? Or are the philosophers right, and is a behavioural understanding of intelligence simply too weak? I think both of these are wrong. I suggest in the context of question-answering that what matters when it comes to the science of AI is not a good semblance of intelligent behaviour at all, but the behaviour itself, what it depends on, and how it can be achieved. I go on to discuss two major hurdles that I believe will need to be cleared.}
}
@article{SALEMDEEB2022200069,
title = {Beyond recycling: An LCA-based decision-support tool to accelerate Scotland's transition to a circular economy},
journal = {Resources, Conservation & Recycling Advances},
volume = {13},
pages = {200069},
year = {2022},
issn = {2667-3789},
doi = {https://doi.org/10.1016/j.rcradv.2022.200069},
url = {https://www.sciencedirect.com/science/article/pii/S2667378922000074},
author = {Ramy Salemdeeb and Ruth Saint and Francesco Pomponi and Kimberley Pratt and Michael Lenaghan},
keywords = {Life cycle assessment, Policy development, Resource and waste management, Circular economy, Zero waste},
abstract = {Resources and waste strategies have recently seen a shift in focus from weight-based recycling targets to impact-driven policies. To support this transition, numerous decision-support tools were developed to help identify waste streams with the highest impacts. However, the majority of these tools focus solely on greenhouse gas emissions and show a narrow picture of the overall environmental impacts. Furthermore, they cover burdens associated with direct waste management activities and hence fall short when it comes to highlighting the substantial benefits that can be achieved by preventing waste in the first place. This paper quantitatively demonstrates the necessity to adopt impact-based targets that go beyond estimating the greenhouse gas emissions of waste and highlights the substantial benefits of waste reduction and prevention. Using a state-of-the-art waste environmental footprint tool, the paper quantifies the overall environmental impacts of Scotland's household waste and shows how targeting ‘heavy’ materials does not necessarily have the highest overall environmental benefit. Results show that embodied environmental impacts of household waste dominate the total environmental burdens, contributing more than 90% to the whole life cycle impacts, and hence policymakers should prioritise interventions that aim at waste reduction and prevention. Moreover, our analysis shows that food and textile wastes are high-priority materials in Scotland, with the largest contribution to overall environmental burdens; up to 42% and 30%, respectively. Considering the overall environmental impacts of specific waste materials will enable policymakers to develop more granular and targeted interventions to accelerate our transition to a sustainable circular economy.}
}
@incollection{VHORA2024709,
title = {Investigating Fluid Flow Dynamics in Triply Periodic Minimal Surfaces (TPMS) Structures Using CFD Simulation},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {709-714},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50119-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288241501198},
author = {Kasimhussen Vhora and Tanya Neeraj and Dominique Thévenin and Gábor Janiga and Kai Sundmacher},
keywords = {Computational Fluid Dynamic, TPMS Structure, Pressure Drop, LBM},
abstract = {Efficient absorption processes require optimized packed bed column structures, which affect gas-liquid contact, flow distribution, and pressure drop. An optimal setup ensures efficient mass transfer with high surface area while keeping down the pressure drop, which leads to energy savings and better absorption. TPMS structures such as the Gyroid, Schwarz-P, and Schwarz-D were investigated in this study, with a focus on balancing porosity and surface area to achieve reduced pressure drops and optimal phase contact. Single-phase flow simulations were conducted using the commercial software STAR- CCM+, compared to the lattice Boltzmann method (LBM) to provide an alternative perspective on fluid dynamics. Validation, analysis of the results and identification of possible improvements were achieved through these comparisons. According to the results, the Schwarz-D structure with 70% porosity and 2 mm unit cell leads to the best performance, exhibiting a pressure drop of 655 Pa m-1 and a specific surface area of 1776 m2 m-3 when analysed with STAR-CCM+. The predicted pressure drop was successfully confirmed using LBM simulations, adding robustness to the findings.}
}
@article{OKEREKE2014637,
title = {Virtual testing of advanced composites, cellular materials and biomaterials: A review},
journal = {Composites Part B: Engineering},
volume = {60},
pages = {637-662},
year = {2014},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2014.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1359836814000109},
author = {M.I. Okereke and A.I. Akpoyomare and M.S. Bingley},
keywords = {A. Polymer–matrix composites (PMCs), C. Computational modelling, C. Numerical analysis, E. Weaving, Virtual testing},
abstract = {This paper documents the emergence of virtual testing frameworks for prediction of the constitutive responses of engineering materials. A detailed study is presented, of the philosophy underpinning virtual testing schemes: highlighting the structure, challenges and opportunities posed by a virtual testing strategy compared with traditional laboratory experiments. The virtual testing process has been discussed from atomistic to macrostructural length scales of analyses. Several implementations of virtual testing frameworks for diverse categories of materials are also presented, with particular emphasis on composites, cellular materials and biomaterials (collectively described as heterogeneous systems, in this context). The robustness of virtual frameworks for prediction of the constitutive behaviour of these materials is discussed. The paper also considers the current thinking on developing virtual laboratories in relation to availability of computational resources as well as the development of multi-scale material model algorithms. In conclusion, the paper highlights the challenges facing developments of future virtual testing frameworks. This review represents a comprehensive documentation of the state of knowledge on virtual testing from microscale to macroscale length scales for heterogeneous materials across constitutive responses from elastic to damage regimes.}
}
@article{FURNARI2023103763,
title = {Streaming egocentric action anticipation: An evaluation scheme and approach},
journal = {Computer Vision and Image Understanding},
volume = {234},
pages = {103763},
year = {2023},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2023.103763},
url = {https://www.sciencedirect.com/science/article/pii/S1077314223001431},
author = {Antonino Furnari and Giovanni Maria Farinella},
keywords = {Action anticipation, Egocentric vision, Streaming perception},
abstract = {Egocentric action anticipation aims to predict the future actions the camera wearer will perform from the observation of the past. While predictions about the future should be available before the predicted events take place, most approaches do not pay attention to the computational time required to make such predictions. As a result, current evaluation schemes assume that predictions are available right after the input video is observed, i.e., presuming a negligible runtime, which may lead to overly optimistic evaluations. We propose a streaming egocentric action evaluation scheme which assumes that predictions are performed online and made available only after the model has processed the current input segment, which depends on its runtime. To evaluate all models considering the same prediction horizon, we hence propose that slower models should base their predictions on temporal segments sampled ahead of time. Based on the observation that model runtime can affect performance in the considered streaming evaluation scenario, we further propose a lightweight action anticipation model based on feed-forward 3D CNNs which is optimized using knowledge distillation techniques with a novel past-to-future distillation loss. Experiments on the three popular datasets EPIC-KITCHENS-55, EPIC-KITCHENS-100 and EGTEA Gaze+ show that (i) the proposed evaluation scheme induces a different ranking on state-of-the-art methods as compared to classic evaluations, (ii) lightweight approaches tend to outmatch more computationally expensive ones, and (iii) the proposed model based on feed-forward 3D CNNs and knowledge distillation outperforms current art in the streaming egocentric action anticipation scenario.}
}
@article{CAUDEK2021317,
title = {Susceptibility to eating disorders is associated with cognitive inflexibility in female university students},
journal = {Journal of Behavioral and Cognitive Therapy},
volume = {31},
number = {4},
pages = {317-328},
year = {2021},
issn = {2589-9791},
doi = {https://doi.org/10.1016/j.jbct.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2589979121000159},
author = {Corrado Caudek and Claudio Sica and Silvia Cerea and Ilaria Colpizzi and Debora Stendardi},
keywords = {Eating disorders, Cognitive inflexibility, Individual differences, Computational modeling, Reversal learning},
abstract = {Summary
The inability to learn from and adapt to changing feedback in our environment may be etiologically linked to eating disorders (EDs). However, previous investigations on this issue have shown conflicting results. In the current study with a non-clinical sample of female students, we investigated the relation between cognitive inflexibility (CI) and vulnerability to EDs by using a modified version of the probabilistic reversal learning (PRL) task, which requires participants to adapt their response strategy according to changes in stimulus-reward contingencies. We found that females vulnerable to EDs in the general population showed an impaired PRL performance, also after controlling for comorbidity. However, our results also show that the ED construct comprises separate dimensions, which affect contingency learning in opposite manners: some individuals vulnerable to EDs showed impaired contingency learning; others used unimpaired contingency learning skills to pursue self-harming goals. Such results point to the necessity of an appropriate assessment of CI in order to better apply individualized treatment.}
}
@article{ANDROUTSOPOULOS2023141,
title = {Punctuating the other: Graphic cues, voice, and positioning in digital discourse},
journal = {Language & Communication},
volume = {88},
pages = {141-152},
year = {2023},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2022.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0271530922000957},
author = {Jannis Androutsopoulos},
keywords = {Voice, Positioning, Graphic cues, <!!1!>, Reddit, Punctuation},
abstract = {This article investigates the nested relationship between graphic cues, voice, and positioning in digital discourse. The focus is on the ‘indignation mark’, or <!!1!>, an allographic sign used in German-language discussion boards on Reddit. The study's theoretical backdrop brings research on graphic practices in digitally-mediated communication into dialogue with sociolinguistic approaches to the enactment of group relations in discourse, in particular double-voicing, stylization, and positioning, thereby aiming to foster theory-building on both sides. Data is extracted from a large German-language forum (‘subreddit’) on Reddit and subjected to computational, sequential, and microlinguistic analysis. The findings show how participants in public online discussions use punctuation signs and other graphic cues to animate voices, i.e. ways of speaking that index recognizable social positions and ideologies; how these stylized voices provide a resource for positioning; and how participants display recognition of and alignment to this feature's indexical meaning. The findings also suggest that the ‘indignation mark’ is part of a wider ecology of graphic cues, which evolve constantly to enable multi-voicedness in public digital discourse. Overall, this paper aims to advance our understanding about how graphic elements of digital discourse are indexically and ideologically connected with positioning activities in online communities of practice.}
}
@article{XIE2023119,
title = {2D magnetotelluric inversion based on ResNet},
journal = {Artificial Intelligence in Geosciences},
volume = {4},
pages = {119-127},
year = {2023},
issn = {2666-5441},
doi = {https://doi.org/10.1016/j.aiig.2023.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666544123000266},
author = {LiAn Xie and Bo Han and Xiangyun Hu and Ningbo Bai},
keywords = {Magnetotellurics, 2D inversion, Residual network, Deep learning},
abstract = {In this study, a deep learning algorithm was applied to two-dimensional magnetotelluric (MT) data inversion. Compared with the traditional linear iterative inversion methods, the MT inversion method based on convolutional neural networks (CNN) does not rely on the selection of the initial model parameters and does not fall into the local optima. Although the CNN inversion models can provide a clear electrical interface division, their inversion results may remain prone to abrupt electrical interfaces as opposed to the actual underground electrical situation. To solve this issue, a neural network with a residual network architecture (ResNet-50) was constructed in this study. With the apparent resistivity and phase pseudo-section data as the inputs and with the resistivity parameters of the geoelectric model as the training labels, the modified ResNet-50 model was trained end-to-end for producing samples according to the corresponding production strategy of the study area. Through experiments, the training of the ResNet-50 with the dice loss function effectively solved the issue of over-segmentation of the electrical interface by the cross-entropy function, avoided its abrupt inversion, and overcame the computational inefficiency of the traditional iterative methods. The proposed algorithm was validated against MT data measured from a geothermal field prospect in Huanggang, Hubei Province, which showed that the deep learning method has opened up broad prospects in the field of MT data inversion.}
}
@article{BALKHI20051223,
title = {Proteomics of Acute Myeloid Leukemia: Cytogenetic Risk Groups Differ Specifically in Their Proteome, Interactome and Posttranslational Protein Modifications.},
journal = {Blood},
volume = {106},
number = {11},
pages = {1223},
year = {2005},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood.V106.11.1223.1223},
url = {https://www.sciencedirect.com/science/article/pii/S0006497119761138},
author = {Mumtaz Y. Balkhi and Mulu Geletu and Maximilian Christopeit and Hermann M. Behre and Gerhard Behre},
abstract = {Acute Myeloid Leukemia (AML) is characterized by specific cytogenetic aberrations which are strong determinants of prognostic outcome and therapeutic response. Because the clinical outcome in AML cytogenetic groups differs considerably, we hypothesized that cytogenetic risk groups of AML might differ specifically in their proteome, protein interaction pathways and posttranslational modifications (PTMs). Thus, we determined the proteome of 30 AML patients belonging to various cytogenetic groups based on two-dimensional gel electrophoresis and Nano LC coupled MALDI-TOF-TOF tandem mass spectrometry. We could identify substantial differences in the proteome, protein expression and peak pattern between cytogenetic risk groups of AML. The interactome analysis based on computational bioinformatics using Ingenuity analysis revealed major regulating networks: MAPK8 and MYC for complex aberrant karyotype AML, TP53 for t(8;21)-AML, TP53- MYC- PRKAC for 11q23-AML, JUN and MYC for inv(16)-AML. Most interestingly, peak explorer analysis revealed a modification of O-linked acetyl glucosamine of hnRNPH1 in AML patients with a 11q23 translocation, an acetylation of calreticulin in t(8;21) translocation AML, an increased intensity of dimethylated peptide of hnRNPA2/B1 in AML patients with translocations of t(8;21) and inv(16) in comparison to healthy bone marrow. We show for the first time that cytogenetic risk groups of AML differ specifically both in their proteome, interactome and PTMs. These findings lead to a new thinking about the pathogenesis of AML and has major therapeutic implications because PTMs are the primary drug targets.}
}
@article{JARLEBLAD2024108930,
title = {A framework for synthetic diagnostics using energetic-particle orbits in tokamaks},
journal = {Computer Physics Communications},
volume = {294},
pages = {108930},
year = {2024},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2023.108930},
url = {https://www.sciencedirect.com/science/article/pii/S0010465523002758},
author = {H. Järleblad and L. Stagner and M. Salewski and J. Eriksson and M. Nocente and B.S. Schmidt and M. {Rud Larsen}},
keywords = {Nuclear fusion, Fast ions, Orbits, Weight functions},
abstract = {In fusion plasma physics, the large-scale trajectories of energetic particles in magnetic confinement devices are known as orbits. To effectively and efficiently be able to work with orbits, the Orbit Weight Computational Framework (OWCF) was developed. The OWCF constitutes a set of scripts, functions and applications capable of computing, visualizing and working with quantities related to fast-ion (FI) orbits in toroidally symmetric fusion devices. The current version is highly integrated with the DRESS code, which enables the OWCF to compute and analyze the orbit sensitivity for arbitrary neutron- and gamma-diagnostics. However, the framework is modular in the sense that any future codes (e.g. FIDASIM) can be easily integrated. The OWCF can also compute projected velocity spectra for FI orbits, which play a key role in many FI diagnostics. Via interactive applications, the OWCF can function both as a tool for investigative research but also for teaching. The OWCF will be used to analyze and simulate the diagnostic results of current and future fusion experiments such as ITER. The orbit weight functions computed with the OWCF can be used to reconstruct the FI distribution in terms of FI orbits from experimental measurements using tomographic inversion.}
}
@article{RAM2022100232,
title = {The role of ‘big data’ and ‘in silico’ New Approach Methodologies (NAMs) in ending animal use – A commentary on progress},
journal = {Computational Toxicology},
volume = {23},
pages = {100232},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000202},
author = {Rebecca N. Ram and Domenico Gadaleta and Timothy E.H. Allen},
keywords = {Computational toxicology, In-silico, NAMs, New approach methodologies, Human relevant, QSAR, Read across, Chemical safety, High throughput, Adverse outcome pathways},
abstract = {In silico (computational) methods continue to evolve as part of a robust 21st century public health strategy in risk assessment, relevant to all sectors of chemical safety including preclinical drug discovery, industrial chemicals testing, food and cosmetics. Alongside in vitro methods as components of intelligent testing and pathway driven strategies, in silico models provide the potential for more human relevant solutions to the use of animals in safety testing and biomedical research. These are often termed ‘New Approach Methodologies’ (NAMs). Some NAMs incorporate the use of ‘big data’, for example the information provided from high throughput or high content in vitro screening assays or ‘omics’ technologies. Big data has increasing relevance to predictive toxicology but must be appropriately defined, particularly with regard to ‘quality vs quantity’. The purpose of this article is to provide a commentary on the progress of in silico human-based research methods within the context of NAMs, as well as discussion of the emerging use of big data with relevance to safety assessment. The current status of in silico methods is discussed, with input from researchers in the field. Scientific and legislative drivers for change are also considered, along with next steps to address challenges in funding and recognition, to achieve regulatory acceptance and uptake within the research community. To provide some wider context, the use of in silico methods alongside other relevant approaches (e.g., human-based in vitro) is also discussed.}
}
@article{ROBSON2014287,
title = {When do aquatic systems models provide useful predictions, what is changing, and what is next?},
journal = {Environmental Modelling & Software},
volume = {61},
pages = {287-296},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214000188},
author = {Barbara J. Robson},
keywords = {Modelling philosophy, Biogeochemical modelling, Ecological models, Developments, Progress, Knowledge gaps},
abstract = {This article considers how aquatic systems modelling has changed since 1995 and how it must change in future if we are to continue to advance. A distinction is made between mechanistic and statistical models, and the relative merits of each are considered. The question of “when do aquatic systems models provide accurate and useful predictions?” is addressed, implying some guidelines for model development. It is proposed that, in general, ecological models only provide management-relevant predictions of the behaviour of real systems when there are strong physical (as opposed to chemical or ecological) drivers. Developments over the past 15 years have included changes in technology, changes in the modelling community and changes in the context in which modelling is conducted: the implications of each are briefly discussed. Current trends include increased uptake of best practice guidelines, increasing integration of models, operationalisation, data assimilation, development of improved tools for skill assessment, and application of models to new management questions and in new social contexts. Deeper merging of statistical and mechanistic modelling approaches through such techniques as Bayesian Melding, Bayesian Hierarchical Modelling and surrogate modelling is identified as a key emerging area. Finally, it is suggested that there is a need to systematically identify areas in which our current models are inadequate. We do not yet know for which categories of problems well-implemented aquatic systems models can (or cannot) be expected to accurately predict observational data and system behaviour. This can be addressed through better modelling and publishing practices.}
}
@article{ISMAILOVA2021341,
title = {Cognitive System to Clarify the Semantic Vulnerability and Destructive Substitutions},
journal = {Procedia Computer Science},
volume = {190},
pages = {341-360},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012898},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {cognitive system, information process, knowledge stage, cognitive interference, semantic web, functor-as-object, dynamics, semantic virus, variable sets, category theory},
abstract = {The development of special mathematics capable of directly taking into account the dynamics of the problem domain, as it turns out, is a non-trivial task. Its very formulation in a refined form and the fixation of the most important features cause noticeable complications in the target formalism, significantly complicating the development of software. A constructive solution to this problem is given, obtained using the original functor-as-object construction. The concept of semantic viralization is introduced. It is expected that the obtained computational model has a high innovative potential for the development of information systems designed for intensive data exchange.}
}
@incollection{MOUSTAFA202149,
title = {3 - Deductive reasoning abilities in schizophrenia and related disorders: A systematic review},
editor = {Ahmed A. Moustafa},
booktitle = {Cognitive and Behavioral Dysfunction in Schizophrenia},
publisher = {Academic Press},
pages = {49-65},
year = {2021},
isbn = {978-0-12-820005-6},
doi = {https://doi.org/10.1016/B978-0-12-820005-6.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200056000049},
author = {Ahmed A. Moustafa and Anchal Garg and Ahmed A. Helal and Eid {Abo Hamza}},
keywords = {Schizophrenia, Delusions, Hallucinations, Negative symptoms, Reasoning, Transitive inference, Wason Selection Task, Syllogism, Inductive vs. deductive reasoning},
abstract = {Background: Schizophrenia is a psychiatric disorder characterized by delusions, hallucinations, negative symptoms, and disorganized thinking. There has been a multitude of studies assessing reasoning performance in schizophrenia patients by using various reasoning tasks. Methods: We reviewed the existing literature using the following reasoning tasks in schizophrenia and related disorders: Transitive Inferences, Wason Card Selection, conditional reasoning, syllogisms, and other related tasks. Results: Some deductive reasoning studies have reported conflicting results where schizophrenia patients sometimes, outperform or underperform healthy controls. These findings are related to the plausibility, emotional content of logical sentences used in these studies. Importantly, data show that performance in deductive reasoning tasks is impacted by emotional and cognitive processes, such as theory of mind and working memory. However, neural studies report different brain mechanisms underlying different deductive reasoning task performance. Conclusions: Overall, there are differences in the findings of reasoning tasks which should be investigated in future studies as it will contribute towards an accurate understanding of reasoning processes in schizophrenia spectrum and related disorders.}
}
@article{THEISE200417,
title = {Understanding cell lineages as complex adaptive systems},
journal = {Blood Cells, Molecules, and Diseases},
volume = {32},
number = {1},
pages = {17-20},
year = {2004},
note = {Stem Cell Plasticity},
issn = {1079-9796},
doi = {https://doi.org/10.1016/j.bcmd.2003.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1079979603002523},
author = {Neil D Theise and Mark d'Inverno},
keywords = {Stem cells, Lineage system, Reactive systems},
abstract = {Stem cells may be considered complex reactive systems because of their vast number in a living system, their reactive nature, and the influence of local environmental factors (such as the state of neighboring cells, tissue matrix, stem cell physiological processes) on their behavior. In such systems, emergent global behavior arises through the multitude of local interactions among the cell agents. Approaching hematopoietic and other stem cell lineages from this perspective have critical ramifications on current thinking relating to the plasticity of these lineage systems, the modeling of stem cell systems, and the interpretation of clinical data regarding many diseases within such models.}
}
@article{MIRZAEI2021102839,
title = {CFD modeling of micro and urban climates: Problems to be solved in the new decade},
journal = {Sustainable Cities and Society},
volume = {69},
pages = {102839},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102839},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721001293},
author = {Parham A. Mirzaei},
keywords = {Computational fluid dynamics, Urban climate, Microclimate, Data acquisition, Building energy simulation, Sustainable, Resilient, Smart cities},
abstract = {Despite the popularity of the micro/urban climate CFD modeling as a powerful approach to simulate convective exchanges in urban areas, yet its application faces three profound limitations, including (1) computational barriers, (2) data acquisition, and (3) over-simplifications of underlying physics. Computational resources are not qualitatively studied to be allocated to their best of performance in urban climate models. Moreover, bigdata of city components and inhabitants are sometimes inaccessible or difficult to be effectively interpreted to be fed into CFD models. Furthermore, commonly adopted oversimplifications, and misinterpretation of underlying physics of urban climate can substantially render falsified results, no matter if they look otherwise followed by extravagant visual reports. This paper, hence, aims to explore the capabilities and limitations of urban climate CFD modeling. It further scrutinizes the common oversimplifications in the modeling techniques, potentially resulting in CFD capacities to be lost in the translation. The paper describes the extend to which CFD tools can be the favourable options and otherwise, while it underpins the areas in which further research is needed to conform urban climate CFD models as practical design and decision-making tools. It also offers a brief overview in the recent advancements in response to the mentioned challenges.}
}
@article{JIANG2021116441,
title = {Impacts of COVID-19 on energy demand and consumption: Challenges, lessons and emerging opportunities},
journal = {Applied Energy},
volume = {285},
pages = {116441},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116441},
url = {https://www.sciencedirect.com/science/article/pii/S030626192100009X},
author = {Peng Jiang and Yee Van Fan and Jiří Jaromír Klemeš},
keywords = {COVID-19, Energy impacts, Environmental impacts, Energy recovery, Lessons, Emerging opportunities},
abstract = {COVID-19 has caused great challenges to the energy industry. Potential new practices and social forms being facilitated by the pandemics are having impacts on energy demand and consumption. Spatial and temporal heterogeneities of impacts appear gradually due to the dynamics of pandemics and mitigation measures. This paper overviews the impacts and challenges of COVID-19 pandemics on energy demand and consumption and highlights energy-related lessons and emerging opportunities. The discussion on energy-related issues is divided into four main sections: emergency situation and its impacts, environmental impacts and stabilising energy demand, recovering energy demand, and lessons and emerging opportunities. The changes in energy requirements are compared and analysed from multiple perspectives according to available data and information. In general, although the overall energy demand declines, the spatial and temporal variations are complicated. The energy intensity has presented apparent changes, the extra energy for COVID-19 fighting is non-negligible for stabilising energy demand, and the energy recovery in different regions presents significant differences. A crucial issue has been to allocate and find energy-related emerging opportunities for the post pandemics. This study could offer a direction in opening new avenues for increasing energy efficiency and promoting energy saving.}
}
@article{DESCIOLI2011204,
title = {The omission effect in moral cognition: toward a functional explanation},
journal = {Evolution and Human Behavior},
volume = {32},
number = {3},
pages = {204-215},
year = {2011},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2011.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1090513811000055},
author = {Peter DeScioli and Rebecca Bruening and Robert Kurzban},
keywords = {Omission, Transparency, Moral judgment, Moral psychology},
abstract = {Moral judgment involves much more than computations of the expected consequences of behavior. A prime example of the complexity of moral thinking is the frequently replicated finding that violations by omission are judged less morally wrong than violations by commission, holding intentions constant. Here we test a novel hypothesis: Omissions are judged less harshly because they produce little material evidence of wrongdoing. Evidence is crucial because moral accusations are potentially very costly unless supported by others. In our experiments, the omission effect was eliminated when physical evidence showed that an omission was chosen. Perpetrators who “opted out” by pressing a button that would clearly have no causal effects on the victim, rather than rescuing them, were judged as harshly as perpetrators who directly caused death. These results show that, to reduce condemnation, omissions must not only be noncausal, they must also leave little or no material evidence that a choice was made.}
}
@incollection{HUANG2006586,
title = {Neo-Gricean Pragmatics},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {586-590},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/04529-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542045296},
author = {Y. Huang},
keywords = {classical Gricean pragmatics, conversational implicature, division of pragmatic labor, explicature, Grice's circle, Horn-scales, I-impliciture, implicature, interaction between Q-, I, and M-implicatures, maxims, M-implicature, neo-Gricean pragmatics, pragmatic intrusion, pragmatics, presumptive meaning, Q-implicature, R-implicature},
abstract = {Since its inception, Gricean pragmatics has revolutionized pragmatic theorizing and has to date remained one of the cornerstones of contemporary thinking in linguistic pragmatics and the philosophy of language. This article undertakes to present and assess a neo-Gricean pragmatic theory of conversational implicature, focusing on the bipartite model developed by Laurence Horn and the tripartite model advanced by Stephen Levinson.}
}
@article{HAUSER201778,
title = {The Universal Generative Faculty: The source of our expressive power in language, mathematics, morality, and music},
journal = {Journal of Neurolinguistics},
volume = {43},
pages = {78-94},
year = {2017},
note = {Language Evolution: On the Origin of Lexical and Syntactic Structures},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2016.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0911604416300811},
author = {Marc D. Hauser and Jeffrey Watumull},
keywords = {Domain-specificity, Evolution, Generative functions, Language faculty, Recursion, Turing machine, Universal generative faculty},
abstract = {Many have argued that the expressive power of human thought comes from language. Language plays this role, so the argument goes, because its generative computations construct hierarchically structured, abstract representations, covering virtually any content and communicated in linguistic expressions. However, language is not the only domain to implement generative computations and abstract representations, and linguistic communication is not the only medium of expression. Mathematics, morality, and music are three others. These similarities are not, we argue, accidental. Rather, we suggest they derive from a common computational system that we call the Universal Generative Faculty or UGF. UGF is, at its core, a suite of contentless generative procedures that interface with different domains of knowledge to create contentful expressions in thought and action. The representational signatures of different domains are organized and synthesized by UGF into a global system of thought. What was once considered the language of thought is, on our view, the more specific operation of UGF and its interfaces to different conceptual domains. This view of the mind changes the conversation about domain-specificity, evolution, and development. On domain-specificity, we suggest that if UGF provides the generative engine for different domains of human knowledge, then the specificity of a given domain (e.g., language, mathematics, music, morality) is restricted to its repository of primitive representations and to its interfaces with UGF. Evolutionarily, some generative computations are shared with other animals (e.g., combinatorics), both for recognition-learning and generation-production, whereas others are uniquely human (e.g., recursion); in some cases, the cross-species parallels may be restricted to recognition-learning, with no observable evidence of generation-production. Further, many of the differences observed between humans and other animals, as well as among nonhuman animals, are the result of differences in the interfaces: whereas humans promiscuously traverse (consciously and unconsciously) interface conditions so as to combine and analogize concepts across many domains, nonhuman animals are far more limited, often restricted to a specific domain as well as a specific sensory modality within the domain. Developmentally, the UGF perspective may help explain why the generative powers of different domains appear at different stages of development. In particular, because UGF must interface with domain-specific representations, which develop on different time scales, the generative power of some domains may mature more slowly (e.g., mathematics) than others (e.g., language). This explanation may also contribute to a deeper understanding of cross-cultural differences among human populations, especially cases where the generative power of a domain appears absent (e.g., cultures with only a few count words). This essay provides an introduction to these ideas, including a discussion of implications and applications for evolutionary biology, human cognitive development, cross-cultural variation, and artificial intelligence.}
}
@article{LIANG2022119384,
title = {Dynamic Causal Modelling of Hierarchical Planning},
journal = {NeuroImage},
volume = {258},
pages = {119384},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119384},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922005031},
author = {Qunjun Liang and Jinhui Li and Senning Zheng and Jiajun Liao and Ruiwang Huang},
keywords = {Dynamic Causal Modelling (DCM), Parametric Empirical Bayes (PEB), fMRI, neural architecture, individual difference},
abstract = {Hierarchical planning (HP) is a strategy that optimizes the planning by storing the steps towards the goal (lower-level planning) into subgoals (higher-level planning). In the framework of model-based reinforcement learning, HP requires the computation through the transition value between higher-level hierarchies. Previous study identified the dmPFC, PMC and SPL were involved in the computation process of HP respectively. However, it is still unclear about how these regions interaction with each other to support the computation in HP, which could deepen our understanding about the implementation of plan algorithm in hierarchical environment. To address this question, we conducted an fMRI experiment using a virtual subway navigation task. We identified the activity of the dmPFC, premotor cortex (PMC) and superior parietal lobe (SPL) with general linear model (GLM) in HP. Then, Dynamic Causal Modelling (DCM) was performed to quantify the influence of the higher- and lower-planning on the connectivity between the brain areas identified by the GLM. The strongest modulation effect of the higher-level planning was found on the dmPFC→right PMC connection. Furthermore, using Parametric Empirical Bayes (PEB), we found the modulation of higher-level planning on the dmPFC→right PMC and right PMC→SPL connections could explain the individual difference of the response time. We conclude that the dmPFC-related connectivity takes the response to the higher-level planning, while the PMC acts as the bridge between the higher-level planning to behavior outcome.}
}
@article{CHEN201910,
title = {An artificial intelligence based data-driven approach for design ideation},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {10-22},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300604},
author = {Liuqing Chen and Pan Wang and Hao Dong and Feng Shi and Ji Han and Yike Guo and Peter R.N. Childs and Jun Xiao and Chao Wu},
keywords = {Idea generation, Artificial intelligence in design, Data-driven design, Generative adversarial networks, Semantic network analysis, Network visualisation, Computational creativity},
abstract = {Ideation is a source of innovation and creativity, and is commonly used in early stages of engineering design processes. This paper proposes an integrated approach for enhancing design ideation by applying artificial intelligence and data mining techniques. This approach consists of two models, a semantic ideation network and a visual concepts combination model, which provide inspiration semantically and visually based on computational creativity theory. The semantic ideation network aims to provoke new ideas by mining potential knowledge connections across multiple knowledge domains, and this was achieved by applying “step-forward” and “path-track” algorithms which assist in exploring forward given a concept and in tracking back the paths going from a departure concept through a destination concept. In the visual concepts combination model, a generative adversarial networks model is proposed for generating images which synthesize two distinct concepts. An implementation of these two models was developed and tested in a design case study, which indicated that the proposed approach is able to not only generate a variety of cross-domain concept associations but also advance the ideation process quickly and easily in terms of quantity and novelty.}
}
@article{ANDRADE2022102986,
title = {Writing styles and modes of engagement with the future},
journal = {Futures},
volume = {141},
pages = {102986},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102986},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000866},
author = {Stefan B. Andrade and Anneke Sools and Yashar Saghai},
keywords = {Anticipation, Writing style, Modes of engagement with the future, Anticipatory functions, narrative, Digital story grammar},
abstract = {This paper present a new approach to analyze how people anticipate the future in times of uncertainty. Our approach combines insights from narrative theory and the sociology of anticipatory modes of engagement with the future. We applied a mixed method approach to analyze 166 letters from a creative writing exercise where residents from five countries was asked to write retrospectively from the viewpoint of a desired post-corona future. Using the methodology of Digital Story Grammar, we first categorized the letters given their grammatical structure in terms of who are in stories (characters), what the stories are about (type of action), and to what or whom were the actions directed to (objects for the character’s actions). This resulted in four writing styles: (1) analytical-observational, (2) collective-moral, (3) dialogical-personal, and (4) sensory-emotional. Consequently, we interpreted the four writing styles qualitatively in relation to the theory of modes of engagement with the future (i.e., familiarity, plans, exploration, and justification). We conclude by reflecting on the relationships between writing styles and modes in a multi-paradigmatic approach to the study of anticipation and the relevance to scenario-building practices.}
}
@incollection{POULINDUBOIS2017653,
title = {Chapter 26 - The Development of Object Categories: What, When, and How?},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {San Diego},
pages = {653-671},
year = {2017},
isbn = {978-0-08-101107-2},
doi = {https://doi.org/10.1016/B978-0-08-101107-2.00027-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081011072000270},
author = {Diane Poulin-Dubois and Sabina Pauen},
keywords = {Categorization, development, infant, child, categories, perceptual, conceptual},
abstract = {From birth, infants are exposed to a wealth of information from their surroundings. This makes early categorization abilities especially important for infants and children to process information and come to understand the world around them. As a result of several sophisticated experimental paradigms, it is well-established that early categorization abilities become refined over the developmental trajectory. Researchers have identified a global-to-basic shift in early categorical thinking, such that preverbal infants discriminate between global-level categories (i.e., dogs, cats, chairs, tables, etc.) before basic-level categories (i.e., different breeds of cats and dogs). However, differences in the literature regarding the timing of this shift emerge depending on the paradigm used to measure categorization. There is evidence to suggest that infants also use dynamic, causal, and functional information to guide their object categorization and discrimination. This chapter provides a comprehensive review of the research and theory on early categorization and concept development.}
}
@article{KANSELAAR2001123,
title = {Computer supported collaborative learning Computer supported collaborative learning: cognitive and computational approaches: P. Dillenbourg (Ed.); Pergamon, Elsevier Science Ltd., Oxford, 1999, 246pp., ISBN 0-08-043073-2},
journal = {Teaching and Teacher Education},
volume = {17},
number = {1},
pages = {123-129},
year = {2001},
issn = {0742-051X},
doi = {https://doi.org/10.1016/S0742-051X(00)00042-1},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X00000421},
author = {Gellof Kanselaar and Gijsbert Erkens and Jos Jaspers and Hermi (Tabachneck-) Schijf}
}
@incollection{DEBEUKELAER2018455,
title = {Chapter 21 - Relating movements in aesthetic spaces: Immersing, distancing, and remembering},
editor = {Julia F. Christensen and Antoni Gomila},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {237},
pages = {455-469},
year = {2018},
booktitle = {The Arts and The Brain},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0079612318300141},
author = {Sophie {De Beukelaer} and Ruben Azevedo and Manos Tsakiris},
keywords = {Aesthetic experience, Embodied simulation, Associative processing, Constructive memory, Aesthetic spaces},
abstract = {According to Aby Warburg, the aesthetic experience is informed by a pendulum-like movement of the observer's mind that allows him to immerse as well as to take distance from the artwork's composing elements. To account for Warburg's definition, we are proposing embodied simulation and associative processing as constitutive mechanisms of this pendulum-like movement within the aesthetic experience that enable the observer to relate to the displayed artistic material within aesthetic spaces. Furthermore, we suggest that associative processing elicits constructive memory processes that permit the development of a knowledge within which the objects of art become part of memory networks, potentially informing future ways of thinking, feeling, and behaving in real-world situations, as an individual or collectively.}
}
@incollection{VODOVOTZ201589,
title = {Chapter 4.2 - Data-Driven and Statistical Models: Everything Old Is New Again},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {89-98},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00012-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000124},
author = {Yoram Vodovotz and Gary An},
keywords = {Data-driven modeling, statistical modeling, systems biology, computational biology, critical illness, inflammation},
abstract = {In this chapter, data-driven and statistical methods, and the thinking process behind them, are introduced. The development of these methods is associated with the thought process behind their use, both in the context of reductionist research as well as in the context of systems and computational biology. The concepts, advantages, and disadvantages of Big Data are discussed and contrasted with those of dynamic mechanistic modeling. Clinically translational applications of data-driven and statistical methods in the context of critical illness are presented and discussed as a gateway to true mechanistic modeling.}
}
@incollection{HALFORD2008298,
title = {Cognitive Developmental Theories},
editor = {Marshall M. Haith and Janette B. Benson},
booktitle = {Encyclopedia of Infant and Early Childhood Development},
publisher = {Academic Press},
address = {San Diego},
pages = {298-308},
year = {2008},
isbn = {978-0-12-370877-9},
doi = {https://doi.org/10.1016/B978-012370877-9.00039-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123708779000396},
author = {G.S. Halford},
abstract = {Theories of cognitive development are reviewed, beginning with pioneering theories by Piaget and Vygotsky. Neo-Piagetian theories which integrated Piagetian theory with other conceptions of cognition were developed by McLaughlin, Pascual-Leone, Case, Fischer, and Chapman. Complexity theories propose that children become capable of dealing with more complex relations as they develop. Information processing theories, neural net theories, dynamic systems theories, and theories of reasoning processes all provide models of the reasoning processes employed by children at different ages. Microgenetic analysis methods are used to study the processes of transition from one level of thinking to the next.}
}
@article{WANG2023120829,
title = {DBCT-Net:A dual branch hybrid CNN-transformer network for remote sensing image fusion},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120829},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120829},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423013313},
author = {Quanli Wang and Xin Jin and Qian Jiang and Liwen Wu and Yunchun Zhang and Wei Zhou},
keywords = {Image fusion, Convolutional neural network, Pansharpening, Transformer},
abstract = {Remote sensing image fusion aims at fusing high spatial resolution single-band panchromatic (PAN) image with spectrally informative multispectral (MS) image to generate panchromatic sharpened image with high resolution and color information, it is also called pansharpening. Most of the proposed single convolutional neural network (CNN) or transformer-based pansharpening methods own several problems, such as inability to acquire long-range features or difficult to train, resulting the loss of spatial details and colors. In addition, the computational complexity of transformer cannot be ignored. In this work, we propose a dual-branch hybrid CNN-Transformer network (DBCT-Net) that utilizes the local specificity of CNN and models the global dependencies by transformer. First, a multi-branch dense connected block (MDCB-4) network is designed to obtain spectral and textural information in MS and PAN images, respectively. Next, an encoder–decoder transformer based on the self-attention and co-attention modules is able to inject the missing local and global information, which can further enhance the results. It is worth noting that an inverted multi-head transposed attention (IMTA) is applied here to build attention maps from feature dimensions, which greatly reduces the computation time. Finally, an image reconstruction module is employed to effectively fuse the acquired texture and spectral features. Furthermore, to generate visually better pansharpened images, we propose a combined loss function that includes a focal frequency loss. Extensive experiments on WorldView II (WV2), GF-2,and QuickBird (QB) datasets show that DBCT-Net can perform better in spatial preservation and spectral feature recovery.}
}
@article{BOWLER2016117,
title = {Mindful makers: Question prompts to help guide young peoples' critical technical practices in maker spaces in libraries, museums, and community-based youth organizations},
journal = {Library & Information Science Research},
volume = {38},
number = {2},
pages = {117-124},
year = {2016},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2016.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740818815300840},
author = {Leanne Bowler and Ryan Champagne},
abstract = {This study examines question prompts as a means to scaffold reflection and reflexivity in the design, development, and use of technological artifacts in maker spaces for youth at public libraries, museums, and community-based organizations. Qualitative analysis is applied to data gathered in four focus groups with teens, three semi-structured interviews with adults who facilitate maker spaces, and six observation sessions. Outcomes include a rich description of critical thinking in the context of technology practice, and secondly, a set of eight activation questions that serve as a tool kit to encourage reflection and scaffold mindful and critical practices in community-based maker spaces for youth. Results from this study support the development of nstruments and practices to support mindful making and critical technical practice in maker spaces for youth.}
}
@article{FERGUSON20141885,
title = {Training in Minimally Invasive Lobectomy: Thoracoscopic Versus Robotic Approaches},
journal = {The Annals of Thoracic Surgery},
volume = {97},
number = {6},
pages = {1885-1892},
year = {2014},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2014.01.055},
url = {https://www.sciencedirect.com/science/article/pii/S0003497514003403},
author = {Mark K. Ferguson and Konstantin Umanskiy and Cindy Warnes and Amy D. Celauro and Wickii T. Vigneswaran and Vivek N. Prachand},
abstract = {Background
Skills required for thoracoscopic and robotic operations likely differ. The needs and abilities of trainees learning these approaches require assessment.
Methods
Trainees performed initial components of minimally invasive lobectomies using thoracoscopic or robotic approaches. Component difficulty was scored by trainees using the NASA task load index (NASATLX). Performance of each component was graded by trainees and attending surgeons on a 5-point ordinal scale (naïve, beginning learner, advanced learner, competent, master).
Results
Eleven surgical trainees performed 87 replications among three lobectomy components (divide pulmonary ligament; dissect level 7/8/9 nodes; dissect level 4/5 nodes). Before performance NASATLX scores did not differ among components or between surgical approaches. Trainees' after performance NASATLX scores appropriately calibrated task load for the components. After performance NASATLX scores were significantly lower for thoracoscopy than before performance estimates; robotic scores were similar before surgery and after performance. Task load was higher for robotic than for thoracoscopic approaches. Trainees rated their performance higher than did attending surgeons in domains of knowledge and thinking, but ratings for other domains were similarly low. Ratings for performance improved significantly as component performance repetitions increased.
Conclusions
Trainees did not differentiate task load among components or surgical approaches before attempting them. Task load scores differentiated difficulty among initial components of lobectomy, and were greater for robotic than for thoracoscopic approaches. Trainees overestimated their level of cognitive performance compared with attending physician evaluation of trainee performance. The study provides insights into how to customize training for thoracoscopic and robotic lobectomy and identifies tools to assess training effectiveness.}
}
@article{AYERS201883,
title = {The axiomatic approach to chemical concepts},
journal = {Computational and Theoretical Chemistry},
volume = {1142},
pages = {83-87},
year = {2018},
issn = {2210-271X},
doi = {https://doi.org/10.1016/j.comptc.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S2210271X18304237},
author = {Paul W. Ayers and Stijn Fias and Farnaz Heidar-Zadeh},
abstract = {Many concepts that are central to chemical language and thought emerge from the wealth of chemists’ historical experience and cannot be precisely defined mathematically from the underlying physics. In such cases, it is useful to take an axiomatic approach: list the chemical, mathematical and computational properties that one desires for a concept to possess, and then find the rigorous (and, if possible, elegant) mathematical formulation of the concept that satisfies those desiderata. This mathematical formulation is most useful if it relies on fundamental quantities—quantum-mechanical observables, reduced density matrices, or the N-electron wavefunction—rather than method-dependent quantities (e.g., orbitals) that are not defined for some computational approaches to the molecular electronic structure problem. This ensures that the pursuit of chemical intuition does not lead one too far from the underlying physics. It also ensures that one can interpret the results of any computational method, even methods (e.g., quantum Monte Carlo) that make no reference to any molecular-orbital or valence-bond model.}
}
@article{STOKLASA2021153,
title = {Possibilistic fuzzy pay-off method for real option valuation with application to research and development investment analysis},
journal = {Fuzzy Sets and Systems},
volume = {409},
pages = {153-169},
year = {2021},
note = {Games and Decision Analysis},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2020.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0165011420302475},
author = {Jan Stoklasa and Pasi Luukka and Mikael Collan},
keywords = {Finance, Transformation, Possibility theory, Real option valuation, Fuzzy pay-off method},
abstract = {This paper presents the first fully possibilistic method for real option valuation of investment projects, a new possibilistic variant of the fuzzy pay-off method for real option valuation. The new variant is derived by using the Luukka-Stoklasa-Collan transformation and is proven to be consistent with financial theory. The new variant is comparatively analyzed with the original method and the previously presented probabilistic variant. Fast computation formulae for the new variant in all use-cases in the triangular context are presented and complete fast computation formulae also for the previously presented probabilistic variant of the method are presented for the first time. The use of the new variant is illustrated with a set of numerical examples including examples of Research and Development investment analysis.}
}
@article{ANGIONE2015102,
title = {Analysis and design of molecular machines},
journal = {Theoretical Computer Science},
volume = {599},
pages = {102-117},
year = {2015},
note = {Advances in Computational Methods in Systems Biology},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2015.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0304397515000663},
author = {C. Angione and J. Costanza and G. Carapezza and P. Lió and G. Nicosia},
keywords = {Pareto optimality,  modelling, Turing machine, Molecular machine, Biological complexity, Petri nets, Register machines, Von Neumann architectures, Trade-off genetic strategies, Flux-balance analysis},
abstract = {Biologically inspired computation has been recently used with mathematical models towards the design of new synthetic organisms. In this work, we use Pareto optimality to optimize these organisms in a multi-objective fashion. We infer the best knockout strategies to perform specific tasks in bacteria, which involve concurrent maximization/minimization of multiple functions (codomain) and optimization of several decision variables (domain). Furthermore, we propose and exploit a mapping between the metabolism and a register machine. We show that optimized bacteria have computational capability and act as molecular Turing machines programmed using a Pareto optimal solution. Finally, we investigate communication between bacteria as a means to evaluate their computational capability. We report that the density and gradient of the Pareto curve are useful tools to compare models and understand their structure, while modelling organisms as computers proves useful to carry out computation using biological machines with specific input–output conditions, as well as to estimate the bacterial computational effort for specific tasks.}
}
@article{VIJAYALAKSHMI2022103179,
title = {Predicting Hepatitis B to be acute or chronic in an infected person using machine learning algorithm},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103179},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103179},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000898},
author = {C. Vijayalakshmi and S. Pakkir Mohideen},
keywords = {Hepatitis B, Machine learning, SVM, Stochastic gradient algorithm, Dataset},
abstract = {Hepatitis B is a viral infection which causes liver damage. It can lead to death. This hepatitis B along with Hepatitis C can cause hepatocellular carcinoma and liver cirrhosis. In this paper it is discussed about Hepatitis B found positive in a person's blood test is acute or chronic. This research work plans to code an endurance forecast model for the dataset which contains the boundaries or data of Hepatitis-B patients. At first the information will be pre-prepared, to improve fit for additional handling and for being in satisfactory configuration for the calculations. At that point, several calculations to indicate the forecast and draw out the precision of the model. What's more, further contrast those calculations with indicate the calculation with most adequacy. The precision is determined by contrasting the anticipated result and ongoing result of the patient. In light of thinking about different boundaries, the model will anticipate the danger of a patient of his endurance rate of acute or chronic infected person accuracy. In this paper we use Stochastic Gradient algorithm to find the Co-connection between boundaries of the date set, kernel approximation to finalise the resulting accuracy of the acute or choric prediction of patients and SVM method we use to clustering the kernel approximation calculation and connection analysis.}
}
@article{GONZALEZFELIU201289,
title = {Modeling Urban Goods Movement: How to be Oriented with so Many Approaches?},
journal = {Procedia - Social and Behavioral Sciences},
volume = {39},
pages = {89-100},
year = {2012},
note = {Seventh International Conference on City Logistics which was held on June 7- 9,2011, Mallorca, Spain},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.03.093},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812005605},
author = {Jesus Gonzalez-Feliu and Jean-Louis Routhier},
keywords = {Urban goods movement, modeling approaches, systematic review, multidisciplinarity},
abstract = {This paper proposes an analysis of the different model construction and development approaches in the context of urban goods movement (UGM). We focus on the model development issues more than on the mathematical tools applied in these models. First, we explore the main UGM models in the field, identifying their main construction schemas and their features limits. From this analysis, we propose a classification of UGM modeling frameworks, synthesizing them on a table that illustrates their construction schemas. Second, we analyze their limits and find a first set of synergies between the different thinking schools. This analysis allows us to highlight the strong points and override their weaknesses, and to propose a set of recommendations for planners and modeling schools in order to find co-operative schemas that improve the models’ efficiency.}
}
@article{POLHILL2023103121,
title = {Cognition and hypocognition: Discursive and simulation-supported decision-making within complex systems},
journal = {Futures},
volume = {148},
pages = {103121},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103121},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723000253},
author = {J. Gareth Polhill and Bruce Edmonds},
keywords = {Simulation, Cognition, Hypocognition, Divination, Ecocyborgs, Blasphemy},
abstract = {Homo sapiens is currently believed to have evolved in the African savannah several hundreds of thousands of years ago. Since then, human societies have become, through technological innovation and application, powerful influencers of the planet’s ecological, hydrological and meteorological systems – for good and ill. They have experimented with many different systems of governance, in order to manage their societies and the environments they inhabit – using computer simulations as a tool to help make decisions concerning highly complex systems, is only the most recent of these. In questioning whether, when and how computer simulations should play a role in determining decision-making in these systems of governance, it is also worth reflecting on whether, when and how humans, or groups of humans, have the capability to make such decisions without the aid of such technology. This paper looks at and compares the characteristics of natural language-based and simulation-based decision-making. We argue that computational tools for decision-making can and should be complementary to natural language discourse approaches, but that this requires that both systems are used with their limitations in mind. All tools and approaches – physical, social and mental – have dangers when used inappropriately, but it seems unlikely humankind can survive without them. The challenge is how to do so.}
}
@article{SHEKHAR2024820,
title = {Topological data analysis enhanced prediction of hydrogen storage in metal–organic frameworks (MOFs)††Electronic supplementary information (ESI) available: Figure showing the effect of training set size. See DOI: https://doi.org/10.1039/d3ma00591g},
journal = {Materials Advances},
volume = {5},
number = {2},
pages = {820-830},
year = {2024},
issn = {2633-5409},
doi = {https://doi.org/10.1039/d3ma00591g},
url = {https://www.sciencedirect.com/science/article/pii/S2633540924000550},
author = {Shivanshu Shekhar and Chandra Chowdhury},
abstract = {Metal–organic frameworks (MOFs) have the capacity to serve as gas capturing, sensing, and storing systems. It is usual practice to select the MOF from a vast database with the best adsorption property in order to do an adsorption calculation. The costs of computing thermodynamic values are sometimes a limiting factor in high-throughput computational research, inhibiting the development of MOFs for separations and storage applications. In recent years, machine learning has emerged as a promising substitute for traditional methods like experiments and simulations when trying to foretell material properties. The most difficult part of this process is choosing characteristics that produce interpretable representations of materials that may be used for a variety of prediction tasks. We investigate a feature-based representation of materials using tools from topological data analysis. In order to describe the geometry of MOFs with greater accuracy, we use persistent homology. We show our method by forecasting the hydrogen storage capacity of MOFs during a temperature and pressure swing from 100 bar/77 K to 5 bar/160 K, using the synthetically compiled CoRE MOF-2019 database of 4029 MOFs. Our topological descriptor is used in conjunction with more conventional structural features, and their usefulness to prediction tasks is explored. In addition to demonstrating significant progress over the baseline, our findings draw attention to the fact that topological features capture information that is supplementary to the structural features.}
}
@article{YEE1991249,
title = {Dynamical approach study of spurious steady-state numerical solutions of nonlinear differential equations. I. The dynamics of time discretization and its implications for algorithm development in computational fluid dynamics},
journal = {Journal of Computational Physics},
volume = {97},
number = {2},
pages = {249-310},
year = {1991},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(91)90001-2},
url = {https://www.sciencedirect.com/science/article/pii/0021999191900012},
author = {H.C Yee and P.K Sweby and D.F Griffiths},
abstract = {The goal of this paper is to utilize the theory of nonlinear dynamics approach to investigate the possible sources of errors and slow convergence and nonconvergence of steady-state numerical solutions when using the time-dependent approach for nonlinear hyperbolic and parabolic partial differential equations terms. This interdisciplinary research belongs to a subset of a new field of study in numerical analysis sometimes referred to as “ the dynamics of numerics and the numerics of dynamics.” At the present time, this new interdisciplinary topic is still the property of an isolated discipline with all too little effort spent in pointing out an underlying generality that could make it adaptable to diverse fields of applications. This is the first of a series of research papers under the same topic. Our hope is to reach researchers in the fields of computational fluid dynamics (CFD) and, in particular, hypersonic and combustion related CFD. By simple examples (in which the exact solutions of the governing equations are known), the application of the apparently straightforward numerical technique to genuinely nonlinear problems can be shown to lead to incorrect or misleading results. One striking phenomenon is that with the same initial data, the continuum and its discretized counterpart can asymptotically approach different stable solutions. This behavior is especially important for employing a time-dependent approach to the steady state since the initial data are usually not known and a freestream condition or an intelligent guess for the initial conditions is often used. With the unique property of the different dependence of the solution on initial data for the partial differential equation and the discretized counterpart, it is not easy to delineate the true physics from numerical artifacts when numerical methods are the sole source of solution procedure for the continuum. Part I concentrates on the dynamical behavior of time discretization for scalar nonlinear ordinary differential equations in order to motivate this new yet unconventional approach to algorithm development in CFD and to serve as an introduction for parts 11 and III of the same series of research papers.}
}
@article{MELNIKOFF2018280,
title = {The Mythical Number Two},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {4},
pages = {280-293},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S136466131830024X},
author = {David E. Melnikoff and John A. Bargh},
keywords = {dual process, dual system, type 1, type 2, automaticity},
abstract = {It is often said that there are two types of psychological processes: one that is intentional, controllable, conscious, and inefficient, and another that is unintentional, uncontrollable, unconscious, and efficient. Yet, there have been persistent and increasing objections to this widely influential dual-process typology. Critics point out that the ‘two types’ framework lacks empirical support, contradicts well-established findings, and is internally incoherent. Moreover, the untested and untenable assumption that psychological phenomena can be partitioned into two types, we argue, has the consequence of systematically thwarting scientific progress. It is time that we as a field come to terms with these issues. In short, the dual-process typology is a convenient and seductive myth, and we think cognitive science can do better.}
}
@incollection{BERNINGER2004197,
title = {Chapter 6 - The Reading Brain in Children and Youth: A Systems Approach},
editor = {Bernice Wong},
booktitle = {Learning About Learning Disabilities (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {San Diego},
pages = {197-248},
year = {2004},
isbn = {978-0-12-762533-1},
doi = {https://doi.org/10.1016/B978-012762533-1/50009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780127625331500093},
author = {Virginia W. Berninger},
abstract = {Publisher Summary
This chapter presents a systems approach to the reading brain in children and youth. A supervisory attentional system in the frontal lobes protects the working brain from external and internal distraction through an inhibitory mechanism that suppresses distraction. Brains are electrochemical computers whose computations create inner mental worlds and overt interactions with the external world. Complete understanding of the functional reading system will require knowledge of regionally specific localized brain activation and interconnectivity of specific regions during the computational processes that create the inner mental worlds as well as the overt reading behavior of reading brains. Visual inspection of the brains of normal and disabled readers reveals no secrets about the structural anomalies that differentiate the neural architecture of those who learn to read easily and those who struggle to learn to read. Domain-general systems that the functional reading system may draw upon include specific sensory systems, fine motor systems for the mouth and hand, attentional systems, networks of supervisory executive functions, the limbic system, and the higher-level thinking and problem solving system.}
}
@article{GUEST1989560,
title = {An overview of vector and parallel processors in scientific computation},
journal = {Computer Physics Communications},
volume = {57},
number = {1},
pages = {560},
year = {1989},
issn = {0010-4655},
doi = {https://doi.org/10.1016/0010-4655(89)90285-3},
url = {https://www.sciencedirect.com/science/article/pii/0010465589902853},
author = {M. Guest}
}
@article{NI2025168,
title = {Hybrid solar-energy harvest model for durable photocatalytic hydrogen production},
journal = {International Journal of Hydrogen Energy},
volume = {116},
pages = {168-177},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2025.03.119},
url = {https://www.sciencedirect.com/science/article/pii/S0360319925012145},
author = {Nan Ni and Yifan Hao and Yinglao Liu and Haibo Li and Jin Feng and Wei Liu and Wenxu Zheng},
keywords = {Photocatalyst, Water splitting, Hydrogen generation, Photon absorption, Semiconductors},
abstract = {Hybrid solar-energy harvest model for next-generation photocatalysts is customized for central metallic manipulation to achieve superior hydrogen generation with outstanding stability and long-lasting reduction reactions. Current single catalysts fail to satisfy demands of photoelectric conversion owing to irreversible photo-corrosion and charge-carrier trapping, leading to inner transmission wanes. Here we present a hybrid solar-energy harvest model with a organic-inorganic hybrid mesoscopic system, CoT(tz)PP/CdS, ranging from ordered to disordered frameworks with sufficient reactive sites and controllable active surface, which erase the energy barriers driven by existing water decomposition. This design enables simultaneous knocked out and coupling interactions of fermions and bosons at surface of binary semiconductors without invisible multi-path fading of generation efficiency. Experimentally, multi-dimension characterizations reveal the intrinsic fine genes of such novel catalyst are innate ideas, preventing unmeaning matters of energy loss and its hydrogen evolution rate reaches up to 66.44 mmol g−1 h−1. Molecular loop domain from CoT(tz)PP limits collapse of molecules in energy transfer, mitigating recombination of electron-hole pairs and enhancing morphological integrity. Overall, these results suggest that CoT(tz)PP/CdS assists hydrogen evolution in predicated review of solar energy utilization trials, reducing internal obstacles and loss on hydrogen outcomes and offering a more valid certificate in sustainable initiative of green energy.}
}
@article{BOHSALI2025105535,
title = {Neural connectivity underlying core language functions},
journal = {Brain and Language},
volume = {262},
pages = {105535},
year = {2025},
issn = {0093-934X},
doi = {https://doi.org/10.1016/j.bandl.2025.105535},
url = {https://www.sciencedirect.com/science/article/pii/S0093934X25000045},
author = {Anastasia A. Bohsali and Joseph M. Gullett and David B. FitzGerald and Thomas Mareci and Bruce Crosson and Keith White and Stephen E. Nadeau},
keywords = {Broca’s region, Tractography, Language, Aphasia, Grammar, Supramarginal gyrus, Angular gyrus, Temporal lobe},
abstract = {Introduction
Although many white matter tracts underlying language functions have been identified, even in aggregate they do not provide a sufficiently detailed and expansive picture to enable us to fully understand the computational processes that might underly language production and comprehension. We employed diffusion tensor tractography (DTT) with a tensor distribution model to more extensively explore the white matter tracts supporting core language functions. Our study was guided by hypotheses stemming largely from the aphasia literature.
Methods
We employed high angular resolution diffusion imaging (HARDI) with a dual region of interest tractography approach. Our diffusion tensor distribution model uses a mixture of Wishart distributions to estimate the water molecule displacement probability functions on a voxel-by-voxel basis and to model crossing/branching fibers using a multicompartmental approach.
Results
We replicated the results of previously published studies of tracts underlying language function. Our study also yielded a number of novel findings: 1) extensive connectivity between Broca’s region and the entirety of the middle and superior frontal gyri; 2) extensive interconnectivity between the four subcomponents of Broca’s region, pars orbitalis, pars triangularis, pars opercularis, and the inferior precentral gyrus; 3) connectivity between the mid-superior temporal gyrus and the transverse gyrus; 4) connectivity between the mid-superior temporal gyrus, the transverse gyrus, and the planum temporale and the inferior and middle temporal gyri; and 5) connectivity between mid- and anterior superior temporal gyrus and all components of Broca’s region.
Discussion
These results, which replicate the results of prior DTT studies, also considerably extend them and thereby provide a fuller picture of the structural basis of language function and the basis for a novel model of the neural network architecture of language function. This new model is entirely consistent with discoveries from the aphasia literature and with parallel distributed processing conceptualizations of language function.}
}
@article{TEMPL20249,
title = {Advancing forensic research: An examination of compositional data analysis with an application on petrol fraud detection},
journal = {Science & Justice},
volume = {64},
number = {1},
pages = {9-18},
year = {2024},
issn = {1355-0306},
doi = {https://doi.org/10.1016/j.scijus.2023.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1355030623001223},
author = {M. Templ and J. Gonzalez-Rodriguez},
keywords = {Forensic science, Petrol data, Chemical compounds, Compositional data analysis, Classification},
abstract = {In recent years, numerous studies have examined the chemical compounds of petrol and petrol data for forensic research. Standard quantitative methods often assume that the variables or compounds do not have compositional constraints or are not part of a constrained whole, operating within an Euclidean vector space. However, chemical compounds are typically part of a whole, and the appropriate vector space for their analysis is the simplex. Biased and arbitrary results result when statistical analysis are applied on such data without proper pre-processing of such data. Compositional analysis of data has not yet been considered in forensic science. Therefore, we compare classical statistical analysis as applied in forensic research and the new proposed paradigm of compositional data analysis (CoDa). It is demonstrated how such analysis improves the analysis in petrol and forensic science. Our study shows how principal component analysis (PCA) and classification results are affected by the preprocessing steps performed on the raw data. Our results indicate that results from a log ratio analysis provides a better separation between subgroups of the data and leads to an easier interpretation of the results. In addition, with a compositional analysis a higher classification accuracy is obtained. Even a non-linear classification method - in our case a random forest - was shown to perform poorly when applied without using compositional methods. Moreover, normalization of samples due to laboratory/unit-of-measurement effects is no longer necessary, since the composition of an observation is in compositional thinking equivalent to a multiple of it, because the used (log) ratios on raw and log ratio transformed data are equal. Petrol data from different petrol stations in Brazil are used for the demonstration. This data is highly susceptible to counterfeit petrol. Forensic analysis of its chemical elements requires non-biased statistical analysis designed for compositional data to detect fraud. Based on these results, we recommend the use of compositional data methods for gasoline and petrol chemical element analysis and gasoline product characterization, authentication and fraud detection in forensic sciences.}
}
@article{GU2025105551,
title = {Semantic memory structure mediates the role of brain functional connectivity in creative writing},
journal = {Brain and Language},
volume = {264},
pages = {105551},
year = {2025},
issn = {0093-934X},
doi = {https://doi.org/10.1016/j.bandl.2025.105551},
url = {https://www.sciencedirect.com/science/article/pii/S0093934X25000203},
author = {Jing Gu and Xueyang Wang and Cheng Liu and Kaixiang Zhuang and Li Fan and Jingyi Zhang and Jiangzhou Sun and Jiang Qiu},
keywords = {Creativity, Writing, Semantic network, Functional connectivity},
abstract = {Associative theories of creativity posit that high-creativity individuals possess flexible semantic memory structures that allow broad access to varied information. However, the semantic memory structure characteristics and neural substrates of creative writing are unclear. Here, we explored the semantic network features and the predictive whole-brain functional connectivity associated with creative writing and generated mediation models. Participants completed two creative story continuation tasks. We found that keywords from written texts with superior creative writing performance encompassed more semantic categories and were highly interconnected and transferred efficiently. Connectome predictive modeling (CPM) was conducted with resting-state functional magnetic resonance imaging (fMRI) data to identify whole-brain functional connectivity patterns related to creative writing, dominated by default mode network (DMN). Semantic network features were found to mediate the relationship between brain functional connectivity and creative writing performance. These results highlight how semantic memory structure and the DMN-driven brain functional connectivity patterns support creative writing performance. Our findings extend prior research on the role of semantic memory structure and the DMN in creativity, expand upon previous research on semantic creativity, and provide insight into the cognitive and neural foundations of creative writing.}
}
@article{YEUNG2024104999,
title = {A systematic review of Drone integrated STEM education at secondary schools (2005–2023): Trends, pedagogies, and learning outcomes},
journal = {Computers & Education},
volume = {212},
pages = {104999},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.104999},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524000137},
author = {Richard Chung Yiu Yeung and Chi Ho Yeung and Daner Sun and Chee-Kit Looi},
keywords = {Systematic review, Drone-integrated learning, STEM education, Secondary schools},
abstract = {As the prominence of drone technology continues to captivate interest for its myriad applications in education, an understanding of the current status of drone-integrated education becomes imperative. This systematic review endeavors to furnish an updated and comprehensive analysis of the drone education studies across academic levels, with a specific emphasis on secondary education settings. To accomplish this objective, a review study with 181 publications was conducted, with a particular focus on 41 publications explicitly addressing the integration of drones in secondary STEM education. Employing a systematic approach, this review identifies, analyzes, and synthesizes pertinent literature, ensuring a thorough comprehension of the current state of the field. The key findings of this review can be summarized as follows: 1) Among the diverse array of subjects incorporating drones, STEM disciplines emerge as the most prominently featured. 2) Experiential and project-based learning stand out as the most commonly adopted pedagogical methods in drone-integrated STEM education. The incorporation of teamwork and hands-on activities is frequently cited as instructional strategies aimed at enhancing drone-integrated STEM learning experiences. 3) Beyond the acquisition of drone-related technical skills, the reported learning outcomes encompass a spectrum of aspects, including heightened STEM career awareness, increased engagement and learning interest, and collaborative problem-solving abilities. The findings underscore the potential of drones to ignite passion for STEM subjects among secondary students, achieved through interdisciplinary, hands-on applications that foster problem-solving and design competencies.}
}
@article{ALI2023e14993,
title = {Small hydropower generation using pump as turbine; a smart solution for the development of Pakistan's energy},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e14993},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e14993},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023022004},
author = {Asad Ali and Jianping Yuan and Hamza Javed and Qiaorui Si and Ibra Fall and Israel Enema Ohiemi and Fareed Konadu Osman and Rice ul Islam},
keywords = {Pump-as-turbine, Small hydropower, Renewable energy, Environment friendly, Energy resources, Energy in developing countries},
abstract = {Energy supply that is sustainable, effective, and economical has a strong association with socio-economic growth, particularly in developing countries such as Pakistan. Due to the ever-increasing gap between supply and demand, Pakistan has become an energy-deficient nation, with most people having no-to-limited access to power. Pakistan has been suffering from power shortages and an energy crisis because of its strong reliance on fossil-fuels to provide expensive electricity. Therefore, this paper offers a novel concept for developing Pakistan's energy by producing small-hydropower using Pump-As-Turbine (PAT), which is a form of Renewable-energy with lower environmental-impact and has not been used in Pakistan previously. PATs have shown several advantages over traditional hydro-turbines, such as minimum expenses, low-complexity, short delivery time, ease of spare parts, easy installation, availability in a large number of standard sizes, and massive production for broad-range of heads and flow rates. According to technical standards, any sort of pump could be used as PAT, including radial, mixed, single-stage, multi-stage etc. for power generation, which are capable of producing 5kW–1000kW of power, depending on their usage. However, Pakistan has shown little to no interest in exploring small/micro hydropower generation (PATs technology). Thus, this study offers public awareness and forward thinking regarding the use of advanced SHPs and draws the interests of legislators and different investors via solid recommendations about the cost-effective and environmental-friendly technology (PAT).}
}
@article{TUCKER199223,
title = {Deterministic and nondeterministic computation, and horn programs, on abstract data types},
journal = {The Journal of Logic Programming},
volume = {13},
number = {1},
pages = {23-55},
year = {1992},
issn = {0743-1066},
doi = {https://doi.org/10.1016/0743-1066(92)90020-4},
url = {https://www.sciencedirect.com/science/article/pii/0743106692900204},
author = {J.V. Tucker and J.I. Zucker},
abstract = {We investigate the notion of “semicomputability,” intended to generalize the notion of recursive enumerability of relations to abstract structures. Two characterizations are considered and shown to be equivalent: one in terms of “partial computable functions” (for a suitable notion of computability over abstract structures) and one in terms of definability by means of Horn programs over such structures. This leads to the formulation of a “Generalized Church-Turing Thesis” for definability of relations on abstract structures.}
}
@article{ERA2021105070,
title = {Dissociating cognitive, behavioral and physiological stress-related responses through dorsolateral prefrontal cortex inhibition},
journal = {Psychoneuroendocrinology},
volume = {124},
pages = {105070},
year = {2021},
issn = {0306-4530},
doi = {https://doi.org/10.1016/j.psyneuen.2020.105070},
url = {https://www.sciencedirect.com/science/article/pii/S0306453020304935},
author = {Vanessa Era and Luca Carnevali and Julian F. Thayer and Matteo Candidi and Cristina Ottaviani},
keywords = {Dorsolateral prefrontal cortex, Perseverative cognition, Cortisol, Heart rate variability, High-frequency repetitive transcranial magnetic stimulation},
abstract = {The left dorsolateral prefrontal cortex (dlPFC) has been implicated in the regulation of stress-related cognitive processes and physiological responses and is the principal target of noninvasive brain stimulation techniques applied to psychiatric conditions. However, existing studies are mostly correlational and causal evidence on the role of this region in mediating specific psychophysiological mechanisms underpinning stress-related responses are needed to make the application of such techniques more efficient. To fill this gap, this study used inhibitory continuous theta burst stimulation (cTBS) in healthy individuals to examine the extent to which activity of the left dlPFC is associated with cognitive (subjective focus on a tracking task), behavioral (reaction times and variability), and physiological responses (heart rate and its variability and cortisol level) following induction of perseverative cognition. Compared to sham and left ventral PreMotor area stimulation (as active control area), inhibition of left dlPFC determined sustained autonomic and neuroendocrine activation and increased the subjective perception of being task-focused, while not changing the behavioral and self-reported stress-related responses. Adopting a causative approach, we describe a role of left dlPFC in inhibitory control of the physiological stress-response associated to perseverative thinking.}
}
@incollection{LEEFRANCISS2025237,
title = {Chapter 13 - Generative artificial intelligence in genetics: A comprehensive review},
editor = {Khalid Raza},
booktitle = {Deep Learning in Genetics and Genomics},
publisher = {Academic Press},
pages = {237-247},
year = {2025},
isbn = {978-0-443-27523-4},
doi = {https://doi.org/10.1016/B978-0-443-27523-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443275234000056},
author = {Nicholas {Lee Franciss}},
keywords = {Generative artificial intelligence, Generative pretrained transformers, Large language models, Model architecture, Transformers},
abstract = {Generative artificial intelligence (GenAI) is revolutionizing genetics by applying the computational capabilities of predictive algorithms to unveil the genome's intricate complexities. From protein prediction to gene discovery and motif detection, GenAI techniques are transforming our understanding of genetic processes that were not previously possible. Here we explore how Markov chains, long-standing predecessors of more modern technologies like large language models (LLMs) and generative pretrained transformers (GPTs), have been complemented by these advanced methods, empowering researchers to extract unprecedented levels of information from DNA sequences, including regulatory networks that govern gene expression. We dive deep into how the individual model architectures enable their capability to implicitly understand and generate biological data. The cultural and intellectual implications of DeepMind's AlphaFold on the prediction of three-dimensional protein structures and, with it, its cultural impact on generative approaches in protein design and is also explored.}
}
@article{KOICHU2015233,
title = {Proving as problem solving: The role of cognitive decoupling},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {233-244},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315300067},
author = {Boris Koichu and Uri Leron},
keywords = {Proving, Problem solving, Cognitive decoupling, Cycles in problem solving, Drawings and diagrams, Dual process theory},
abstract = {This paper discusses the process of proving from a novel theoretical perspective, imported from cognitive psychology research. This perspective highlights the role of hypothetical thinking, mental representations and working memory capacity in proving, in particular the effortful mechanism of cognitive decoupling: problem solvers need to form in their working memory two closely related models of the problem situation – the so-called primary and secondary representations – and to keep the two models decoupled, that is, keep the first fixed while performing various transformations on the second, while constantly struggling to protect the primary representation from being “contaminated” by the secondary one. We first illustrate the framework by analyzing a common scenario of introducing complex numbers to college-level students. The main part of the paper consists of re-analyzing, from the perspective of cognitive decoupling, previously published data of students searching for a non-trivial proof of a theorem in geometry. We suggest alternative (or additional) explanations for some well-documented phenomena, such as the appearance of cycles in repeated proving attempts, and the use of multiple drawings.}
}
@article{VAR2025e41447,
title = {A new strategy for constructing alternative consumer confidence indexes to explain household consumption: A fuzzy DEMATEL approach},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e41447},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e41447},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024174786},
author = {Özge Var and Alptekin Durmuşoğlu and Türkay Dereli},
keywords = {Consumer confidence index, Consumer surveys, Fuzzy DEMATEL, Household consumption, Lasso regression},
abstract = {Background
Consumer Confidence Index (CCI) is a measure obtained from consumer surveys (CS) that gauges assessments and expectations of the economic environment. Common practice uses 4 of the 12 questions in CCI calculation. However, efforts to find best set of questions continue, such as the European Commission swapping two questions in 2019. Literature studies employ different combinations of questions; however all-alternative combinations take too much time and computational power. The questions also exhibit cause-and-effect relationships as household consumption predictors and are not statistically independent of one another.
Objective
We suggest classifying the CS questions as "Causes" and "Effects." It makes sense that inquiries in the cause group should provide a better explanation of household consumption. If this theory turns out to be correct, a smaller solution space will be able to be used to find the ideal substitute CCI.
Method
A fuzzy DEMATEL (Decision-Making Trial and Evaluation Laboratory), a reliable method to present causal relationships, is used to classification. The prediction power of cause group (in terms of explaining household expenditures) is measured with the Lasso regression (Least Absolute Shrinkage and Selection Operator), which provides more interpretable regression models. This approach was applied to European Union dataset from 2007Q3 to 2021Q2.
Results
The cause group included four CS questions and explained the 75% variability of the consumption expenditures. It is performed comparably to earlier studies that took into account all possible question combinations. The Türkiye case, covering data from 2007 to 2021, supported the finding of EU case, explaining 84% variation in consumption expenditures.
Conclusion
These encouraging results suggest that comparable prediction power can be attained with a significant reduction in effort (in comparison to all brute force). Therefore, this approach would provide shortcut for constructing alternative CCIs to the authorities.}
}
@incollection{BLAGOJEVIC20171,
title = {Chapter One - A Systematic Approach to Generation of New Ideas for PhD Research in Computing},
editor = {Ali R. Hurson and Veljko Milutinović},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {104},
pages = {1-31},
year = {2017},
booktitle = {Creativity in Computing and DataFlow SuperComputing},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245816300572},
author = {V. Blagojević and D. Bojić and M. Bojović and M. Cvetanović and J. Đorđević and Đ. Đurđević and B. Furlan and S. Gajin and Z. Jovanović and D. Milićev and V. Milutinović and B. Nikolić and J. Protić and M. Punt and Z. Radivojević and Ž. Stanisavljević and S. Stojanović and I. Tartalja and M. Tomašević and P. Vuletić},
keywords = {PhD research, Idea generation, Research methodology, Research idea classification, Creative thinking},
abstract = {This article represents an effort to help PhD students in computer science and engineering to generate good original ideas for their PhD research. Our effort is motivated by the fact that most PhD programs nowadays include several courses, as well as the research component, that should result in journal publications and the PhD thesis, all in a timeframe of 3–6 years. In order to help PhD students in computing disciplines to get focused on generating ideas and finding appropriate subject for their PhD research, we have analyzed some state-of-the-art inventions in the area of computing, as well as the PhD thesis research of faculty members of our department, and came up with a proposal of 10 methods that could be implemented to derive new ideas, based on the existing body of knowledge in the research field. This systematic approach provides guidance for PhD students, in order to improve their efficiency and reduce the dropout rate, especially in the area of computing.}
}
@article{MITTAL2021102927,
title = {Modified-MaMeMi filter bank for efficient extraction of brainwaves from electroencephalograms},
journal = {Biomedical Signal Processing and Control},
volume = {69},
pages = {102927},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102927},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421005243},
author = {Rakshit Mittal and A. Amalin Prince and Saif Nalband and Femi Robert and Agastinose Ronickom Jac Fredo},
keywords = {Electroencephalogram, Band-pass filter, Brainwaves, MaMeMi filter},
abstract = {Electroencephalography (EEG) is an important tool for characterizing the functioning of the brain. Studies based on EEG involve the extraction of different spectra from EEG signals. Traditional methods of extracting these brainwaves (commonly δ, θ, α, β, γ) from EEG signals, like impulse-response filtering or wavelet decomposition, are computationally inefficient or unsuitable for real-time implementation. The Maximum-Mean-Minimum (MaMeMi) filter is a signal processing algorithm that is computationally efficient for signal filtering. The response of the MaMeMi filter is dependent on pre-decided filter coefficients. An obstacle to its implementation is that the filter coefficients have to be tuned to the sampling frequency. We propose the Modified-MaMeMi (MoMaMeMi) filter, in which the choice of coefficients is independent from the sampling frequency. Furthermore, we develop a band-pass MoMaMeMi filter which is duplicated in a filter bank, to decompose EEG signals into five common brainwaves. We validate the efficiency of the proposed filter bank by the increase in Signal-to-Noise Ratio (SNR). The maximum average increase in SNR is 19.68 dB. To prove utility of the filter-bank, we statistically compare the values of windowed average power extracted from the MoMaMeMi-filtered signals, between seizure and non-seizure components of the EEG data-set. A significant difference between the distributions suggests utility for classification problems. Since EEG-signal processing algorithms are highly customised and not limited to the 5 common brainwaves reported in this paper, we also develop a program to determine filter parameters for extraction of unique frequency bands in a bespoke MoMaMeMi filter.}
}
@article{SAHU2023105206,
title = {SCZ-SCAN: An automated Schizophrenia detection system from electroencephalogram signals},
journal = {Biomedical Signal Processing and Control},
volume = {86},
pages = {105206},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105206},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423006390},
author = {Geet Sahu and Mohan Karnati and Abhishek Gupta and Ayan Seal},
keywords = {Schizophrenia, Electroencephalography, Continuous wavelet transform, Scalogram, Convolutional neural network},
abstract = {Schizophrenia (SCZ) is a severe neurological and physiological syndrome that perverts a patient’s perception of reality. SCZ exhibits several symptoms, including hallucinations, delusions, aberrant behavior, and thinking. It affects their professional, academic, personal, and social lives. Neurologists use a variety of verbal and visual tests to determine SCZ. However, these methods are laborious, time-consuming, superficial, and vulnerable to mistakes. Therefore, it is necessary to create an automated model for SCZ detection. Convolutional neural networks have swiftly established themselves in the field of mental health care due to the growth of deep learning in recent decades. Electroencephalogram (EEG) data records the variations in the neural dynamics of human memory. Using EEG data, this study proposes an automatic SCZ detection method using separable convolution attention network (SCZ-SCAN). The proposed network employs depth-wise separable convolution and attention networks on high-level and low-level to aggregate characteristics of 2-D scalogram images acquired from the continuous wavelet transform. The depth-wise separable convolutions help to create a lightweight framework, while attention techniques concentrate on significant features and reduce futile computations by removing the transmission of irrelevant features. The proposed approach has an average classification accuracy of 99% and 95% on the IBIB-PAN and EEG data from the basic sensory task in SZ dataset. Moreover, statistical hypothesis testing is performed using Wilcoxon’s Rank-Sum test to signify the model performance and it proves that SCZ-SCAN is statistically efficient to nine cutting-edge methods. Experimental results show that the PSFAN statistically defeats 11 contemporary methods, proving its effectiveness for medical industrial applications.}
}
@article{GENNARI2023103006,
title = {Design for social digital well-being with young generations: Engage them and make them reflect},
journal = {International Journal of Human-Computer Studies},
volume = {173},
pages = {103006},
year = {2023},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2023.103006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581923000125},
author = {Rosella Gennari and Maristella Matera and Diego Morra and Alessandra Melonio and Mehdi Rizvi},
keywords = {Digital well-being, Social digital well-being, Responsible design, Smart-thing design, Toolkit},
abstract = {Digital well-being traditionally means limiting the effects on individuals of technology abuses. However, in a broader perspective, it can be crucial to consider the pervasiveness of technology, and the effect it can have not only on individuals but also on their peers in the context of diverse everyday-life situations. Within this view, which emphasises the social side of digital well-being, the paper argues the need of educating young generations to participate in the making of technology for a social goal and have a reflective attitude towards technology and its impact on society. It, therefore, presents a design toolkit as a means to (i) engage young generations to become active in design for social digital well-being and, thanks to the exposure to how technology works, (ii) reflect deeply on the pros and cons of technology in use in their everyday life. By presenting the results of a study with 24 high-school pupils and their teachers, the paper discusses how a phygital toolkit, which structures the design process, engages them in the rapid prototyping of their own smart things, and how it acts as a proxy for soliciting their own reflections around technology and social digital well-being.}
}
@article{THEOFILIDIS2024219,
title = {Mental Imagery: Investigating the Limits of Mental Partitioning},
journal = {Revista Colombiana de Psiquiatría (English ed.)},
volume = {53},
number = {3},
pages = {219-228},
year = {2024},
issn = {2530-3120},
doi = {https://doi.org/10.1016/j.rcpeng.2024.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2530312024000602},
author = {Antonios Theofilidis and Maria-Valeria Karakasi and Filippos Kargopoulos},
keywords = {Mental imagery, Mental partitioning, Memory, Cognition, Neuroscience, Imaginería mental, Partición mental, Memoria, Cognición, Neurociencia},
abstract = {Introduction
Do we form mental models which bear an analogical relation to the real world like those of a photograph? Has the language of thought an analogue nature (it makes use of mental imagery) or whether it is exclusively of digital nature like language?
Objectives
The basic aim of the present study is to contribute to the ongoing work on mental imagery by extending the research to an unexplored area that of mental partitioning.
Methods
The present research sample consisted of 498 participants (234 males and 264 females). We used the SPSS software package in order to analyze our data.
Results
According to our results, we detected significant peculiarities in the cognitive performance of the participants in the tasks of mental partitioning of the Moebius strip, indicating certain limitations inherent in human thinking.
Conclusions
The position we are led to adopt is closer to that of Pylyshyn (2003), who maintained that visual mental imagery depends on abstract form of thought and on previous knowledge. Specifically, it rests on previous abstract propositional thought and knowledge rather than on concrete perceptual processes like the ones proposed by Kosslyn and Sheppard. The present work investigates a potentially valuable theoretical basis in imagery research for understanding maladaptive imagery across various related clinical disorders, while encouraging multidisciplinary approaches among cognitive psychological/neuroscientific and clinical domains.
Resumen
Introducción
¿Formamos modelos mentales que guardan una relación analógica con el mundo real como los de una fotografía? ¿Tiene el lenguaje del pensamiento una naturaleza analógica (hace uso de imágenes mentales) o es exclusivamente de naturaleza digital como el lenguaje?
Objetivos
El objetivo básico del presente estudio es contribuir al trabajo en curso sobre la imaginería mental extendiendo la investigación a un área inexplorada que es la partición mental.
Métodos
La muestra de la presente investigación estuvo compuesta por 498 participantes (234 varones y 264 mujeres). Usamos el paquete de software SPSS® para analizar nuestros datos.
Resultados
De acuerdo con nuestros resultados, detectamos peculiaridades significativas en el desempeño cognitivo de los participantes en las tareas de partición mental de la tira de Moebius, indicando ciertas limitaciones inherentes al pensamiento humano.
Conclusiones
La posición a la que nos vemos llevados a adoptar se acerca más a la de Pylyshyn (2003), quien sostenía que la imaginería mental visual depende de formas abstractas de pensamiento y de conocimientos previos. Específicamente, se basa en el pensamiento y el conocimiento proposicionales abstractos previos más que en procesos de percepción concretos como los propuestos por Kosslyn y Sheppard. El presente trabajo investiga una base teórica potencialmente valiosa en la investigación de imágenes para comprender las imágenes desadaptativas en varios trastornos clínicos relacionados, al tiempo que fomenta enfoques multidisciplinarios entre los dominios cognitivos psicológicos/neurocientíficos y clínicos.}
}
@incollection{MILLER2020205,
title = {Chapter 10 - AI, autonomous machines and human awareness: Towards shared machine-human contexts in medicine},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {205-220},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000109},
author = {D. Douglas Miller and Elena A. Wood},
keywords = {Medicine, Health care, Artificial intelligence, Medical education, Applications, Challenges},
abstract = {Medical curricula trend to integrate clinical skills training and to create efficiencies in preclinical medical sciences, but the rapid emergence big data-intensive health care has led to initiating collaborations among data scientists, computer engineers, and medical educators that might generate novel educational high-technology platforms and innovative AI practice applications. The preprocessing of big data improves neural network feature recognition, improving the speed and accuracy of AI diagnostics and permitting chronic disease predictions. Applications of generative adversarial networks to create virtual patient phenotypes and image sets exposes medical learners to endless illness presentations, improving system-1 critical thinking for differential diagnosis development. AI offers great potential for education data managers working in support of medical educators and learners. These opportunities to build a shared context, in keeping with these themes of this book, include emerging data-driven AI applications for medical education and provider training include individual aptitude-based career advising, early identification of learners with academic difficulties, highly focused e-tutoring interventions, and natural language processing of standardized exam questions.}
}
@article{BENEDEK2014125,
title = {To create or to recall? Neural mechanisms underlying the generation of creative new ideas},
journal = {NeuroImage},
volume = {88},
pages = {125-133},
year = {2014},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2013.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S1053811913011130},
author = {Mathias Benedek and Emanuel Jauk and Andreas Fink and Karl Koschutnig and Gernot Reishofer and Franz Ebner and Aljoscha C. Neubauer},
keywords = {Creativity, fMRI, Human cognition, Memory retrieval, Inferior parietal cortex},
abstract = {This fMRI study investigated brain activation during creative idea generation using a novel approach allowing spontaneous self-paced generation and expression of ideas. Specifically, we addressed the fundamental question of what brain processes are relevant for the generation of genuinely new creative ideas, in contrast to the mere recollection of old ideas from memory. In general, creative idea generation (i.e., divergent thinking) was associated with extended activations in the left prefrontal cortex and the right medial temporal lobe, and with deactivation of the right temporoparietal junction. The generation of new ideas, as opposed to the retrieval of old ideas, was associated with stronger activation in the left inferior parietal cortex which is known to be involved in mental simulation, imagining, and future thought. Moreover, brain activation in the orbital part of the inferior frontal gyrus was found to increase as a function of the creativity (i.e., originality and appropriateness) of ideas pointing to the role of executive processes for overcoming dominant but uncreative responses. We conclude that the process of idea generation can be generally understood as a state of focused internally-directed attention involving controlled semantic retrieval. Moreover, left inferior parietal cortex and left prefrontal regions may subserve the flexible integration of previous knowledge for the construction of new and creative ideas.}
}
@article{KHAN2024e31470,
title = {Catch-up growth with alpha and beta decoupling and their relationships between CO2 emissions by GDP, population, energy production, and consumption},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e31470},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31470},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024075017},
author = {Rabnawaz Khan},
keywords = {Economic growth, Alpha and beta decoupling, CO emissions, Energy production and consumption, Populace},
abstract = {This study explores the relationship between CO2 emissions by GDP, population, energy production, and consumption in the United States, China, Romania, and Thailand economies from 1990 to 2019. It evaluates the phenomenon of catch-up growth, which transpires when an lagging economy goes through an expansionary phase after a period of below-average performance. We used the stochastic model to illustrate in terms of alpha and beta decoupling techniques. The outcomes validated by positive and negative decoupling attitudes play a crucial role in predicting a rise in CO2 emissions owing to oil, gas, and coal use in comparison to Romania. Thailand and Romania have a more viable road to sustainability than the United States and China. The United States and China appear to have an antagonistic relationship, as suggested by decoupling attitudes. Thailand and Romania are considered to be highly environmentally sustainable countries on account of their minimal carbon emissions, efficient energy usage, and forward-thinking environmental policies. Accordingly, policy recommendations are offered based on CO2 emissions and effective mitigation policies, since this allows for determining which countries with high emissions need technological advances, best practices, and intersectoral policies.}
}
@article{WU2020242,
title = {Mentalizing during social InterAction: A four component model},
journal = {Cortex},
volume = {126},
pages = {242-252},
year = {2020},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2019.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0010945220300277},
author = {Haiyan Wu and Xun Liu and Cindy C. Hagan and Dean Mobbs},
keywords = {Metacognition, Mentalizing, Vicarious mentalizing, Co-mentalizing, Social inference},
abstract = {Mentalizing, conventionally defined as the process in which we infer the inner thoughts and intentions of others, is a fundamental component of human social cognition. Yet its role, and the nuanced layers involved, in real world social interaction are rarely discussed. To account for this lack of theory, we propose the interactive mentalizing theory (IMT) -to emphasize the role of metacognition in different mentalizing components. We discuss the connection between mentalizing, metacognition, and social interaction in the context of four elements of mentalizing: (i) Metacognition–inference of our own thought processes and social cognitions and which is central to all other components of mentalizing including: (ii) first-order mentalizing–inferring the thoughts and intentions of an agent's mind; (iii) personal second-order mentalizing–inference of other's mentalizing of one's own mind; (iv) Collective mentalizing: which takes at least two forms (a) vicarious mentalizing: adopting another's mentalizing of an agent (i.e., what we think others think of an agent) and (b) co-mentalizing: mentalizing about an agent in conjunction with others' mentalizing of that agent (i.e., conforming to others beliefs about another agent's internal states). The weights of these four elements is determined by metacognitive insight and confidence in one's own or another's mentalizing ability, yielding a dynamic interaction between these circuits. To advance our knowledge on mentalizing during live social interaction, we identify how these subprocesses can be organized by different target agents and facilitated by combining computational modeling and interactive brain approaches.}
}
@article{THOMPSON2013256,
title = {The role of answer fluency and perceptual fluency in the monitoring and control of reasoning: Reply to Alter, Oppenheimer, and Epley (2013)},
journal = {Cognition},
volume = {128},
number = {2},
pages = {256-258},
year = {2013},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2013.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0010027713000553},
author = {Valerie A. Thompson and Rakefet Ackerman and Yael Sidi and Linden J. Ball and Gordon Pennycook and Jamie A. {Prowse Turner}},
keywords = {Perceptual fluency, Answer fluency, Dual process theories, Metacognition, Intuition, Analytic thinking},
abstract = {In this reply, we provide an analysis of Alter et al. (2013) response to our earlier paper (Thompson et al., 2013). In that paper, we reported difficulty in replicating Alter, Oppenheimer, Epley, and Eyre’s (2007) main finding, namely that a sense of disfluency produced by making stimuli difficult to perceive, increased accuracy on a variety of reasoning tasks. Alter, Oppenheimer, and Epley (2013) argue that we misunderstood the meaning of accuracy on these tasks, a claim that we reject. We argue and provide evidence that the tasks were not too difficult for our populations (such that no amount of “metacognitive unease” would promote correct responding) and point out that in many cases performance on our tasks was well above chance or on a par with Alter et al.’s (2007) participants. Finally, we reiterate our claim that the distinction between answer fluency (the ease with which an answer comes to mind) and perceptual fluency (the ease with which a problem can be read) is genuine, and argue that Thompson et al. (2013) provided evidence that these are distinct factors that have different downstream effects on cognitive processes.}
}
@article{JONES201795,
title = {An exploratory study on student understandings of derivatives in real-world, non-kinematics contexts},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {95-110},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316301791},
author = {Steven R. Jones},
keywords = {Calculus, Derivative, Applications, Real-world, Student understanding},
abstract = {Much research on calculus students’ understanding of applied derivatives has been done in kinematics-based contexts (i.e. position, velocity, acceleration). However, given the wide range of applications in science and engineering that are not based on kinematics, nor even explicitly on time, it is important to know how students understand applied derivatives in non-kinematics contexts. In this study, interviews with six students and surveys with 38 students were used to explore students’ “ways of understanding” and “ways of thinking” regarding applied, non-kinematics derivatives. In particular, six categories of ways of understanding emerged from the data as having been shared by a substantial portion of the students in this study: (1) covariation, (2) invoking time, (3) other symbols as constants, (4) other symbols as implicit functions, (5) implicit differentiation, and (6) output values as amounts instead of rates of change.}
}
@article{ROGOWSKI2024109246,
title = {Unlocking massively parallel spectral proper orthogonal decompositions in the PySPOD package},
journal = {Computer Physics Communications},
volume = {302},
pages = {109246},
year = {2024},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2024.109246},
url = {https://www.sciencedirect.com/science/article/pii/S0010465524001693},
author = {Marcin Rogowski and Brandon C.Y. Yeung and Oliver T. Schmidt and Romit Maulik and Lisandro Dalcin and Matteo Parsani and Gianmarco Mengaldo},
keywords = {Spectral proper orthogonal decomposition, SPOD, Parallel, Distributed, MPI, Modal decomposition, Dynamical systems},
abstract = {We propose a parallel (distributed) version of the spectral proper orthogonal decomposition (SPOD) technique. The parallel SPOD algorithm distributes the spatial dimension of the dataset preserving time. This approach is adopted to preserve the non-distributed fast Fourier transform of the data in time, thereby avoiding the associated bottlenecks. The parallel SPOD algorithm is implemented in the PySPOD library and makes use of the standard message passing interface (MPI) library, implemented in Python via mpi4py. An extensive performance evaluation of the parallel package is provided, including strong and weak scalability analyses. The open-source library allows the analysis of large datasets of interest across the scientific community. Here, we present applications in fluid dynamics and geophysics, that are extremely difficult (if not impossible) to achieve without a parallel algorithm. This work opens the path toward modal analyses of big quasi-stationary data, helping to uncover new unexplored spatio-temporal patterns.
Program summary
Program Title: PySPOD CPC Library link to program files: https://doi.org/10.17632/jf5bf26jcj.1 Developer's repository link: https://github.com/MathEXLab/PySPOD Licensing provisions: MIT License Programming language: Python Nature of problem: Large spatio-temporal datasets may contain coherent patterns that can be leveraged to better understand, model, and possibly predict the behavior of complex dynamical systems. To this end, modal decomposition methods, such as the proper orthogonal decomposition (POD) and its spectral counterpart (SPOD), constitute powerful tools. The SPOD algorithm allows the systematic identification of space-time coherent patterns. This can be used to understand better the physics of the process of interest, and provide a path for mathematical modeling, including reduced order modeling. The SPOD algorithm has been successfully applied to fluid dynamics, geophysics and other domains. However, the existing open-source implementations are serial, and they prevent running on the increasingly large datasets that are becoming available, especially in computational physics. The inability to analyze via SPOD large dataset in turn prevents unlocking novel mechanisms and dynamical behaviors in complex systems. Solution method: We provide an open-source parallel (MPI distributed) code, namely PySPOD, that is able to run on large datasets (the ones considered in the present paper reach about 200 Terabytes). The code is built on the previous serial open-source code PySPOD that was published in https://joss.theoj.org/papers/10.21105/joss.02862.pdf. The new parallel implementation is able to scale on several nodes (we show both weak and strong scalability) and solve some of the bottlenecks that are commonly found at the I/O stage. The current parallel code allows running on datasets that was not easy or possible to analyze with serial SPOD algorithms, hence providing a path towards unlocking novel findings in computational physics. Additional comments including restrictions and unusual features: The code comes with a set of built-in postprocessing tools, for visualizing the results. It also comes with extensive continuous integration, documentation, and tutorials, as well as a dedicated website in addition to the associated GiHub repository. Within the package we also provide a parallel implementation of the proper orthogonal decomposition (POD), that leverages the I/O parallel capabilities of the SPOD algorithm.}
}